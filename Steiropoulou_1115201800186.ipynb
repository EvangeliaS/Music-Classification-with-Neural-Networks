{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0E3IEwAW4lir"
      },
      "source": [
        "# Αναγνώριση Προτύπων - Μηχανική Μάθηση 3η Εργασία\n",
        "## Στειροπούλου Ευαγγελία \n",
        "### 1115201800186"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jFkkiAGv4li5"
      },
      "source": [
        "# FeedForward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "rzuja9jw4ljB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in ./.venv/lib/python3.8/site-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.8/site-packages (from torch) (4.6.2)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.8/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.8/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.venv/lib/python3.8/site-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.venv/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.venv/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.venv/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.venv/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.venv/lib/python3.8/site-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.venv/lib/python3.8/site-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in ./.venv/lib/python3.8/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (44.0.0)\n",
            "Requirement already satisfied: wheel in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
            "Requirement already satisfied: cmake in ./.venv/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
            "Requirement already satisfied: lit in ./.venv/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.8/site-packages (0.15.2)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from torchvision) (1.24.3)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in ./.venv/lib/python3.8/site-packages (from torchvision) (2.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.8/site-packages (from torchvision) (9.5.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (4.6.2)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in ./.venv/lib/python3.8/site-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (44.0.0)\n",
            "Requirement already satisfied: wheel in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (0.40.0)\n",
            "Requirement already satisfied: cmake in ./.venv/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.26.3)\n",
            "Requirement already satisfied: lit in ./.venv/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->torchvision) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->torchvision) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.8/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (4.39.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install -U scikit-learn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rbJmDdqq4ljK"
      },
      "source": [
        "## Load Data (mfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "0zP_A39N4ljP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/eva/Documents/machine_learning/Project3\n",
            "torch.Size([3200, 26])\n",
            "torch.Size([3200])\n",
            "torch.Size([800, 26])\n",
            "torch.Size([800])\n",
            "torch.Size([1376, 26])\n",
            "torch.Size([1376])\n"
          ]
        }
      ],
      "source": [
        "path = os.getcwd()\n",
        "print(path)\n",
        "\n",
        "#path = '/content/gdrive/My Drive/Project3'\n",
        "\n",
        "#create the paths for the data\n",
        "training_data_path = path + '/data/music_genre_data_di/train/mfccs'\n",
        "test_data_path = path + '/data/music_genre_data_di/test/mfccs'\n",
        "validation_data_path = path + '/data/music_genre_data_di/val/mfccs'\n",
        "\n",
        "X_train = np.load(training_data_path + '/X.npy')\n",
        "y_train = np.load(training_data_path + '/labels.npy')\n",
        "X_val   = np.load(validation_data_path + '/X.npy')\n",
        "y_val   = np.load(validation_data_path + '/labels.npy')\n",
        "X_test  = np.load(test_data_path + '/X.npy')\n",
        "y_test  = np.load(test_data_path + '/labels.npy')\n",
        "\n",
        "# Μετατροπή των labels σε ακέραιους αριθμούς (0, 1, 2, 3)\n",
        "classes = np.unique(y_train)\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "y_train = np.array([class_to_idx[label] for label in y_train])\n",
        "y_val = np.array([class_to_idx[label] for label in y_val])\n",
        "y_test = np.array([class_to_idx[label] for label in y_test])\n",
        "\n",
        "# Δημιουργία των Tensor αντικειμένων από τα numpy arrays\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train.astype(np.float32)), torch.from_numpy(y_train))\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Εμφάνιση των διαστάσεων των Tensor\n",
        "print(train_dataset.tensors[0].shape)\n",
        "print(train_dataset.tensors[1].shape)\n",
        "print(val_dataset.tensors[0].shape)\n",
        "print(val_dataset.tensors[1].shape)\n",
        "print(test_dataset.tensors[0].shape)\n",
        "print(test_dataset.tensors[1].shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C4UR4r3f4ljY"
      },
      "source": [
        "## Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "k_FW5zjl4ljb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FullyConnectedNeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# # if we want to utilize the GPU (if available), we need to assign the model to the correct device\n",
        "# if torch.cuda.is_available():\n",
        "#   device=\"cuda\"\n",
        "# else:\n",
        "#     device=\"cpu\"\n",
        "\n",
        "# print (\"device=\",device)\n",
        "\n",
        "class FullyConnectedNeuralNetwork(nn.Module): ##NeuralNetwork iS A subclass of nn.module\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedNeuralNetwork, self).__init__() ## call the constructor of super\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(26 , 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    ## Every nn.Module subclass implements the operations on input data in the forward method.\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = \"cpu\"\n",
        "model = FullyConnectedNeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "# X = torch.rand(1, 26, device = device)  ##dummy data στην  GPU\n",
        "\n",
        "# # this call uses the forward function of the model along with some background operations.\n",
        "# # Notice that we do not explicitly call forward. This is handled by PyTorch.\n",
        "# log = model(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BXaHI3564ljh"
      },
      "source": [
        "## Set Neural Network training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "RQiVfvit4ljm"
      },
      "outputs": [],
      "source": [
        "def train_network(model, optimizer, dataloader, loss_function, num_epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        loss = 0.0\n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            outputs = model(data)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            \n",
        "            # Clear the gradients and compute new gradients (backward pass)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step() # Update the weights\n",
        "\n",
        "            print(f\"loss: {loss:>7f}  [{batch_idx:>5d}/{size:>5d}]\")\n",
        "            \n",
        "    print('Training finished.')\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VGnhtXV24ljs"
      },
      "source": [
        "## Set evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "6jQDUzRf4ljv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(model, dataloader, loss_function):\n",
        "    model.eval()  # Put the model in evaluation mode\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            predictions = model(data)\n",
        "            loss = loss_function(predictions, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_predictions.extend(predictions.argmax(1).cpu().detach().numpy())\n",
        "            all_targets.extend(targets.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    # Calculate F1 score, accuracy, and confusion matrix\n",
        "    f1_score_macro = f1_score(all_targets, all_predictions, average='macro')\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    confusion_mat = confusion_matrix(all_targets, all_predictions)\n",
        "\n",
        "    return avg_loss, f1_score_macro, accuracy, confusion_mat"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-us3rn1u4lj0"
      },
      "source": [
        "## Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "ybPZ_qpS4lj4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "loss: 1.414604  [    0/ 3200]\n",
            "loss: 1.402424  [    1/ 3200]\n",
            "loss: 1.338547  [    2/ 3200]\n",
            "loss: 1.343698  [    3/ 3200]\n",
            "loss: 1.405591  [    4/ 3200]\n",
            "loss: 1.389248  [    5/ 3200]\n",
            "loss: 1.425171  [    6/ 3200]\n",
            "loss: 1.421166  [    7/ 3200]\n",
            "loss: 1.312324  [    8/ 3200]\n",
            "loss: 1.387834  [    9/ 3200]\n",
            "loss: 1.373621  [   10/ 3200]\n",
            "loss: 1.438251  [   11/ 3200]\n",
            "loss: 1.398564  [   12/ 3200]\n",
            "loss: 1.408786  [   13/ 3200]\n",
            "loss: 1.344333  [   14/ 3200]\n",
            "loss: 1.428947  [   15/ 3200]\n",
            "loss: 1.384700  [   16/ 3200]\n",
            "loss: 1.420650  [   17/ 3200]\n",
            "loss: 1.375800  [   18/ 3200]\n",
            "loss: 1.394507  [   19/ 3200]\n",
            "loss: 1.371830  [   20/ 3200]\n",
            "loss: 1.359450  [   21/ 3200]\n",
            "loss: 1.418412  [   22/ 3200]\n",
            "loss: 1.408723  [   23/ 3200]\n",
            "loss: 1.366140  [   24/ 3200]\n",
            "loss: 1.410359  [   25/ 3200]\n",
            "loss: 1.366331  [   26/ 3200]\n",
            "loss: 1.385406  [   27/ 3200]\n",
            "loss: 1.383281  [   28/ 3200]\n",
            "loss: 1.394567  [   29/ 3200]\n",
            "loss: 1.363650  [   30/ 3200]\n",
            "loss: 1.362927  [   31/ 3200]\n",
            "loss: 1.401277  [   32/ 3200]\n",
            "loss: 1.388327  [   33/ 3200]\n",
            "loss: 1.389912  [   34/ 3200]\n",
            "loss: 1.375224  [   35/ 3200]\n",
            "loss: 1.340824  [   36/ 3200]\n",
            "loss: 1.400000  [   37/ 3200]\n",
            "loss: 1.356446  [   38/ 3200]\n",
            "loss: 1.384898  [   39/ 3200]\n",
            "loss: 1.375576  [   40/ 3200]\n",
            "loss: 1.376776  [   41/ 3200]\n",
            "loss: 1.376276  [   42/ 3200]\n",
            "loss: 1.368080  [   43/ 3200]\n",
            "loss: 1.385010  [   44/ 3200]\n",
            "loss: 1.394431  [   45/ 3200]\n",
            "loss: 1.384059  [   46/ 3200]\n",
            "loss: 1.359409  [   47/ 3200]\n",
            "loss: 1.342340  [   48/ 3200]\n",
            "loss: 1.438275  [   49/ 3200]\n",
            "loss: 1.374399  [   50/ 3200]\n",
            "loss: 1.354629  [   51/ 3200]\n",
            "loss: 1.377497  [   52/ 3200]\n",
            "loss: 1.348759  [   53/ 3200]\n",
            "loss: 1.438672  [   54/ 3200]\n",
            "loss: 1.390345  [   55/ 3200]\n",
            "loss: 1.362921  [   56/ 3200]\n",
            "loss: 1.399898  [   57/ 3200]\n",
            "loss: 1.385491  [   58/ 3200]\n",
            "loss: 1.349350  [   59/ 3200]\n",
            "loss: 1.390207  [   60/ 3200]\n",
            "loss: 1.359089  [   61/ 3200]\n",
            "loss: 1.369427  [   62/ 3200]\n",
            "loss: 1.330353  [   63/ 3200]\n",
            "loss: 1.336163  [   64/ 3200]\n",
            "loss: 1.331737  [   65/ 3200]\n",
            "loss: 1.370962  [   66/ 3200]\n",
            "loss: 1.346772  [   67/ 3200]\n",
            "loss: 1.416875  [   68/ 3200]\n",
            "loss: 1.336736  [   69/ 3200]\n",
            "loss: 1.398767  [   70/ 3200]\n",
            "loss: 1.331768  [   71/ 3200]\n",
            "loss: 1.366355  [   72/ 3200]\n",
            "loss: 1.403735  [   73/ 3200]\n",
            "loss: 1.361350  [   74/ 3200]\n",
            "loss: 1.352630  [   75/ 3200]\n",
            "loss: 1.412759  [   76/ 3200]\n",
            "loss: 1.348699  [   77/ 3200]\n",
            "loss: 1.404950  [   78/ 3200]\n",
            "loss: 1.339167  [   79/ 3200]\n",
            "loss: 1.420349  [   80/ 3200]\n",
            "loss: 1.362837  [   81/ 3200]\n",
            "loss: 1.420435  [   82/ 3200]\n",
            "loss: 1.386954  [   83/ 3200]\n",
            "loss: 1.375349  [   84/ 3200]\n",
            "loss: 1.372431  [   85/ 3200]\n",
            "loss: 1.388191  [   86/ 3200]\n",
            "loss: 1.378419  [   87/ 3200]\n",
            "loss: 1.382944  [   88/ 3200]\n",
            "loss: 1.372117  [   89/ 3200]\n",
            "loss: 1.365040  [   90/ 3200]\n",
            "loss: 1.365470  [   91/ 3200]\n",
            "loss: 1.377947  [   92/ 3200]\n",
            "loss: 1.376477  [   93/ 3200]\n",
            "loss: 1.379789  [   94/ 3200]\n",
            "loss: 1.378009  [   95/ 3200]\n",
            "loss: 1.361610  [   96/ 3200]\n",
            "loss: 1.348831  [   97/ 3200]\n",
            "loss: 1.401350  [   98/ 3200]\n",
            "loss: 1.395540  [   99/ 3200]\n",
            "loss: 1.391069  [  100/ 3200]\n",
            "loss: 1.373495  [  101/ 3200]\n",
            "loss: 1.394871  [  102/ 3200]\n",
            "loss: 1.392482  [  103/ 3200]\n",
            "loss: 1.387867  [  104/ 3200]\n",
            "loss: 1.373931  [  105/ 3200]\n",
            "loss: 1.381662  [  106/ 3200]\n",
            "loss: 1.362695  [  107/ 3200]\n",
            "loss: 1.374961  [  108/ 3200]\n",
            "loss: 1.372129  [  109/ 3200]\n",
            "loss: 1.360985  [  110/ 3200]\n",
            "loss: 1.361104  [  111/ 3200]\n",
            "loss: 1.369541  [  112/ 3200]\n",
            "loss: 1.371527  [  113/ 3200]\n",
            "loss: 1.362310  [  114/ 3200]\n",
            "loss: 1.385519  [  115/ 3200]\n",
            "loss: 1.371753  [  116/ 3200]\n",
            "loss: 1.376636  [  117/ 3200]\n",
            "loss: 1.370510  [  118/ 3200]\n",
            "loss: 1.424811  [  119/ 3200]\n",
            "loss: 1.381988  [  120/ 3200]\n",
            "loss: 1.379100  [  121/ 3200]\n",
            "loss: 1.353976  [  122/ 3200]\n",
            "loss: 1.386951  [  123/ 3200]\n",
            "loss: 1.396992  [  124/ 3200]\n",
            "loss: 1.376130  [  125/ 3200]\n",
            "loss: 1.349277  [  126/ 3200]\n",
            "loss: 1.383989  [  127/ 3200]\n",
            "loss: 1.363636  [  128/ 3200]\n",
            "loss: 1.359004  [  129/ 3200]\n",
            "loss: 1.378776  [  130/ 3200]\n",
            "loss: 1.370576  [  131/ 3200]\n",
            "loss: 1.399199  [  132/ 3200]\n",
            "loss: 1.362353  [  133/ 3200]\n",
            "loss: 1.345516  [  134/ 3200]\n",
            "loss: 1.365652  [  135/ 3200]\n",
            "loss: 1.394847  [  136/ 3200]\n",
            "loss: 1.307564  [  137/ 3200]\n",
            "loss: 1.349817  [  138/ 3200]\n",
            "loss: 1.414071  [  139/ 3200]\n",
            "loss: 1.422072  [  140/ 3200]\n",
            "loss: 1.383558  [  141/ 3200]\n",
            "loss: 1.373868  [  142/ 3200]\n",
            "loss: 1.366076  [  143/ 3200]\n",
            "loss: 1.395374  [  144/ 3200]\n",
            "loss: 1.418296  [  145/ 3200]\n",
            "loss: 1.367547  [  146/ 3200]\n",
            "loss: 1.376527  [  147/ 3200]\n",
            "loss: 1.388291  [  148/ 3200]\n",
            "loss: 1.363553  [  149/ 3200]\n",
            "loss: 1.388746  [  150/ 3200]\n",
            "loss: 1.346992  [  151/ 3200]\n",
            "loss: 1.387948  [  152/ 3200]\n",
            "loss: 1.355570  [  153/ 3200]\n",
            "loss: 1.380296  [  154/ 3200]\n",
            "loss: 1.397655  [  155/ 3200]\n",
            "loss: 1.412828  [  156/ 3200]\n",
            "loss: 1.371140  [  157/ 3200]\n",
            "loss: 1.374994  [  158/ 3200]\n",
            "loss: 1.362661  [  159/ 3200]\n",
            "loss: 1.346148  [  160/ 3200]\n",
            "loss: 1.388858  [  161/ 3200]\n",
            "loss: 1.424528  [  162/ 3200]\n",
            "loss: 1.345295  [  163/ 3200]\n",
            "loss: 1.369796  [  164/ 3200]\n",
            "loss: 1.378545  [  165/ 3200]\n",
            "loss: 1.361923  [  166/ 3200]\n",
            "loss: 1.391763  [  167/ 3200]\n",
            "loss: 1.401565  [  168/ 3200]\n",
            "loss: 1.374751  [  169/ 3200]\n",
            "loss: 1.323353  [  170/ 3200]\n",
            "loss: 1.383091  [  171/ 3200]\n",
            "loss: 1.353400  [  172/ 3200]\n",
            "loss: 1.400159  [  173/ 3200]\n",
            "loss: 1.388316  [  174/ 3200]\n",
            "loss: 1.373379  [  175/ 3200]\n",
            "loss: 1.417255  [  176/ 3200]\n",
            "loss: 1.360635  [  177/ 3200]\n",
            "loss: 1.394307  [  178/ 3200]\n",
            "loss: 1.359243  [  179/ 3200]\n",
            "loss: 1.370536  [  180/ 3200]\n",
            "loss: 1.305816  [  181/ 3200]\n",
            "loss: 1.358611  [  182/ 3200]\n",
            "loss: 1.359766  [  183/ 3200]\n",
            "loss: 1.384297  [  184/ 3200]\n",
            "loss: 1.439887  [  185/ 3200]\n",
            "loss: 1.365650  [  186/ 3200]\n",
            "loss: 1.360427  [  187/ 3200]\n",
            "loss: 1.348184  [  188/ 3200]\n",
            "loss: 1.411350  [  189/ 3200]\n",
            "loss: 1.386121  [  190/ 3200]\n",
            "loss: 1.364150  [  191/ 3200]\n",
            "loss: 1.366266  [  192/ 3200]\n",
            "loss: 1.383141  [  193/ 3200]\n",
            "loss: 1.360965  [  194/ 3200]\n",
            "loss: 1.369509  [  195/ 3200]\n",
            "loss: 1.342304  [  196/ 3200]\n",
            "loss: 1.338527  [  197/ 3200]\n",
            "loss: 1.358567  [  198/ 3200]\n",
            "loss: 1.360125  [  199/ 3200]\n",
            "Epoch:  2\n",
            "loss: 1.357311  [    0/ 3200]\n",
            "loss: 1.373410  [    1/ 3200]\n",
            "loss: 1.365717  [    2/ 3200]\n",
            "loss: 1.366696  [    3/ 3200]\n",
            "loss: 1.371733  [    4/ 3200]\n",
            "loss: 1.395997  [    5/ 3200]\n",
            "loss: 1.372418  [    6/ 3200]\n",
            "loss: 1.339644  [    7/ 3200]\n",
            "loss: 1.380228  [    8/ 3200]\n",
            "loss: 1.382065  [    9/ 3200]\n",
            "loss: 1.371415  [   10/ 3200]\n",
            "loss: 1.344583  [   11/ 3200]\n",
            "loss: 1.405563  [   12/ 3200]\n",
            "loss: 1.410247  [   13/ 3200]\n",
            "loss: 1.397784  [   14/ 3200]\n",
            "loss: 1.347740  [   15/ 3200]\n",
            "loss: 1.378991  [   16/ 3200]\n",
            "loss: 1.361033  [   17/ 3200]\n",
            "loss: 1.359306  [   18/ 3200]\n",
            "loss: 1.375434  [   19/ 3200]\n",
            "loss: 1.361472  [   20/ 3200]\n",
            "loss: 1.408603  [   21/ 3200]\n",
            "loss: 1.364830  [   22/ 3200]\n",
            "loss: 1.311703  [   23/ 3200]\n",
            "loss: 1.386715  [   24/ 3200]\n",
            "loss: 1.374839  [   25/ 3200]\n",
            "loss: 1.362829  [   26/ 3200]\n",
            "loss: 1.374840  [   27/ 3200]\n",
            "loss: 1.394911  [   28/ 3200]\n",
            "loss: 1.362613  [   29/ 3200]\n",
            "loss: 1.367727  [   30/ 3200]\n",
            "loss: 1.335936  [   31/ 3200]\n",
            "loss: 1.402552  [   32/ 3200]\n",
            "loss: 1.394919  [   33/ 3200]\n",
            "loss: 1.355731  [   34/ 3200]\n",
            "loss: 1.368557  [   35/ 3200]\n",
            "loss: 1.386030  [   36/ 3200]\n",
            "loss: 1.349232  [   37/ 3200]\n",
            "loss: 1.361036  [   38/ 3200]\n",
            "loss: 1.343270  [   39/ 3200]\n",
            "loss: 1.381186  [   40/ 3200]\n",
            "loss: 1.377445  [   41/ 3200]\n",
            "loss: 1.379321  [   42/ 3200]\n",
            "loss: 1.410838  [   43/ 3200]\n",
            "loss: 1.364644  [   44/ 3200]\n",
            "loss: 1.337425  [   45/ 3200]\n",
            "loss: 1.367498  [   46/ 3200]\n",
            "loss: 1.381145  [   47/ 3200]\n",
            "loss: 1.351305  [   48/ 3200]\n",
            "loss: 1.364228  [   49/ 3200]\n",
            "loss: 1.379584  [   50/ 3200]\n",
            "loss: 1.352784  [   51/ 3200]\n",
            "loss: 1.360772  [   52/ 3200]\n",
            "loss: 1.376520  [   53/ 3200]\n",
            "loss: 1.380047  [   54/ 3200]\n",
            "loss: 1.376366  [   55/ 3200]\n",
            "loss: 1.379856  [   56/ 3200]\n",
            "loss: 1.369736  [   57/ 3200]\n",
            "loss: 1.366526  [   58/ 3200]\n",
            "loss: 1.381483  [   59/ 3200]\n",
            "loss: 1.379109  [   60/ 3200]\n",
            "loss: 1.369600  [   61/ 3200]\n",
            "loss: 1.369285  [   62/ 3200]\n",
            "loss: 1.359334  [   63/ 3200]\n",
            "loss: 1.353334  [   64/ 3200]\n",
            "loss: 1.347747  [   65/ 3200]\n",
            "loss: 1.394565  [   66/ 3200]\n",
            "loss: 1.367553  [   67/ 3200]\n",
            "loss: 1.424060  [   68/ 3200]\n",
            "loss: 1.368731  [   69/ 3200]\n",
            "loss: 1.364176  [   70/ 3200]\n",
            "loss: 1.371075  [   71/ 3200]\n",
            "loss: 1.412182  [   72/ 3200]\n",
            "loss: 1.325008  [   73/ 3200]\n",
            "loss: 1.347347  [   74/ 3200]\n",
            "loss: 1.383808  [   75/ 3200]\n",
            "loss: 1.381914  [   76/ 3200]\n",
            "loss: 1.336044  [   77/ 3200]\n",
            "loss: 1.388618  [   78/ 3200]\n",
            "loss: 1.331650  [   79/ 3200]\n",
            "loss: 1.381869  [   80/ 3200]\n",
            "loss: 1.369314  [   81/ 3200]\n",
            "loss: 1.360752  [   82/ 3200]\n",
            "loss: 1.397858  [   83/ 3200]\n",
            "loss: 1.362283  [   84/ 3200]\n",
            "loss: 1.371429  [   85/ 3200]\n",
            "loss: 1.374425  [   86/ 3200]\n",
            "loss: 1.383312  [   87/ 3200]\n",
            "loss: 1.347808  [   88/ 3200]\n",
            "loss: 1.342195  [   89/ 3200]\n",
            "loss: 1.404619  [   90/ 3200]\n",
            "loss: 1.376968  [   91/ 3200]\n",
            "loss: 1.406430  [   92/ 3200]\n",
            "loss: 1.357109  [   93/ 3200]\n",
            "loss: 1.355291  [   94/ 3200]\n",
            "loss: 1.358883  [   95/ 3200]\n",
            "loss: 1.354054  [   96/ 3200]\n",
            "loss: 1.353493  [   97/ 3200]\n",
            "loss: 1.357880  [   98/ 3200]\n",
            "loss: 1.351431  [   99/ 3200]\n",
            "loss: 1.383380  [  100/ 3200]\n",
            "loss: 1.365770  [  101/ 3200]\n",
            "loss: 1.384307  [  102/ 3200]\n",
            "loss: 1.363005  [  103/ 3200]\n",
            "loss: 1.360079  [  104/ 3200]\n",
            "loss: 1.350818  [  105/ 3200]\n",
            "loss: 1.382576  [  106/ 3200]\n",
            "loss: 1.368147  [  107/ 3200]\n",
            "loss: 1.372308  [  108/ 3200]\n",
            "loss: 1.356764  [  109/ 3200]\n",
            "loss: 1.362235  [  110/ 3200]\n",
            "loss: 1.367796  [  111/ 3200]\n",
            "loss: 1.343582  [  112/ 3200]\n",
            "loss: 1.394229  [  113/ 3200]\n",
            "loss: 1.369471  [  114/ 3200]\n",
            "loss: 1.330964  [  115/ 3200]\n",
            "loss: 1.370435  [  116/ 3200]\n",
            "loss: 1.366565  [  117/ 3200]\n",
            "loss: 1.362892  [  118/ 3200]\n",
            "loss: 1.402202  [  119/ 3200]\n",
            "loss: 1.364626  [  120/ 3200]\n",
            "loss: 1.346793  [  121/ 3200]\n",
            "loss: 1.343974  [  122/ 3200]\n",
            "loss: 1.343540  [  123/ 3200]\n",
            "loss: 1.362290  [  124/ 3200]\n",
            "loss: 1.378904  [  125/ 3200]\n",
            "loss: 1.373303  [  126/ 3200]\n",
            "loss: 1.370375  [  127/ 3200]\n",
            "loss: 1.332323  [  128/ 3200]\n",
            "loss: 1.360223  [  129/ 3200]\n",
            "loss: 1.352462  [  130/ 3200]\n",
            "loss: 1.393512  [  131/ 3200]\n",
            "loss: 1.324850  [  132/ 3200]\n",
            "loss: 1.354466  [  133/ 3200]\n",
            "loss: 1.347543  [  134/ 3200]\n",
            "loss: 1.337737  [  135/ 3200]\n",
            "loss: 1.405242  [  136/ 3200]\n",
            "loss: 1.362563  [  137/ 3200]\n",
            "loss: 1.343072  [  138/ 3200]\n",
            "loss: 1.380690  [  139/ 3200]\n",
            "loss: 1.359337  [  140/ 3200]\n",
            "loss: 1.379407  [  141/ 3200]\n",
            "loss: 1.403992  [  142/ 3200]\n",
            "loss: 1.337630  [  143/ 3200]\n",
            "loss: 1.360004  [  144/ 3200]\n",
            "loss: 1.355323  [  145/ 3200]\n",
            "loss: 1.333895  [  146/ 3200]\n",
            "loss: 1.376267  [  147/ 3200]\n",
            "loss: 1.372597  [  148/ 3200]\n",
            "loss: 1.363579  [  149/ 3200]\n",
            "loss: 1.372118  [  150/ 3200]\n",
            "loss: 1.360170  [  151/ 3200]\n",
            "loss: 1.390221  [  152/ 3200]\n",
            "loss: 1.355205  [  153/ 3200]\n",
            "loss: 1.373882  [  154/ 3200]\n",
            "loss: 1.346282  [  155/ 3200]\n",
            "loss: 1.352553  [  156/ 3200]\n",
            "loss: 1.375954  [  157/ 3200]\n",
            "loss: 1.404134  [  158/ 3200]\n",
            "loss: 1.356283  [  159/ 3200]\n",
            "loss: 1.366356  [  160/ 3200]\n",
            "loss: 1.325720  [  161/ 3200]\n",
            "loss: 1.356945  [  162/ 3200]\n",
            "loss: 1.418762  [  163/ 3200]\n",
            "loss: 1.356930  [  164/ 3200]\n",
            "loss: 1.395179  [  165/ 3200]\n",
            "loss: 1.344364  [  166/ 3200]\n",
            "loss: 1.358872  [  167/ 3200]\n",
            "loss: 1.365344  [  168/ 3200]\n",
            "loss: 1.322533  [  169/ 3200]\n",
            "loss: 1.360927  [  170/ 3200]\n",
            "loss: 1.427676  [  171/ 3200]\n",
            "loss: 1.380740  [  172/ 3200]\n",
            "loss: 1.370209  [  173/ 3200]\n",
            "loss: 1.373006  [  174/ 3200]\n",
            "loss: 1.357795  [  175/ 3200]\n",
            "loss: 1.385415  [  176/ 3200]\n",
            "loss: 1.378052  [  177/ 3200]\n",
            "loss: 1.347670  [  178/ 3200]\n",
            "loss: 1.360328  [  179/ 3200]\n",
            "loss: 1.341017  [  180/ 3200]\n",
            "loss: 1.372138  [  181/ 3200]\n",
            "loss: 1.365080  [  182/ 3200]\n",
            "loss: 1.346914  [  183/ 3200]\n",
            "loss: 1.380387  [  184/ 3200]\n",
            "loss: 1.351971  [  185/ 3200]\n",
            "loss: 1.382379  [  186/ 3200]\n",
            "loss: 1.368600  [  187/ 3200]\n",
            "loss: 1.379913  [  188/ 3200]\n",
            "loss: 1.368945  [  189/ 3200]\n",
            "loss: 1.359032  [  190/ 3200]\n",
            "loss: 1.329874  [  191/ 3200]\n",
            "loss: 1.374318  [  192/ 3200]\n",
            "loss: 1.335835  [  193/ 3200]\n",
            "loss: 1.338623  [  194/ 3200]\n",
            "loss: 1.391872  [  195/ 3200]\n",
            "loss: 1.357701  [  196/ 3200]\n",
            "loss: 1.370973  [  197/ 3200]\n",
            "loss: 1.363941  [  198/ 3200]\n",
            "loss: 1.370013  [  199/ 3200]\n",
            "Epoch:  3\n",
            "loss: 1.404252  [    0/ 3200]\n",
            "loss: 1.374460  [    1/ 3200]\n",
            "loss: 1.382133  [    2/ 3200]\n",
            "loss: 1.373165  [    3/ 3200]\n",
            "loss: 1.351665  [    4/ 3200]\n",
            "loss: 1.351528  [    5/ 3200]\n",
            "loss: 1.365677  [    6/ 3200]\n",
            "loss: 1.354939  [    7/ 3200]\n",
            "loss: 1.376863  [    8/ 3200]\n",
            "loss: 1.350754  [    9/ 3200]\n",
            "loss: 1.353001  [   10/ 3200]\n",
            "loss: 1.414107  [   11/ 3200]\n",
            "loss: 1.357086  [   12/ 3200]\n",
            "loss: 1.346513  [   13/ 3200]\n",
            "loss: 1.334339  [   14/ 3200]\n",
            "loss: 1.383854  [   15/ 3200]\n",
            "loss: 1.387475  [   16/ 3200]\n",
            "loss: 1.367489  [   17/ 3200]\n",
            "loss: 1.371190  [   18/ 3200]\n",
            "loss: 1.335051  [   19/ 3200]\n",
            "loss: 1.361856  [   20/ 3200]\n",
            "loss: 1.362199  [   21/ 3200]\n",
            "loss: 1.420577  [   22/ 3200]\n",
            "loss: 1.377659  [   23/ 3200]\n",
            "loss: 1.336154  [   24/ 3200]\n",
            "loss: 1.335749  [   25/ 3200]\n",
            "loss: 1.327291  [   26/ 3200]\n",
            "loss: 1.337510  [   27/ 3200]\n",
            "loss: 1.351940  [   28/ 3200]\n",
            "loss: 1.376816  [   29/ 3200]\n",
            "loss: 1.390026  [   30/ 3200]\n",
            "loss: 1.318841  [   31/ 3200]\n",
            "loss: 1.383614  [   32/ 3200]\n",
            "loss: 1.357922  [   33/ 3200]\n",
            "loss: 1.417552  [   34/ 3200]\n",
            "loss: 1.379205  [   35/ 3200]\n",
            "loss: 1.356005  [   36/ 3200]\n",
            "loss: 1.350353  [   37/ 3200]\n",
            "loss: 1.353408  [   38/ 3200]\n",
            "loss: 1.367956  [   39/ 3200]\n",
            "loss: 1.348852  [   40/ 3200]\n",
            "loss: 1.341843  [   41/ 3200]\n",
            "loss: 1.380715  [   42/ 3200]\n",
            "loss: 1.313655  [   43/ 3200]\n",
            "loss: 1.362771  [   44/ 3200]\n",
            "loss: 1.338192  [   45/ 3200]\n",
            "loss: 1.374294  [   46/ 3200]\n",
            "loss: 1.352715  [   47/ 3200]\n",
            "loss: 1.354931  [   48/ 3200]\n",
            "loss: 1.314479  [   49/ 3200]\n",
            "loss: 1.435473  [   50/ 3200]\n",
            "loss: 1.415568  [   51/ 3200]\n",
            "loss: 1.383356  [   52/ 3200]\n",
            "loss: 1.341997  [   53/ 3200]\n",
            "loss: 1.356735  [   54/ 3200]\n",
            "loss: 1.344021  [   55/ 3200]\n",
            "loss: 1.331186  [   56/ 3200]\n",
            "loss: 1.358338  [   57/ 3200]\n",
            "loss: 1.344290  [   58/ 3200]\n",
            "loss: 1.351579  [   59/ 3200]\n",
            "loss: 1.313690  [   60/ 3200]\n",
            "loss: 1.338507  [   61/ 3200]\n",
            "loss: 1.359320  [   62/ 3200]\n",
            "loss: 1.363608  [   63/ 3200]\n",
            "loss: 1.354331  [   64/ 3200]\n",
            "loss: 1.339607  [   65/ 3200]\n",
            "loss: 1.385963  [   66/ 3200]\n",
            "loss: 1.365761  [   67/ 3200]\n",
            "loss: 1.382181  [   68/ 3200]\n",
            "loss: 1.391176  [   69/ 3200]\n",
            "loss: 1.327684  [   70/ 3200]\n",
            "loss: 1.351070  [   71/ 3200]\n",
            "loss: 1.373136  [   72/ 3200]\n",
            "loss: 1.343370  [   73/ 3200]\n",
            "loss: 1.390101  [   74/ 3200]\n",
            "loss: 1.385132  [   75/ 3200]\n",
            "loss: 1.374923  [   76/ 3200]\n",
            "loss: 1.343252  [   77/ 3200]\n",
            "loss: 1.343004  [   78/ 3200]\n",
            "loss: 1.361501  [   79/ 3200]\n",
            "loss: 1.350691  [   80/ 3200]\n",
            "loss: 1.349077  [   81/ 3200]\n",
            "loss: 1.348570  [   82/ 3200]\n",
            "loss: 1.366870  [   83/ 3200]\n",
            "loss: 1.331324  [   84/ 3200]\n",
            "loss: 1.376618  [   85/ 3200]\n",
            "loss: 1.354145  [   86/ 3200]\n",
            "loss: 1.384287  [   87/ 3200]\n",
            "loss: 1.362138  [   88/ 3200]\n",
            "loss: 1.355870  [   89/ 3200]\n",
            "loss: 1.339463  [   90/ 3200]\n",
            "loss: 1.340850  [   91/ 3200]\n",
            "loss: 1.390682  [   92/ 3200]\n",
            "loss: 1.364669  [   93/ 3200]\n",
            "loss: 1.358379  [   94/ 3200]\n",
            "loss: 1.340231  [   95/ 3200]\n",
            "loss: 1.339770  [   96/ 3200]\n",
            "loss: 1.348513  [   97/ 3200]\n",
            "loss: 1.348513  [   98/ 3200]\n",
            "loss: 1.377218  [   99/ 3200]\n",
            "loss: 1.372475  [  100/ 3200]\n",
            "loss: 1.339333  [  101/ 3200]\n",
            "loss: 1.340135  [  102/ 3200]\n",
            "loss: 1.375450  [  103/ 3200]\n",
            "loss: 1.332820  [  104/ 3200]\n",
            "loss: 1.323310  [  105/ 3200]\n",
            "loss: 1.366873  [  106/ 3200]\n",
            "loss: 1.391190  [  107/ 3200]\n",
            "loss: 1.396476  [  108/ 3200]\n",
            "loss: 1.371639  [  109/ 3200]\n",
            "loss: 1.342673  [  110/ 3200]\n",
            "loss: 1.348937  [  111/ 3200]\n",
            "loss: 1.368685  [  112/ 3200]\n",
            "loss: 1.358101  [  113/ 3200]\n",
            "loss: 1.354965  [  114/ 3200]\n",
            "loss: 1.369264  [  115/ 3200]\n",
            "loss: 1.357535  [  116/ 3200]\n",
            "loss: 1.351934  [  117/ 3200]\n",
            "loss: 1.378276  [  118/ 3200]\n",
            "loss: 1.350469  [  119/ 3200]\n",
            "loss: 1.368069  [  120/ 3200]\n",
            "loss: 1.378008  [  121/ 3200]\n",
            "loss: 1.355260  [  122/ 3200]\n",
            "loss: 1.363472  [  123/ 3200]\n",
            "loss: 1.350690  [  124/ 3200]\n",
            "loss: 1.317958  [  125/ 3200]\n",
            "loss: 1.337372  [  126/ 3200]\n",
            "loss: 1.384077  [  127/ 3200]\n",
            "loss: 1.338617  [  128/ 3200]\n",
            "loss: 1.320432  [  129/ 3200]\n",
            "loss: 1.384617  [  130/ 3200]\n",
            "loss: 1.362686  [  131/ 3200]\n",
            "loss: 1.345413  [  132/ 3200]\n",
            "loss: 1.338800  [  133/ 3200]\n",
            "loss: 1.336266  [  134/ 3200]\n",
            "loss: 1.382050  [  135/ 3200]\n",
            "loss: 1.294111  [  136/ 3200]\n",
            "loss: 1.390895  [  137/ 3200]\n",
            "loss: 1.332005  [  138/ 3200]\n",
            "loss: 1.337713  [  139/ 3200]\n",
            "loss: 1.346159  [  140/ 3200]\n",
            "loss: 1.289207  [  141/ 3200]\n",
            "loss: 1.376953  [  142/ 3200]\n",
            "loss: 1.410829  [  143/ 3200]\n",
            "loss: 1.413973  [  144/ 3200]\n",
            "loss: 1.346651  [  145/ 3200]\n",
            "loss: 1.387073  [  146/ 3200]\n",
            "loss: 1.359898  [  147/ 3200]\n",
            "loss: 1.358888  [  148/ 3200]\n",
            "loss: 1.352405  [  149/ 3200]\n",
            "loss: 1.372291  [  150/ 3200]\n",
            "loss: 1.345014  [  151/ 3200]\n",
            "loss: 1.393302  [  152/ 3200]\n",
            "loss: 1.351768  [  153/ 3200]\n",
            "loss: 1.336409  [  154/ 3200]\n",
            "loss: 1.396162  [  155/ 3200]\n",
            "loss: 1.373693  [  156/ 3200]\n",
            "loss: 1.352751  [  157/ 3200]\n",
            "loss: 1.355538  [  158/ 3200]\n",
            "loss: 1.313824  [  159/ 3200]\n",
            "loss: 1.341579  [  160/ 3200]\n",
            "loss: 1.344828  [  161/ 3200]\n",
            "loss: 1.327790  [  162/ 3200]\n",
            "loss: 1.400266  [  163/ 3200]\n",
            "loss: 1.334679  [  164/ 3200]\n",
            "loss: 1.371927  [  165/ 3200]\n",
            "loss: 1.328378  [  166/ 3200]\n",
            "loss: 1.327773  [  167/ 3200]\n",
            "loss: 1.332577  [  168/ 3200]\n",
            "loss: 1.329147  [  169/ 3200]\n",
            "loss: 1.327055  [  170/ 3200]\n",
            "loss: 1.345749  [  171/ 3200]\n",
            "loss: 1.421604  [  172/ 3200]\n",
            "loss: 1.342766  [  173/ 3200]\n",
            "loss: 1.380282  [  174/ 3200]\n",
            "loss: 1.357581  [  175/ 3200]\n",
            "loss: 1.343836  [  176/ 3200]\n",
            "loss: 1.376641  [  177/ 3200]\n",
            "loss: 1.316143  [  178/ 3200]\n",
            "loss: 1.333108  [  179/ 3200]\n",
            "loss: 1.406929  [  180/ 3200]\n",
            "loss: 1.342984  [  181/ 3200]\n",
            "loss: 1.369607  [  182/ 3200]\n",
            "loss: 1.382618  [  183/ 3200]\n",
            "loss: 1.368941  [  184/ 3200]\n",
            "loss: 1.332765  [  185/ 3200]\n",
            "loss: 1.332713  [  186/ 3200]\n",
            "loss: 1.352511  [  187/ 3200]\n",
            "loss: 1.293642  [  188/ 3200]\n",
            "loss: 1.324888  [  189/ 3200]\n",
            "loss: 1.362576  [  190/ 3200]\n",
            "loss: 1.404677  [  191/ 3200]\n",
            "loss: 1.315414  [  192/ 3200]\n",
            "loss: 1.337469  [  193/ 3200]\n",
            "loss: 1.332075  [  194/ 3200]\n",
            "loss: 1.341118  [  195/ 3200]\n",
            "loss: 1.379010  [  196/ 3200]\n",
            "loss: 1.399892  [  197/ 3200]\n",
            "loss: 1.410735  [  198/ 3200]\n",
            "loss: 1.317051  [  199/ 3200]\n",
            "Epoch:  4\n",
            "loss: 1.344531  [    0/ 3200]\n",
            "loss: 1.351135  [    1/ 3200]\n",
            "loss: 1.376144  [    2/ 3200]\n",
            "loss: 1.341387  [    3/ 3200]\n",
            "loss: 1.355396  [    4/ 3200]\n",
            "loss: 1.319316  [    5/ 3200]\n",
            "loss: 1.380487  [    6/ 3200]\n",
            "loss: 1.367566  [    7/ 3200]\n",
            "loss: 1.366235  [    8/ 3200]\n",
            "loss: 1.341870  [    9/ 3200]\n",
            "loss: 1.300548  [   10/ 3200]\n",
            "loss: 1.380512  [   11/ 3200]\n",
            "loss: 1.381083  [   12/ 3200]\n",
            "loss: 1.335628  [   13/ 3200]\n",
            "loss: 1.340894  [   14/ 3200]\n",
            "loss: 1.336119  [   15/ 3200]\n",
            "loss: 1.335580  [   16/ 3200]\n",
            "loss: 1.375477  [   17/ 3200]\n",
            "loss: 1.359810  [   18/ 3200]\n",
            "loss: 1.358690  [   19/ 3200]\n",
            "loss: 1.325876  [   20/ 3200]\n",
            "loss: 1.349659  [   21/ 3200]\n",
            "loss: 1.374644  [   22/ 3200]\n",
            "loss: 1.369616  [   23/ 3200]\n",
            "loss: 1.365819  [   24/ 3200]\n",
            "loss: 1.332933  [   25/ 3200]\n",
            "loss: 1.321250  [   26/ 3200]\n",
            "loss: 1.353138  [   27/ 3200]\n",
            "loss: 1.333023  [   28/ 3200]\n",
            "loss: 1.400027  [   29/ 3200]\n",
            "loss: 1.365994  [   30/ 3200]\n",
            "loss: 1.354973  [   31/ 3200]\n",
            "loss: 1.325679  [   32/ 3200]\n",
            "loss: 1.376719  [   33/ 3200]\n",
            "loss: 1.330954  [   34/ 3200]\n",
            "loss: 1.362576  [   35/ 3200]\n",
            "loss: 1.345246  [   36/ 3200]\n",
            "loss: 1.342782  [   37/ 3200]\n",
            "loss: 1.340010  [   38/ 3200]\n",
            "loss: 1.343787  [   39/ 3200]\n",
            "loss: 1.347104  [   40/ 3200]\n",
            "loss: 1.333755  [   41/ 3200]\n",
            "loss: 1.375932  [   42/ 3200]\n",
            "loss: 1.335390  [   43/ 3200]\n",
            "loss: 1.356569  [   44/ 3200]\n",
            "loss: 1.295921  [   45/ 3200]\n",
            "loss: 1.366175  [   46/ 3200]\n",
            "loss: 1.378740  [   47/ 3200]\n",
            "loss: 1.332215  [   48/ 3200]\n",
            "loss: 1.323481  [   49/ 3200]\n",
            "loss: 1.351432  [   50/ 3200]\n",
            "loss: 1.307578  [   51/ 3200]\n",
            "loss: 1.363868  [   52/ 3200]\n",
            "loss: 1.345481  [   53/ 3200]\n",
            "loss: 1.293006  [   54/ 3200]\n",
            "loss: 1.364956  [   55/ 3200]\n",
            "loss: 1.420496  [   56/ 3200]\n",
            "loss: 1.299483  [   57/ 3200]\n",
            "loss: 1.282994  [   58/ 3200]\n",
            "loss: 1.318000  [   59/ 3200]\n",
            "loss: 1.310483  [   60/ 3200]\n",
            "loss: 1.414826  [   61/ 3200]\n",
            "loss: 1.352306  [   62/ 3200]\n",
            "loss: 1.333951  [   63/ 3200]\n",
            "loss: 1.386442  [   64/ 3200]\n",
            "loss: 1.318456  [   65/ 3200]\n",
            "loss: 1.371236  [   66/ 3200]\n",
            "loss: 1.354295  [   67/ 3200]\n",
            "loss: 1.361501  [   68/ 3200]\n",
            "loss: 1.358582  [   69/ 3200]\n",
            "loss: 1.363357  [   70/ 3200]\n",
            "loss: 1.339486  [   71/ 3200]\n",
            "loss: 1.323522  [   72/ 3200]\n",
            "loss: 1.352767  [   73/ 3200]\n",
            "loss: 1.388979  [   74/ 3200]\n",
            "loss: 1.340634  [   75/ 3200]\n",
            "loss: 1.343245  [   76/ 3200]\n",
            "loss: 1.351737  [   77/ 3200]\n",
            "loss: 1.385715  [   78/ 3200]\n",
            "loss: 1.388555  [   79/ 3200]\n",
            "loss: 1.379231  [   80/ 3200]\n",
            "loss: 1.392757  [   81/ 3200]\n",
            "loss: 1.340145  [   82/ 3200]\n",
            "loss: 1.338080  [   83/ 3200]\n",
            "loss: 1.350184  [   84/ 3200]\n",
            "loss: 1.361568  [   85/ 3200]\n",
            "loss: 1.355652  [   86/ 3200]\n",
            "loss: 1.311439  [   87/ 3200]\n",
            "loss: 1.338463  [   88/ 3200]\n",
            "loss: 1.397870  [   89/ 3200]\n",
            "loss: 1.371195  [   90/ 3200]\n",
            "loss: 1.343167  [   91/ 3200]\n",
            "loss: 1.370962  [   92/ 3200]\n",
            "loss: 1.335800  [   93/ 3200]\n",
            "loss: 1.339785  [   94/ 3200]\n",
            "loss: 1.355215  [   95/ 3200]\n",
            "loss: 1.370033  [   96/ 3200]\n",
            "loss: 1.329384  [   97/ 3200]\n",
            "loss: 1.371753  [   98/ 3200]\n",
            "loss: 1.362690  [   99/ 3200]\n",
            "loss: 1.311483  [  100/ 3200]\n",
            "loss: 1.323488  [  101/ 3200]\n",
            "loss: 1.353212  [  102/ 3200]\n",
            "loss: 1.343714  [  103/ 3200]\n",
            "loss: 1.342181  [  104/ 3200]\n",
            "loss: 1.352382  [  105/ 3200]\n",
            "loss: 1.335544  [  106/ 3200]\n",
            "loss: 1.365599  [  107/ 3200]\n",
            "loss: 1.356858  [  108/ 3200]\n",
            "loss: 1.347865  [  109/ 3200]\n",
            "loss: 1.324820  [  110/ 3200]\n",
            "loss: 1.328426  [  111/ 3200]\n",
            "loss: 1.329239  [  112/ 3200]\n",
            "loss: 1.356473  [  113/ 3200]\n",
            "loss: 1.338336  [  114/ 3200]\n",
            "loss: 1.339916  [  115/ 3200]\n",
            "loss: 1.335270  [  116/ 3200]\n",
            "loss: 1.327457  [  117/ 3200]\n",
            "loss: 1.365219  [  118/ 3200]\n",
            "loss: 1.343241  [  119/ 3200]\n",
            "loss: 1.346101  [  120/ 3200]\n",
            "loss: 1.330773  [  121/ 3200]\n",
            "loss: 1.348871  [  122/ 3200]\n",
            "loss: 1.381900  [  123/ 3200]\n",
            "loss: 1.334891  [  124/ 3200]\n",
            "loss: 1.366250  [  125/ 3200]\n",
            "loss: 1.347061  [  126/ 3200]\n",
            "loss: 1.328883  [  127/ 3200]\n",
            "loss: 1.359885  [  128/ 3200]\n",
            "loss: 1.364181  [  129/ 3200]\n",
            "loss: 1.336607  [  130/ 3200]\n",
            "loss: 1.332580  [  131/ 3200]\n",
            "loss: 1.315939  [  132/ 3200]\n",
            "loss: 1.326276  [  133/ 3200]\n",
            "loss: 1.351346  [  134/ 3200]\n",
            "loss: 1.326544  [  135/ 3200]\n",
            "loss: 1.337299  [  136/ 3200]\n",
            "loss: 1.336184  [  137/ 3200]\n",
            "loss: 1.347477  [  138/ 3200]\n",
            "loss: 1.328063  [  139/ 3200]\n",
            "loss: 1.328776  [  140/ 3200]\n",
            "loss: 1.332312  [  141/ 3200]\n",
            "loss: 1.345739  [  142/ 3200]\n",
            "loss: 1.353762  [  143/ 3200]\n",
            "loss: 1.347554  [  144/ 3200]\n",
            "loss: 1.391251  [  145/ 3200]\n",
            "loss: 1.350833  [  146/ 3200]\n",
            "loss: 1.337322  [  147/ 3200]\n",
            "loss: 1.352846  [  148/ 3200]\n",
            "loss: 1.361831  [  149/ 3200]\n",
            "loss: 1.345477  [  150/ 3200]\n",
            "loss: 1.356775  [  151/ 3200]\n",
            "loss: 1.338584  [  152/ 3200]\n",
            "loss: 1.363258  [  153/ 3200]\n",
            "loss: 1.339859  [  154/ 3200]\n",
            "loss: 1.324749  [  155/ 3200]\n",
            "loss: 1.318968  [  156/ 3200]\n",
            "loss: 1.321836  [  157/ 3200]\n",
            "loss: 1.346690  [  158/ 3200]\n",
            "loss: 1.368568  [  159/ 3200]\n",
            "loss: 1.345077  [  160/ 3200]\n",
            "loss: 1.362884  [  161/ 3200]\n",
            "loss: 1.349780  [  162/ 3200]\n",
            "loss: 1.290229  [  163/ 3200]\n",
            "loss: 1.320424  [  164/ 3200]\n",
            "loss: 1.314487  [  165/ 3200]\n",
            "loss: 1.449208  [  166/ 3200]\n",
            "loss: 1.293958  [  167/ 3200]\n",
            "loss: 1.377085  [  168/ 3200]\n",
            "loss: 1.386830  [  169/ 3200]\n",
            "loss: 1.368408  [  170/ 3200]\n",
            "loss: 1.359109  [  171/ 3200]\n",
            "loss: 1.376145  [  172/ 3200]\n",
            "loss: 1.360348  [  173/ 3200]\n",
            "loss: 1.374664  [  174/ 3200]\n",
            "loss: 1.319816  [  175/ 3200]\n",
            "loss: 1.342713  [  176/ 3200]\n",
            "loss: 1.353410  [  177/ 3200]\n",
            "loss: 1.331868  [  178/ 3200]\n",
            "loss: 1.303918  [  179/ 3200]\n",
            "loss: 1.350041  [  180/ 3200]\n",
            "loss: 1.321140  [  181/ 3200]\n",
            "loss: 1.320539  [  182/ 3200]\n",
            "loss: 1.338769  [  183/ 3200]\n",
            "loss: 1.349515  [  184/ 3200]\n",
            "loss: 1.335146  [  185/ 3200]\n",
            "loss: 1.337705  [  186/ 3200]\n",
            "loss: 1.293823  [  187/ 3200]\n",
            "loss: 1.359847  [  188/ 3200]\n",
            "loss: 1.376933  [  189/ 3200]\n",
            "loss: 1.311469  [  190/ 3200]\n",
            "loss: 1.322427  [  191/ 3200]\n",
            "loss: 1.319779  [  192/ 3200]\n",
            "loss: 1.296838  [  193/ 3200]\n",
            "loss: 1.407986  [  194/ 3200]\n",
            "loss: 1.321930  [  195/ 3200]\n",
            "loss: 1.397538  [  196/ 3200]\n",
            "loss: 1.354107  [  197/ 3200]\n",
            "loss: 1.319055  [  198/ 3200]\n",
            "loss: 1.310686  [  199/ 3200]\n",
            "Epoch:  5\n",
            "loss: 1.375913  [    0/ 3200]\n",
            "loss: 1.298060  [    1/ 3200]\n",
            "loss: 1.397451  [    2/ 3200]\n",
            "loss: 1.315224  [    3/ 3200]\n",
            "loss: 1.359837  [    4/ 3200]\n",
            "loss: 1.357772  [    5/ 3200]\n",
            "loss: 1.336932  [    6/ 3200]\n",
            "loss: 1.301673  [    7/ 3200]\n",
            "loss: 1.369904  [    8/ 3200]\n",
            "loss: 1.308373  [    9/ 3200]\n",
            "loss: 1.384753  [   10/ 3200]\n",
            "loss: 1.352705  [   11/ 3200]\n",
            "loss: 1.347764  [   12/ 3200]\n",
            "loss: 1.331098  [   13/ 3200]\n",
            "loss: 1.374402  [   14/ 3200]\n",
            "loss: 1.329095  [   15/ 3200]\n",
            "loss: 1.345247  [   16/ 3200]\n",
            "loss: 1.350140  [   17/ 3200]\n",
            "loss: 1.393443  [   18/ 3200]\n",
            "loss: 1.326933  [   19/ 3200]\n",
            "loss: 1.298663  [   20/ 3200]\n",
            "loss: 1.352388  [   21/ 3200]\n",
            "loss: 1.348859  [   22/ 3200]\n",
            "loss: 1.351949  [   23/ 3200]\n",
            "loss: 1.299359  [   24/ 3200]\n",
            "loss: 1.357447  [   25/ 3200]\n",
            "loss: 1.330800  [   26/ 3200]\n",
            "loss: 1.390215  [   27/ 3200]\n",
            "loss: 1.351194  [   28/ 3200]\n",
            "loss: 1.330509  [   29/ 3200]\n",
            "loss: 1.304024  [   30/ 3200]\n",
            "loss: 1.321892  [   31/ 3200]\n",
            "loss: 1.349299  [   32/ 3200]\n",
            "loss: 1.339384  [   33/ 3200]\n",
            "loss: 1.338121  [   34/ 3200]\n",
            "loss: 1.369712  [   35/ 3200]\n",
            "loss: 1.314505  [   36/ 3200]\n",
            "loss: 1.349987  [   37/ 3200]\n",
            "loss: 1.315392  [   38/ 3200]\n",
            "loss: 1.357610  [   39/ 3200]\n",
            "loss: 1.388800  [   40/ 3200]\n",
            "loss: 1.299155  [   41/ 3200]\n",
            "loss: 1.348801  [   42/ 3200]\n",
            "loss: 1.408961  [   43/ 3200]\n",
            "loss: 1.348017  [   44/ 3200]\n",
            "loss: 1.322343  [   45/ 3200]\n",
            "loss: 1.384126  [   46/ 3200]\n",
            "loss: 1.328482  [   47/ 3200]\n",
            "loss: 1.313293  [   48/ 3200]\n",
            "loss: 1.291928  [   49/ 3200]\n",
            "loss: 1.392647  [   50/ 3200]\n",
            "loss: 1.317975  [   51/ 3200]\n",
            "loss: 1.347003  [   52/ 3200]\n",
            "loss: 1.324330  [   53/ 3200]\n",
            "loss: 1.347289  [   54/ 3200]\n",
            "loss: 1.298982  [   55/ 3200]\n",
            "loss: 1.342924  [   56/ 3200]\n",
            "loss: 1.291914  [   57/ 3200]\n",
            "loss: 1.309599  [   58/ 3200]\n",
            "loss: 1.387319  [   59/ 3200]\n",
            "loss: 1.389578  [   60/ 3200]\n",
            "loss: 1.306753  [   61/ 3200]\n",
            "loss: 1.343858  [   62/ 3200]\n",
            "loss: 1.351444  [   63/ 3200]\n",
            "loss: 1.302533  [   64/ 3200]\n",
            "loss: 1.293836  [   65/ 3200]\n",
            "loss: 1.369449  [   66/ 3200]\n",
            "loss: 1.320721  [   67/ 3200]\n",
            "loss: 1.394192  [   68/ 3200]\n",
            "loss: 1.368757  [   69/ 3200]\n",
            "loss: 1.295465  [   70/ 3200]\n",
            "loss: 1.297314  [   71/ 3200]\n",
            "loss: 1.380624  [   72/ 3200]\n",
            "loss: 1.354296  [   73/ 3200]\n",
            "loss: 1.304249  [   74/ 3200]\n",
            "loss: 1.310341  [   75/ 3200]\n",
            "loss: 1.316595  [   76/ 3200]\n",
            "loss: 1.416913  [   77/ 3200]\n",
            "loss: 1.294194  [   78/ 3200]\n",
            "loss: 1.333193  [   79/ 3200]\n",
            "loss: 1.334429  [   80/ 3200]\n",
            "loss: 1.283502  [   81/ 3200]\n",
            "loss: 1.357775  [   82/ 3200]\n",
            "loss: 1.359439  [   83/ 3200]\n",
            "loss: 1.393939  [   84/ 3200]\n",
            "loss: 1.341132  [   85/ 3200]\n",
            "loss: 1.338285  [   86/ 3200]\n",
            "loss: 1.334832  [   87/ 3200]\n",
            "loss: 1.317538  [   88/ 3200]\n",
            "loss: 1.306512  [   89/ 3200]\n",
            "loss: 1.331774  [   90/ 3200]\n",
            "loss: 1.320496  [   91/ 3200]\n",
            "loss: 1.342624  [   92/ 3200]\n",
            "loss: 1.315717  [   93/ 3200]\n",
            "loss: 1.308477  [   94/ 3200]\n",
            "loss: 1.358220  [   95/ 3200]\n",
            "loss: 1.371125  [   96/ 3200]\n",
            "loss: 1.318014  [   97/ 3200]\n",
            "loss: 1.308939  [   98/ 3200]\n",
            "loss: 1.370822  [   99/ 3200]\n",
            "loss: 1.308584  [  100/ 3200]\n",
            "loss: 1.340762  [  101/ 3200]\n",
            "loss: 1.310777  [  102/ 3200]\n",
            "loss: 1.296911  [  103/ 3200]\n",
            "loss: 1.304984  [  104/ 3200]\n",
            "loss: 1.418352  [  105/ 3200]\n",
            "loss: 1.401585  [  106/ 3200]\n",
            "loss: 1.343224  [  107/ 3200]\n",
            "loss: 1.339775  [  108/ 3200]\n",
            "loss: 1.316128  [  109/ 3200]\n",
            "loss: 1.313495  [  110/ 3200]\n",
            "loss: 1.357670  [  111/ 3200]\n",
            "loss: 1.286746  [  112/ 3200]\n",
            "loss: 1.400107  [  113/ 3200]\n",
            "loss: 1.348787  [  114/ 3200]\n",
            "loss: 1.319536  [  115/ 3200]\n",
            "loss: 1.353368  [  116/ 3200]\n",
            "loss: 1.318709  [  117/ 3200]\n",
            "loss: 1.363695  [  118/ 3200]\n",
            "loss: 1.266180  [  119/ 3200]\n",
            "loss: 1.288779  [  120/ 3200]\n",
            "loss: 1.380157  [  121/ 3200]\n",
            "loss: 1.320077  [  122/ 3200]\n",
            "loss: 1.375050  [  123/ 3200]\n",
            "loss: 1.319272  [  124/ 3200]\n",
            "loss: 1.321200  [  125/ 3200]\n",
            "loss: 1.340520  [  126/ 3200]\n",
            "loss: 1.295251  [  127/ 3200]\n",
            "loss: 1.298380  [  128/ 3200]\n",
            "loss: 1.414410  [  129/ 3200]\n",
            "loss: 1.341304  [  130/ 3200]\n",
            "loss: 1.293733  [  131/ 3200]\n",
            "loss: 1.377038  [  132/ 3200]\n",
            "loss: 1.352522  [  133/ 3200]\n",
            "loss: 1.343395  [  134/ 3200]\n",
            "loss: 1.323434  [  135/ 3200]\n",
            "loss: 1.362860  [  136/ 3200]\n",
            "loss: 1.295062  [  137/ 3200]\n",
            "loss: 1.339209  [  138/ 3200]\n",
            "loss: 1.361369  [  139/ 3200]\n",
            "loss: 1.346027  [  140/ 3200]\n",
            "loss: 1.369795  [  141/ 3200]\n",
            "loss: 1.320047  [  142/ 3200]\n",
            "loss: 1.302408  [  143/ 3200]\n",
            "loss: 1.338772  [  144/ 3200]\n",
            "loss: 1.345418  [  145/ 3200]\n",
            "loss: 1.320759  [  146/ 3200]\n",
            "loss: 1.325353  [  147/ 3200]\n",
            "loss: 1.350692  [  148/ 3200]\n",
            "loss: 1.294098  [  149/ 3200]\n",
            "loss: 1.302502  [  150/ 3200]\n",
            "loss: 1.322842  [  151/ 3200]\n",
            "loss: 1.263652  [  152/ 3200]\n",
            "loss: 1.404601  [  153/ 3200]\n",
            "loss: 1.351011  [  154/ 3200]\n",
            "loss: 1.355289  [  155/ 3200]\n",
            "loss: 1.329900  [  156/ 3200]\n",
            "loss: 1.310866  [  157/ 3200]\n",
            "loss: 1.352422  [  158/ 3200]\n",
            "loss: 1.312238  [  159/ 3200]\n",
            "loss: 1.342485  [  160/ 3200]\n",
            "loss: 1.341710  [  161/ 3200]\n",
            "loss: 1.333458  [  162/ 3200]\n",
            "loss: 1.339759  [  163/ 3200]\n",
            "loss: 1.328809  [  164/ 3200]\n",
            "loss: 1.335591  [  165/ 3200]\n",
            "loss: 1.360153  [  166/ 3200]\n",
            "loss: 1.337493  [  167/ 3200]\n",
            "loss: 1.332697  [  168/ 3200]\n",
            "loss: 1.340900  [  169/ 3200]\n",
            "loss: 1.338256  [  170/ 3200]\n",
            "loss: 1.316901  [  171/ 3200]\n",
            "loss: 1.263676  [  172/ 3200]\n",
            "loss: 1.315136  [  173/ 3200]\n",
            "loss: 1.270567  [  174/ 3200]\n",
            "loss: 1.403564  [  175/ 3200]\n",
            "loss: 1.362571  [  176/ 3200]\n",
            "loss: 1.311440  [  177/ 3200]\n",
            "loss: 1.332255  [  178/ 3200]\n",
            "loss: 1.326430  [  179/ 3200]\n",
            "loss: 1.324844  [  180/ 3200]\n",
            "loss: 1.344417  [  181/ 3200]\n",
            "loss: 1.311411  [  182/ 3200]\n",
            "loss: 1.363550  [  183/ 3200]\n",
            "loss: 1.353454  [  184/ 3200]\n",
            "loss: 1.360393  [  185/ 3200]\n",
            "loss: 1.292444  [  186/ 3200]\n",
            "loss: 1.336604  [  187/ 3200]\n",
            "loss: 1.340925  [  188/ 3200]\n",
            "loss: 1.322697  [  189/ 3200]\n",
            "loss: 1.305688  [  190/ 3200]\n",
            "loss: 1.275996  [  191/ 3200]\n",
            "loss: 1.302418  [  192/ 3200]\n",
            "loss: 1.344598  [  193/ 3200]\n",
            "loss: 1.334249  [  194/ 3200]\n",
            "loss: 1.269056  [  195/ 3200]\n",
            "loss: 1.326189  [  196/ 3200]\n",
            "loss: 1.258044  [  197/ 3200]\n",
            "loss: 1.497456  [  198/ 3200]\n",
            "loss: 1.375495  [  199/ 3200]\n",
            "Epoch:  6\n",
            "loss: 1.324084  [    0/ 3200]\n",
            "loss: 1.407946  [    1/ 3200]\n",
            "loss: 1.319764  [    2/ 3200]\n",
            "loss: 1.325495  [    3/ 3200]\n",
            "loss: 1.319698  [    4/ 3200]\n",
            "loss: 1.275738  [    5/ 3200]\n",
            "loss: 1.338798  [    6/ 3200]\n",
            "loss: 1.345976  [    7/ 3200]\n",
            "loss: 1.333389  [    8/ 3200]\n",
            "loss: 1.328533  [    9/ 3200]\n",
            "loss: 1.326861  [   10/ 3200]\n",
            "loss: 1.344689  [   11/ 3200]\n",
            "loss: 1.376865  [   12/ 3200]\n",
            "loss: 1.315682  [   13/ 3200]\n",
            "loss: 1.346906  [   14/ 3200]\n",
            "loss: 1.343565  [   15/ 3200]\n",
            "loss: 1.379620  [   16/ 3200]\n",
            "loss: 1.345657  [   17/ 3200]\n",
            "loss: 1.358014  [   18/ 3200]\n",
            "loss: 1.295846  [   19/ 3200]\n",
            "loss: 1.309584  [   20/ 3200]\n",
            "loss: 1.338160  [   21/ 3200]\n",
            "loss: 1.321065  [   22/ 3200]\n",
            "loss: 1.334136  [   23/ 3200]\n",
            "loss: 1.294813  [   24/ 3200]\n",
            "loss: 1.359172  [   25/ 3200]\n",
            "loss: 1.348067  [   26/ 3200]\n",
            "loss: 1.321570  [   27/ 3200]\n",
            "loss: 1.340118  [   28/ 3200]\n",
            "loss: 1.350089  [   29/ 3200]\n",
            "loss: 1.310847  [   30/ 3200]\n",
            "loss: 1.356587  [   31/ 3200]\n",
            "loss: 1.325105  [   32/ 3200]\n",
            "loss: 1.318925  [   33/ 3200]\n",
            "loss: 1.311713  [   34/ 3200]\n",
            "loss: 1.331348  [   35/ 3200]\n",
            "loss: 1.326667  [   36/ 3200]\n",
            "loss: 1.336724  [   37/ 3200]\n",
            "loss: 1.306974  [   38/ 3200]\n",
            "loss: 1.372981  [   39/ 3200]\n",
            "loss: 1.283481  [   40/ 3200]\n",
            "loss: 1.345778  [   41/ 3200]\n",
            "loss: 1.310425  [   42/ 3200]\n",
            "loss: 1.292779  [   43/ 3200]\n",
            "loss: 1.354459  [   44/ 3200]\n",
            "loss: 1.337123  [   45/ 3200]\n",
            "loss: 1.358660  [   46/ 3200]\n",
            "loss: 1.314828  [   47/ 3200]\n",
            "loss: 1.332749  [   48/ 3200]\n",
            "loss: 1.335562  [   49/ 3200]\n",
            "loss: 1.310315  [   50/ 3200]\n",
            "loss: 1.326914  [   51/ 3200]\n",
            "loss: 1.274369  [   52/ 3200]\n",
            "loss: 1.250319  [   53/ 3200]\n",
            "loss: 1.296525  [   54/ 3200]\n",
            "loss: 1.345067  [   55/ 3200]\n",
            "loss: 1.313502  [   56/ 3200]\n",
            "loss: 1.333565  [   57/ 3200]\n",
            "loss: 1.300357  [   58/ 3200]\n",
            "loss: 1.321404  [   59/ 3200]\n",
            "loss: 1.379566  [   60/ 3200]\n",
            "loss: 1.292460  [   61/ 3200]\n",
            "loss: 1.345425  [   62/ 3200]\n",
            "loss: 1.339699  [   63/ 3200]\n",
            "loss: 1.361584  [   64/ 3200]\n",
            "loss: 1.338605  [   65/ 3200]\n",
            "loss: 1.326688  [   66/ 3200]\n",
            "loss: 1.307667  [   67/ 3200]\n",
            "loss: 1.282526  [   68/ 3200]\n",
            "loss: 1.322039  [   69/ 3200]\n",
            "loss: 1.343538  [   70/ 3200]\n",
            "loss: 1.329365  [   71/ 3200]\n",
            "loss: 1.323156  [   72/ 3200]\n",
            "loss: 1.299947  [   73/ 3200]\n",
            "loss: 1.273052  [   74/ 3200]\n",
            "loss: 1.308791  [   75/ 3200]\n",
            "loss: 1.345909  [   76/ 3200]\n",
            "loss: 1.294116  [   77/ 3200]\n",
            "loss: 1.314698  [   78/ 3200]\n",
            "loss: 1.356389  [   79/ 3200]\n",
            "loss: 1.321910  [   80/ 3200]\n",
            "loss: 1.246579  [   81/ 3200]\n",
            "loss: 1.326915  [   82/ 3200]\n",
            "loss: 1.311975  [   83/ 3200]\n",
            "loss: 1.303114  [   84/ 3200]\n",
            "loss: 1.310215  [   85/ 3200]\n",
            "loss: 1.316962  [   86/ 3200]\n",
            "loss: 1.421369  [   87/ 3200]\n",
            "loss: 1.330157  [   88/ 3200]\n",
            "loss: 1.346200  [   89/ 3200]\n",
            "loss: 1.328252  [   90/ 3200]\n",
            "loss: 1.296440  [   91/ 3200]\n",
            "loss: 1.273962  [   92/ 3200]\n",
            "loss: 1.313604  [   93/ 3200]\n",
            "loss: 1.350596  [   94/ 3200]\n",
            "loss: 1.353800  [   95/ 3200]\n",
            "loss: 1.293062  [   96/ 3200]\n",
            "loss: 1.292236  [   97/ 3200]\n",
            "loss: 1.387606  [   98/ 3200]\n",
            "loss: 1.340737  [   99/ 3200]\n",
            "loss: 1.280327  [  100/ 3200]\n",
            "loss: 1.322195  [  101/ 3200]\n",
            "loss: 1.330038  [  102/ 3200]\n",
            "loss: 1.270475  [  103/ 3200]\n",
            "loss: 1.358722  [  104/ 3200]\n",
            "loss: 1.373256  [  105/ 3200]\n",
            "loss: 1.268031  [  106/ 3200]\n",
            "loss: 1.356044  [  107/ 3200]\n",
            "loss: 1.320941  [  108/ 3200]\n",
            "loss: 1.331826  [  109/ 3200]\n",
            "loss: 1.348890  [  110/ 3200]\n",
            "loss: 1.319909  [  111/ 3200]\n",
            "loss: 1.294713  [  112/ 3200]\n",
            "loss: 1.380212  [  113/ 3200]\n",
            "loss: 1.287262  [  114/ 3200]\n",
            "loss: 1.333404  [  115/ 3200]\n",
            "loss: 1.445301  [  116/ 3200]\n",
            "loss: 1.342522  [  117/ 3200]\n",
            "loss: 1.292302  [  118/ 3200]\n",
            "loss: 1.284828  [  119/ 3200]\n",
            "loss: 1.344009  [  120/ 3200]\n",
            "loss: 1.317753  [  121/ 3200]\n",
            "loss: 1.300283  [  122/ 3200]\n",
            "loss: 1.368504  [  123/ 3200]\n",
            "loss: 1.339380  [  124/ 3200]\n",
            "loss: 1.317120  [  125/ 3200]\n",
            "loss: 1.360794  [  126/ 3200]\n",
            "loss: 1.259406  [  127/ 3200]\n",
            "loss: 1.372251  [  128/ 3200]\n",
            "loss: 1.353881  [  129/ 3200]\n",
            "loss: 1.305699  [  130/ 3200]\n",
            "loss: 1.341699  [  131/ 3200]\n",
            "loss: 1.306476  [  132/ 3200]\n",
            "loss: 1.315510  [  133/ 3200]\n",
            "loss: 1.331582  [  134/ 3200]\n",
            "loss: 1.332449  [  135/ 3200]\n",
            "loss: 1.348781  [  136/ 3200]\n",
            "loss: 1.336632  [  137/ 3200]\n",
            "loss: 1.307464  [  138/ 3200]\n",
            "loss: 1.395280  [  139/ 3200]\n",
            "loss: 1.337118  [  140/ 3200]\n",
            "loss: 1.343893  [  141/ 3200]\n",
            "loss: 1.324661  [  142/ 3200]\n",
            "loss: 1.336571  [  143/ 3200]\n",
            "loss: 1.276829  [  144/ 3200]\n",
            "loss: 1.356157  [  145/ 3200]\n",
            "loss: 1.295663  [  146/ 3200]\n",
            "loss: 1.319442  [  147/ 3200]\n",
            "loss: 1.294866  [  148/ 3200]\n",
            "loss: 1.251535  [  149/ 3200]\n",
            "loss: 1.298965  [  150/ 3200]\n",
            "loss: 1.288189  [  151/ 3200]\n",
            "loss: 1.360675  [  152/ 3200]\n",
            "loss: 1.315436  [  153/ 3200]\n",
            "loss: 1.345475  [  154/ 3200]\n",
            "loss: 1.308312  [  155/ 3200]\n",
            "loss: 1.277835  [  156/ 3200]\n",
            "loss: 1.366454  [  157/ 3200]\n",
            "loss: 1.312379  [  158/ 3200]\n",
            "loss: 1.322278  [  159/ 3200]\n",
            "loss: 1.359511  [  160/ 3200]\n",
            "loss: 1.238975  [  161/ 3200]\n",
            "loss: 1.294821  [  162/ 3200]\n",
            "loss: 1.323068  [  163/ 3200]\n",
            "loss: 1.323806  [  164/ 3200]\n",
            "loss: 1.326356  [  165/ 3200]\n",
            "loss: 1.387216  [  166/ 3200]\n",
            "loss: 1.336246  [  167/ 3200]\n",
            "loss: 1.311630  [  168/ 3200]\n",
            "loss: 1.299070  [  169/ 3200]\n",
            "loss: 1.327627  [  170/ 3200]\n",
            "loss: 1.305972  [  171/ 3200]\n",
            "loss: 1.267870  [  172/ 3200]\n",
            "loss: 1.284395  [  173/ 3200]\n",
            "loss: 1.279762  [  174/ 3200]\n",
            "loss: 1.325259  [  175/ 3200]\n",
            "loss: 1.342013  [  176/ 3200]\n",
            "loss: 1.285374  [  177/ 3200]\n",
            "loss: 1.345610  [  178/ 3200]\n",
            "loss: 1.325048  [  179/ 3200]\n",
            "loss: 1.298158  [  180/ 3200]\n",
            "loss: 1.316827  [  181/ 3200]\n",
            "loss: 1.316661  [  182/ 3200]\n",
            "loss: 1.318446  [  183/ 3200]\n",
            "loss: 1.332884  [  184/ 3200]\n",
            "loss: 1.316329  [  185/ 3200]\n",
            "loss: 1.290766  [  186/ 3200]\n",
            "loss: 1.354550  [  187/ 3200]\n",
            "loss: 1.335723  [  188/ 3200]\n",
            "loss: 1.307050  [  189/ 3200]\n",
            "loss: 1.357429  [  190/ 3200]\n",
            "loss: 1.309513  [  191/ 3200]\n",
            "loss: 1.274420  [  192/ 3200]\n",
            "loss: 1.305666  [  193/ 3200]\n",
            "loss: 1.286217  [  194/ 3200]\n",
            "loss: 1.311470  [  195/ 3200]\n",
            "loss: 1.289022  [  196/ 3200]\n",
            "loss: 1.282442  [  197/ 3200]\n",
            "loss: 1.400297  [  198/ 3200]\n",
            "loss: 1.343256  [  199/ 3200]\n",
            "Epoch:  7\n",
            "loss: 1.384696  [    0/ 3200]\n",
            "loss: 1.378478  [    1/ 3200]\n",
            "loss: 1.302724  [    2/ 3200]\n",
            "loss: 1.291950  [    3/ 3200]\n",
            "loss: 1.301744  [    4/ 3200]\n",
            "loss: 1.322963  [    5/ 3200]\n",
            "loss: 1.296202  [    6/ 3200]\n",
            "loss: 1.271198  [    7/ 3200]\n",
            "loss: 1.317611  [    8/ 3200]\n",
            "loss: 1.303524  [    9/ 3200]\n",
            "loss: 1.306404  [   10/ 3200]\n",
            "loss: 1.330767  [   11/ 3200]\n",
            "loss: 1.323590  [   12/ 3200]\n",
            "loss: 1.335789  [   13/ 3200]\n",
            "loss: 1.349319  [   14/ 3200]\n",
            "loss: 1.315421  [   15/ 3200]\n",
            "loss: 1.373922  [   16/ 3200]\n",
            "loss: 1.287592  [   17/ 3200]\n",
            "loss: 1.309287  [   18/ 3200]\n",
            "loss: 1.289649  [   19/ 3200]\n",
            "loss: 1.293795  [   20/ 3200]\n",
            "loss: 1.347677  [   21/ 3200]\n",
            "loss: 1.213440  [   22/ 3200]\n",
            "loss: 1.354429  [   23/ 3200]\n",
            "loss: 1.303524  [   24/ 3200]\n",
            "loss: 1.349129  [   25/ 3200]\n",
            "loss: 1.347563  [   26/ 3200]\n",
            "loss: 1.338349  [   27/ 3200]\n",
            "loss: 1.303946  [   28/ 3200]\n",
            "loss: 1.292520  [   29/ 3200]\n",
            "loss: 1.326144  [   30/ 3200]\n",
            "loss: 1.347624  [   31/ 3200]\n",
            "loss: 1.298537  [   32/ 3200]\n",
            "loss: 1.291172  [   33/ 3200]\n",
            "loss: 1.282222  [   34/ 3200]\n",
            "loss: 1.286227  [   35/ 3200]\n",
            "loss: 1.405055  [   36/ 3200]\n",
            "loss: 1.296434  [   37/ 3200]\n",
            "loss: 1.347928  [   38/ 3200]\n",
            "loss: 1.336083  [   39/ 3200]\n",
            "loss: 1.275692  [   40/ 3200]\n",
            "loss: 1.282314  [   41/ 3200]\n",
            "loss: 1.370113  [   42/ 3200]\n",
            "loss: 1.289328  [   43/ 3200]\n",
            "loss: 1.361735  [   44/ 3200]\n",
            "loss: 1.348674  [   45/ 3200]\n",
            "loss: 1.313315  [   46/ 3200]\n",
            "loss: 1.330315  [   47/ 3200]\n",
            "loss: 1.326439  [   48/ 3200]\n",
            "loss: 1.317976  [   49/ 3200]\n",
            "loss: 1.305966  [   50/ 3200]\n",
            "loss: 1.229622  [   51/ 3200]\n",
            "loss: 1.393529  [   52/ 3200]\n",
            "loss: 1.352076  [   53/ 3200]\n",
            "loss: 1.338573  [   54/ 3200]\n",
            "loss: 1.314391  [   55/ 3200]\n",
            "loss: 1.295763  [   56/ 3200]\n",
            "loss: 1.305540  [   57/ 3200]\n",
            "loss: 1.360246  [   58/ 3200]\n",
            "loss: 1.332065  [   59/ 3200]\n",
            "loss: 1.292391  [   60/ 3200]\n",
            "loss: 1.326272  [   61/ 3200]\n",
            "loss: 1.288231  [   62/ 3200]\n",
            "loss: 1.320374  [   63/ 3200]\n",
            "loss: 1.346039  [   64/ 3200]\n",
            "loss: 1.273720  [   65/ 3200]\n",
            "loss: 1.313002  [   66/ 3200]\n",
            "loss: 1.311089  [   67/ 3200]\n",
            "loss: 1.324367  [   68/ 3200]\n",
            "loss: 1.300344  [   69/ 3200]\n",
            "loss: 1.340846  [   70/ 3200]\n",
            "loss: 1.349026  [   71/ 3200]\n",
            "loss: 1.282726  [   72/ 3200]\n",
            "loss: 1.375126  [   73/ 3200]\n",
            "loss: 1.355546  [   74/ 3200]\n",
            "loss: 1.325252  [   75/ 3200]\n",
            "loss: 1.303925  [   76/ 3200]\n",
            "loss: 1.330155  [   77/ 3200]\n",
            "loss: 1.264051  [   78/ 3200]\n",
            "loss: 1.299712  [   79/ 3200]\n",
            "loss: 1.300995  [   80/ 3200]\n",
            "loss: 1.285381  [   81/ 3200]\n",
            "loss: 1.294307  [   82/ 3200]\n",
            "loss: 1.305821  [   83/ 3200]\n",
            "loss: 1.264609  [   84/ 3200]\n",
            "loss: 1.319013  [   85/ 3200]\n",
            "loss: 1.294635  [   86/ 3200]\n",
            "loss: 1.327973  [   87/ 3200]\n",
            "loss: 1.273629  [   88/ 3200]\n",
            "loss: 1.317767  [   89/ 3200]\n",
            "loss: 1.378300  [   90/ 3200]\n",
            "loss: 1.271113  [   91/ 3200]\n",
            "loss: 1.317382  [   92/ 3200]\n",
            "loss: 1.298788  [   93/ 3200]\n",
            "loss: 1.285097  [   94/ 3200]\n",
            "loss: 1.311308  [   95/ 3200]\n",
            "loss: 1.342674  [   96/ 3200]\n",
            "loss: 1.243212  [   97/ 3200]\n",
            "loss: 1.318794  [   98/ 3200]\n",
            "loss: 1.299129  [   99/ 3200]\n",
            "loss: 1.277645  [  100/ 3200]\n",
            "loss: 1.319885  [  101/ 3200]\n",
            "loss: 1.332679  [  102/ 3200]\n",
            "loss: 1.328752  [  103/ 3200]\n",
            "loss: 1.381014  [  104/ 3200]\n",
            "loss: 1.325781  [  105/ 3200]\n",
            "loss: 1.270691  [  106/ 3200]\n",
            "loss: 1.330740  [  107/ 3200]\n",
            "loss: 1.288478  [  108/ 3200]\n",
            "loss: 1.303763  [  109/ 3200]\n",
            "loss: 1.306926  [  110/ 3200]\n",
            "loss: 1.279174  [  111/ 3200]\n",
            "loss: 1.259768  [  112/ 3200]\n",
            "loss: 1.269999  [  113/ 3200]\n",
            "loss: 1.312564  [  114/ 3200]\n",
            "loss: 1.299566  [  115/ 3200]\n",
            "loss: 1.419389  [  116/ 3200]\n",
            "loss: 1.286699  [  117/ 3200]\n",
            "loss: 1.318056  [  118/ 3200]\n",
            "loss: 1.318482  [  119/ 3200]\n",
            "loss: 1.265122  [  120/ 3200]\n",
            "loss: 1.276855  [  121/ 3200]\n",
            "loss: 1.304715  [  122/ 3200]\n",
            "loss: 1.312154  [  123/ 3200]\n",
            "loss: 1.303905  [  124/ 3200]\n",
            "loss: 1.310811  [  125/ 3200]\n",
            "loss: 1.287019  [  126/ 3200]\n",
            "loss: 1.295003  [  127/ 3200]\n",
            "loss: 1.305171  [  128/ 3200]\n",
            "loss: 1.312510  [  129/ 3200]\n",
            "loss: 1.303021  [  130/ 3200]\n",
            "loss: 1.292302  [  131/ 3200]\n",
            "loss: 1.296405  [  132/ 3200]\n",
            "loss: 1.295468  [  133/ 3200]\n",
            "loss: 1.280412  [  134/ 3200]\n",
            "loss: 1.335296  [  135/ 3200]\n",
            "loss: 1.329321  [  136/ 3200]\n",
            "loss: 1.263472  [  137/ 3200]\n",
            "loss: 1.292638  [  138/ 3200]\n",
            "loss: 1.336000  [  139/ 3200]\n",
            "loss: 1.320534  [  140/ 3200]\n",
            "loss: 1.219064  [  141/ 3200]\n",
            "loss: 1.245793  [  142/ 3200]\n",
            "loss: 1.331302  [  143/ 3200]\n",
            "loss: 1.338623  [  144/ 3200]\n",
            "loss: 1.307620  [  145/ 3200]\n",
            "loss: 1.262413  [  146/ 3200]\n",
            "loss: 1.301574  [  147/ 3200]\n",
            "loss: 1.332477  [  148/ 3200]\n",
            "loss: 1.304874  [  149/ 3200]\n",
            "loss: 1.318874  [  150/ 3200]\n",
            "loss: 1.318610  [  151/ 3200]\n",
            "loss: 1.337860  [  152/ 3200]\n",
            "loss: 1.310205  [  153/ 3200]\n",
            "loss: 1.284182  [  154/ 3200]\n",
            "loss: 1.305355  [  155/ 3200]\n",
            "loss: 1.303280  [  156/ 3200]\n",
            "loss: 1.269287  [  157/ 3200]\n",
            "loss: 1.310007  [  158/ 3200]\n",
            "loss: 1.335840  [  159/ 3200]\n",
            "loss: 1.316644  [  160/ 3200]\n",
            "loss: 1.300673  [  161/ 3200]\n",
            "loss: 1.343090  [  162/ 3200]\n",
            "loss: 1.282335  [  163/ 3200]\n",
            "loss: 1.320655  [  164/ 3200]\n",
            "loss: 1.387758  [  165/ 3200]\n",
            "loss: 1.306434  [  166/ 3200]\n",
            "loss: 1.333465  [  167/ 3200]\n",
            "loss: 1.271403  [  168/ 3200]\n",
            "loss: 1.329163  [  169/ 3200]\n",
            "loss: 1.326288  [  170/ 3200]\n",
            "loss: 1.341964  [  171/ 3200]\n",
            "loss: 1.310386  [  172/ 3200]\n",
            "loss: 1.268034  [  173/ 3200]\n",
            "loss: 1.339614  [  174/ 3200]\n",
            "loss: 1.300201  [  175/ 3200]\n",
            "loss: 1.305127  [  176/ 3200]\n",
            "loss: 1.284831  [  177/ 3200]\n",
            "loss: 1.296351  [  178/ 3200]\n",
            "loss: 1.294884  [  179/ 3200]\n",
            "loss: 1.281849  [  180/ 3200]\n",
            "loss: 1.338931  [  181/ 3200]\n",
            "loss: 1.257975  [  182/ 3200]\n",
            "loss: 1.254377  [  183/ 3200]\n",
            "loss: 1.332765  [  184/ 3200]\n",
            "loss: 1.346532  [  185/ 3200]\n",
            "loss: 1.329579  [  186/ 3200]\n",
            "loss: 1.341112  [  187/ 3200]\n",
            "loss: 1.296180  [  188/ 3200]\n",
            "loss: 1.299164  [  189/ 3200]\n",
            "loss: 1.301090  [  190/ 3200]\n",
            "loss: 1.209167  [  191/ 3200]\n",
            "loss: 1.320338  [  192/ 3200]\n",
            "loss: 1.317961  [  193/ 3200]\n",
            "loss: 1.337158  [  194/ 3200]\n",
            "loss: 1.271971  [  195/ 3200]\n",
            "loss: 1.340550  [  196/ 3200]\n",
            "loss: 1.327744  [  197/ 3200]\n",
            "loss: 1.351049  [  198/ 3200]\n",
            "loss: 1.263655  [  199/ 3200]\n",
            "Epoch:  8\n",
            "loss: 1.282432  [    0/ 3200]\n",
            "loss: 1.313738  [    1/ 3200]\n",
            "loss: 1.240012  [    2/ 3200]\n",
            "loss: 1.287203  [    3/ 3200]\n",
            "loss: 1.290821  [    4/ 3200]\n",
            "loss: 1.285340  [    5/ 3200]\n",
            "loss: 1.231108  [    6/ 3200]\n",
            "loss: 1.361928  [    7/ 3200]\n",
            "loss: 1.312921  [    8/ 3200]\n",
            "loss: 1.315212  [    9/ 3200]\n",
            "loss: 1.253686  [   10/ 3200]\n",
            "loss: 1.368842  [   11/ 3200]\n",
            "loss: 1.265457  [   12/ 3200]\n",
            "loss: 1.281960  [   13/ 3200]\n",
            "loss: 1.308911  [   14/ 3200]\n",
            "loss: 1.337127  [   15/ 3200]\n",
            "loss: 1.272454  [   16/ 3200]\n",
            "loss: 1.228165  [   17/ 3200]\n",
            "loss: 1.312281  [   18/ 3200]\n",
            "loss: 1.272681  [   19/ 3200]\n",
            "loss: 1.339369  [   20/ 3200]\n",
            "loss: 1.281522  [   21/ 3200]\n",
            "loss: 1.306807  [   22/ 3200]\n",
            "loss: 1.332217  [   23/ 3200]\n",
            "loss: 1.360149  [   24/ 3200]\n",
            "loss: 1.280219  [   25/ 3200]\n",
            "loss: 1.317998  [   26/ 3200]\n",
            "loss: 1.310058  [   27/ 3200]\n",
            "loss: 1.313106  [   28/ 3200]\n",
            "loss: 1.312595  [   29/ 3200]\n",
            "loss: 1.320404  [   30/ 3200]\n",
            "loss: 1.328849  [   31/ 3200]\n",
            "loss: 1.275342  [   32/ 3200]\n",
            "loss: 1.275986  [   33/ 3200]\n",
            "loss: 1.352106  [   34/ 3200]\n",
            "loss: 1.279528  [   35/ 3200]\n",
            "loss: 1.283025  [   36/ 3200]\n",
            "loss: 1.382811  [   37/ 3200]\n",
            "loss: 1.301239  [   38/ 3200]\n",
            "loss: 1.296786  [   39/ 3200]\n",
            "loss: 1.306828  [   40/ 3200]\n",
            "loss: 1.296422  [   41/ 3200]\n",
            "loss: 1.352169  [   42/ 3200]\n",
            "loss: 1.370597  [   43/ 3200]\n",
            "loss: 1.351493  [   44/ 3200]\n",
            "loss: 1.307184  [   45/ 3200]\n",
            "loss: 1.282668  [   46/ 3200]\n",
            "loss: 1.317301  [   47/ 3200]\n",
            "loss: 1.305413  [   48/ 3200]\n",
            "loss: 1.328376  [   49/ 3200]\n",
            "loss: 1.300837  [   50/ 3200]\n",
            "loss: 1.362262  [   51/ 3200]\n",
            "loss: 1.333822  [   52/ 3200]\n",
            "loss: 1.285032  [   53/ 3200]\n",
            "loss: 1.299513  [   54/ 3200]\n",
            "loss: 1.296178  [   55/ 3200]\n",
            "loss: 1.311423  [   56/ 3200]\n",
            "loss: 1.248030  [   57/ 3200]\n",
            "loss: 1.319417  [   58/ 3200]\n",
            "loss: 1.340020  [   59/ 3200]\n",
            "loss: 1.271814  [   60/ 3200]\n",
            "loss: 1.316295  [   61/ 3200]\n",
            "loss: 1.252911  [   62/ 3200]\n",
            "loss: 1.264169  [   63/ 3200]\n",
            "loss: 1.333295  [   64/ 3200]\n",
            "loss: 1.298047  [   65/ 3200]\n",
            "loss: 1.234301  [   66/ 3200]\n",
            "loss: 1.319993  [   67/ 3200]\n",
            "loss: 1.248184  [   68/ 3200]\n",
            "loss: 1.351375  [   69/ 3200]\n",
            "loss: 1.306658  [   70/ 3200]\n",
            "loss: 1.284440  [   71/ 3200]\n",
            "loss: 1.258876  [   72/ 3200]\n",
            "loss: 1.274008  [   73/ 3200]\n",
            "loss: 1.297004  [   74/ 3200]\n",
            "loss: 1.346126  [   75/ 3200]\n",
            "loss: 1.310710  [   76/ 3200]\n",
            "loss: 1.271415  [   77/ 3200]\n",
            "loss: 1.373051  [   78/ 3200]\n",
            "loss: 1.208159  [   79/ 3200]\n",
            "loss: 1.293938  [   80/ 3200]\n",
            "loss: 1.262664  [   81/ 3200]\n",
            "loss: 1.318684  [   82/ 3200]\n",
            "loss: 1.236693  [   83/ 3200]\n",
            "loss: 1.264985  [   84/ 3200]\n",
            "loss: 1.340963  [   85/ 3200]\n",
            "loss: 1.334603  [   86/ 3200]\n",
            "loss: 1.294041  [   87/ 3200]\n",
            "loss: 1.304656  [   88/ 3200]\n",
            "loss: 1.282451  [   89/ 3200]\n",
            "loss: 1.336296  [   90/ 3200]\n",
            "loss: 1.325532  [   91/ 3200]\n",
            "loss: 1.286522  [   92/ 3200]\n",
            "loss: 1.319946  [   93/ 3200]\n",
            "loss: 1.296813  [   94/ 3200]\n",
            "loss: 1.335627  [   95/ 3200]\n",
            "loss: 1.245921  [   96/ 3200]\n",
            "loss: 1.328488  [   97/ 3200]\n",
            "loss: 1.297884  [   98/ 3200]\n",
            "loss: 1.258555  [   99/ 3200]\n",
            "loss: 1.363115  [  100/ 3200]\n",
            "loss: 1.310982  [  101/ 3200]\n",
            "loss: 1.285866  [  102/ 3200]\n",
            "loss: 1.308930  [  103/ 3200]\n",
            "loss: 1.325740  [  104/ 3200]\n",
            "loss: 1.276190  [  105/ 3200]\n",
            "loss: 1.308754  [  106/ 3200]\n",
            "loss: 1.298834  [  107/ 3200]\n",
            "loss: 1.298744  [  108/ 3200]\n",
            "loss: 1.345118  [  109/ 3200]\n",
            "loss: 1.301355  [  110/ 3200]\n",
            "loss: 1.340806  [  111/ 3200]\n",
            "loss: 1.327185  [  112/ 3200]\n",
            "loss: 1.241111  [  113/ 3200]\n",
            "loss: 1.320919  [  114/ 3200]\n",
            "loss: 1.295935  [  115/ 3200]\n",
            "loss: 1.311989  [  116/ 3200]\n",
            "loss: 1.311018  [  117/ 3200]\n",
            "loss: 1.311047  [  118/ 3200]\n",
            "loss: 1.254969  [  119/ 3200]\n",
            "loss: 1.264237  [  120/ 3200]\n",
            "loss: 1.326175  [  121/ 3200]\n",
            "loss: 1.336351  [  122/ 3200]\n",
            "loss: 1.311409  [  123/ 3200]\n",
            "loss: 1.281833  [  124/ 3200]\n",
            "loss: 1.267772  [  125/ 3200]\n",
            "loss: 1.277210  [  126/ 3200]\n",
            "loss: 1.277645  [  127/ 3200]\n",
            "loss: 1.308680  [  128/ 3200]\n",
            "loss: 1.322160  [  129/ 3200]\n",
            "loss: 1.251134  [  130/ 3200]\n",
            "loss: 1.248249  [  131/ 3200]\n",
            "loss: 1.290241  [  132/ 3200]\n",
            "loss: 1.285430  [  133/ 3200]\n",
            "loss: 1.290038  [  134/ 3200]\n",
            "loss: 1.261571  [  135/ 3200]\n",
            "loss: 1.293350  [  136/ 3200]\n",
            "loss: 1.343174  [  137/ 3200]\n",
            "loss: 1.263420  [  138/ 3200]\n",
            "loss: 1.262215  [  139/ 3200]\n",
            "loss: 1.240156  [  140/ 3200]\n",
            "loss: 1.255094  [  141/ 3200]\n",
            "loss: 1.356405  [  142/ 3200]\n",
            "loss: 1.259201  [  143/ 3200]\n",
            "loss: 1.268926  [  144/ 3200]\n",
            "loss: 1.290762  [  145/ 3200]\n",
            "loss: 1.226279  [  146/ 3200]\n",
            "loss: 1.345982  [  147/ 3200]\n",
            "loss: 1.275296  [  148/ 3200]\n",
            "loss: 1.276575  [  149/ 3200]\n",
            "loss: 1.370955  [  150/ 3200]\n",
            "loss: 1.234910  [  151/ 3200]\n",
            "loss: 1.283713  [  152/ 3200]\n",
            "loss: 1.252630  [  153/ 3200]\n",
            "loss: 1.263734  [  154/ 3200]\n",
            "loss: 1.239258  [  155/ 3200]\n",
            "loss: 1.290140  [  156/ 3200]\n",
            "loss: 1.216771  [  157/ 3200]\n",
            "loss: 1.265526  [  158/ 3200]\n",
            "loss: 1.381133  [  159/ 3200]\n",
            "loss: 1.316184  [  160/ 3200]\n",
            "loss: 1.219999  [  161/ 3200]\n",
            "loss: 1.370328  [  162/ 3200]\n",
            "loss: 1.292698  [  163/ 3200]\n",
            "loss: 1.324775  [  164/ 3200]\n",
            "loss: 1.301337  [  165/ 3200]\n",
            "loss: 1.339221  [  166/ 3200]\n",
            "loss: 1.294736  [  167/ 3200]\n",
            "loss: 1.289397  [  168/ 3200]\n",
            "loss: 1.323637  [  169/ 3200]\n",
            "loss: 1.289479  [  170/ 3200]\n",
            "loss: 1.284466  [  171/ 3200]\n",
            "loss: 1.321765  [  172/ 3200]\n",
            "loss: 1.270886  [  173/ 3200]\n",
            "loss: 1.283556  [  174/ 3200]\n",
            "loss: 1.256344  [  175/ 3200]\n",
            "loss: 1.253563  [  176/ 3200]\n",
            "loss: 1.260748  [  177/ 3200]\n",
            "loss: 1.360372  [  178/ 3200]\n",
            "loss: 1.281453  [  179/ 3200]\n",
            "loss: 1.325115  [  180/ 3200]\n",
            "loss: 1.269173  [  181/ 3200]\n",
            "loss: 1.236171  [  182/ 3200]\n",
            "loss: 1.300156  [  183/ 3200]\n",
            "loss: 1.334770  [  184/ 3200]\n",
            "loss: 1.332391  [  185/ 3200]\n",
            "loss: 1.295253  [  186/ 3200]\n",
            "loss: 1.331297  [  187/ 3200]\n",
            "loss: 1.245794  [  188/ 3200]\n",
            "loss: 1.245614  [  189/ 3200]\n",
            "loss: 1.244183  [  190/ 3200]\n",
            "loss: 1.219815  [  191/ 3200]\n",
            "loss: 1.306081  [  192/ 3200]\n",
            "loss: 1.305766  [  193/ 3200]\n",
            "loss: 1.327339  [  194/ 3200]\n",
            "loss: 1.299381  [  195/ 3200]\n",
            "loss: 1.258772  [  196/ 3200]\n",
            "loss: 1.318572  [  197/ 3200]\n",
            "loss: 1.257847  [  198/ 3200]\n",
            "loss: 1.359279  [  199/ 3200]\n",
            "Epoch:  9\n",
            "loss: 1.258261  [    0/ 3200]\n",
            "loss: 1.271026  [    1/ 3200]\n",
            "loss: 1.310010  [    2/ 3200]\n",
            "loss: 1.221659  [    3/ 3200]\n",
            "loss: 1.293098  [    4/ 3200]\n",
            "loss: 1.280001  [    5/ 3200]\n",
            "loss: 1.344582  [    6/ 3200]\n",
            "loss: 1.293678  [    7/ 3200]\n",
            "loss: 1.274284  [    8/ 3200]\n",
            "loss: 1.288962  [    9/ 3200]\n",
            "loss: 1.290914  [   10/ 3200]\n",
            "loss: 1.309332  [   11/ 3200]\n",
            "loss: 1.254448  [   12/ 3200]\n",
            "loss: 1.213601  [   13/ 3200]\n",
            "loss: 1.267488  [   14/ 3200]\n",
            "loss: 1.298050  [   15/ 3200]\n",
            "loss: 1.257159  [   16/ 3200]\n",
            "loss: 1.301874  [   17/ 3200]\n",
            "loss: 1.238679  [   18/ 3200]\n",
            "loss: 1.272306  [   19/ 3200]\n",
            "loss: 1.300241  [   20/ 3200]\n",
            "loss: 1.283928  [   21/ 3200]\n",
            "loss: 1.242401  [   22/ 3200]\n",
            "loss: 1.344746  [   23/ 3200]\n",
            "loss: 1.266535  [   24/ 3200]\n",
            "loss: 1.275666  [   25/ 3200]\n",
            "loss: 1.317511  [   26/ 3200]\n",
            "loss: 1.185946  [   27/ 3200]\n",
            "loss: 1.322659  [   28/ 3200]\n",
            "loss: 1.279266  [   29/ 3200]\n",
            "loss: 1.402606  [   30/ 3200]\n",
            "loss: 1.261548  [   31/ 3200]\n",
            "loss: 1.190653  [   32/ 3200]\n",
            "loss: 1.304816  [   33/ 3200]\n",
            "loss: 1.329562  [   34/ 3200]\n",
            "loss: 1.262395  [   35/ 3200]\n",
            "loss: 1.265750  [   36/ 3200]\n",
            "loss: 1.318240  [   37/ 3200]\n",
            "loss: 1.291870  [   38/ 3200]\n",
            "loss: 1.226903  [   39/ 3200]\n",
            "loss: 1.327353  [   40/ 3200]\n",
            "loss: 1.238374  [   41/ 3200]\n",
            "loss: 1.316839  [   42/ 3200]\n",
            "loss: 1.255237  [   43/ 3200]\n",
            "loss: 1.313468  [   44/ 3200]\n",
            "loss: 1.274818  [   45/ 3200]\n",
            "loss: 1.310460  [   46/ 3200]\n",
            "loss: 1.347386  [   47/ 3200]\n",
            "loss: 1.294836  [   48/ 3200]\n",
            "loss: 1.289599  [   49/ 3200]\n",
            "loss: 1.296763  [   50/ 3200]\n",
            "loss: 1.257233  [   51/ 3200]\n",
            "loss: 1.329550  [   52/ 3200]\n",
            "loss: 1.341999  [   53/ 3200]\n",
            "loss: 1.316757  [   54/ 3200]\n",
            "loss: 1.220745  [   55/ 3200]\n",
            "loss: 1.337155  [   56/ 3200]\n",
            "loss: 1.209072  [   57/ 3200]\n",
            "loss: 1.207904  [   58/ 3200]\n",
            "loss: 1.266568  [   59/ 3200]\n",
            "loss: 1.330599  [   60/ 3200]\n",
            "loss: 1.383417  [   61/ 3200]\n",
            "loss: 1.358186  [   62/ 3200]\n",
            "loss: 1.392557  [   63/ 3200]\n",
            "loss: 1.231318  [   64/ 3200]\n",
            "loss: 1.294266  [   65/ 3200]\n",
            "loss: 1.237525  [   66/ 3200]\n",
            "loss: 1.285452  [   67/ 3200]\n",
            "loss: 1.281916  [   68/ 3200]\n",
            "loss: 1.255944  [   69/ 3200]\n",
            "loss: 1.245824  [   70/ 3200]\n",
            "loss: 1.257442  [   71/ 3200]\n",
            "loss: 1.270598  [   72/ 3200]\n",
            "loss: 1.293788  [   73/ 3200]\n",
            "loss: 1.345634  [   74/ 3200]\n",
            "loss: 1.306462  [   75/ 3200]\n",
            "loss: 1.250985  [   76/ 3200]\n",
            "loss: 1.246497  [   77/ 3200]\n",
            "loss: 1.268285  [   78/ 3200]\n",
            "loss: 1.281200  [   79/ 3200]\n",
            "loss: 1.257811  [   80/ 3200]\n",
            "loss: 1.206668  [   81/ 3200]\n",
            "loss: 1.330049  [   82/ 3200]\n",
            "loss: 1.304667  [   83/ 3200]\n",
            "loss: 1.313537  [   84/ 3200]\n",
            "loss: 1.291258  [   85/ 3200]\n",
            "loss: 1.282516  [   86/ 3200]\n",
            "loss: 1.250262  [   87/ 3200]\n",
            "loss: 1.237118  [   88/ 3200]\n",
            "loss: 1.291613  [   89/ 3200]\n",
            "loss: 1.307844  [   90/ 3200]\n",
            "loss: 1.338148  [   91/ 3200]\n",
            "loss: 1.243073  [   92/ 3200]\n",
            "loss: 1.247953  [   93/ 3200]\n",
            "loss: 1.313074  [   94/ 3200]\n",
            "loss: 1.219962  [   95/ 3200]\n",
            "loss: 1.267605  [   96/ 3200]\n",
            "loss: 1.209287  [   97/ 3200]\n",
            "loss: 1.307008  [   98/ 3200]\n",
            "loss: 1.319767  [   99/ 3200]\n",
            "loss: 1.289254  [  100/ 3200]\n",
            "loss: 1.271241  [  101/ 3200]\n",
            "loss: 1.301255  [  102/ 3200]\n",
            "loss: 1.309397  [  103/ 3200]\n",
            "loss: 1.253245  [  104/ 3200]\n",
            "loss: 1.321621  [  105/ 3200]\n",
            "loss: 1.247633  [  106/ 3200]\n",
            "loss: 1.247666  [  107/ 3200]\n",
            "loss: 1.221791  [  108/ 3200]\n",
            "loss: 1.213605  [  109/ 3200]\n",
            "loss: 1.410028  [  110/ 3200]\n",
            "loss: 1.292910  [  111/ 3200]\n",
            "loss: 1.330971  [  112/ 3200]\n",
            "loss: 1.301531  [  113/ 3200]\n",
            "loss: 1.241127  [  114/ 3200]\n",
            "loss: 1.249505  [  115/ 3200]\n",
            "loss: 1.301792  [  116/ 3200]\n",
            "loss: 1.242587  [  117/ 3200]\n",
            "loss: 1.300993  [  118/ 3200]\n",
            "loss: 1.284085  [  119/ 3200]\n",
            "loss: 1.252649  [  120/ 3200]\n",
            "loss: 1.327044  [  121/ 3200]\n",
            "loss: 1.263022  [  122/ 3200]\n",
            "loss: 1.254546  [  123/ 3200]\n",
            "loss: 1.284945  [  124/ 3200]\n",
            "loss: 1.274905  [  125/ 3200]\n",
            "loss: 1.348161  [  126/ 3200]\n",
            "loss: 1.304224  [  127/ 3200]\n",
            "loss: 1.251552  [  128/ 3200]\n",
            "loss: 1.330495  [  129/ 3200]\n",
            "loss: 1.296684  [  130/ 3200]\n",
            "loss: 1.253656  [  131/ 3200]\n",
            "loss: 1.253535  [  132/ 3200]\n",
            "loss: 1.257315  [  133/ 3200]\n",
            "loss: 1.313747  [  134/ 3200]\n",
            "loss: 1.225816  [  135/ 3200]\n",
            "loss: 1.250167  [  136/ 3200]\n",
            "loss: 1.238141  [  137/ 3200]\n",
            "loss: 1.316974  [  138/ 3200]\n",
            "loss: 1.298225  [  139/ 3200]\n",
            "loss: 1.211211  [  140/ 3200]\n",
            "loss: 1.280428  [  141/ 3200]\n",
            "loss: 1.316921  [  142/ 3200]\n",
            "loss: 1.274986  [  143/ 3200]\n",
            "loss: 1.230451  [  144/ 3200]\n",
            "loss: 1.292676  [  145/ 3200]\n",
            "loss: 1.264772  [  146/ 3200]\n",
            "loss: 1.202270  [  147/ 3200]\n",
            "loss: 1.255369  [  148/ 3200]\n",
            "loss: 1.293622  [  149/ 3200]\n",
            "loss: 1.292342  [  150/ 3200]\n",
            "loss: 1.313086  [  151/ 3200]\n",
            "loss: 1.245867  [  152/ 3200]\n",
            "loss: 1.275004  [  153/ 3200]\n",
            "loss: 1.202071  [  154/ 3200]\n",
            "loss: 1.290484  [  155/ 3200]\n",
            "loss: 1.241359  [  156/ 3200]\n",
            "loss: 1.314164  [  157/ 3200]\n",
            "loss: 1.255368  [  158/ 3200]\n",
            "loss: 1.276684  [  159/ 3200]\n",
            "loss: 1.373605  [  160/ 3200]\n",
            "loss: 1.292036  [  161/ 3200]\n",
            "loss: 1.316552  [  162/ 3200]\n",
            "loss: 1.296005  [  163/ 3200]\n",
            "loss: 1.282380  [  164/ 3200]\n",
            "loss: 1.309351  [  165/ 3200]\n",
            "loss: 1.284532  [  166/ 3200]\n",
            "loss: 1.197069  [  167/ 3200]\n",
            "loss: 1.271419  [  168/ 3200]\n",
            "loss: 1.197967  [  169/ 3200]\n",
            "loss: 1.327015  [  170/ 3200]\n",
            "loss: 1.289520  [  171/ 3200]\n",
            "loss: 1.263285  [  172/ 3200]\n",
            "loss: 1.375690  [  173/ 3200]\n",
            "loss: 1.176957  [  174/ 3200]\n",
            "loss: 1.348500  [  175/ 3200]\n",
            "loss: 1.273122  [  176/ 3200]\n",
            "loss: 1.271578  [  177/ 3200]\n",
            "loss: 1.316302  [  178/ 3200]\n",
            "loss: 1.335812  [  179/ 3200]\n",
            "loss: 1.285685  [  180/ 3200]\n",
            "loss: 1.298184  [  181/ 3200]\n",
            "loss: 1.385084  [  182/ 3200]\n",
            "loss: 1.193640  [  183/ 3200]\n",
            "loss: 1.277015  [  184/ 3200]\n",
            "loss: 1.266014  [  185/ 3200]\n",
            "loss: 1.321649  [  186/ 3200]\n",
            "loss: 1.250285  [  187/ 3200]\n",
            "loss: 1.321347  [  188/ 3200]\n",
            "loss: 1.273136  [  189/ 3200]\n",
            "loss: 1.218037  [  190/ 3200]\n",
            "loss: 1.302333  [  191/ 3200]\n",
            "loss: 1.213445  [  192/ 3200]\n",
            "loss: 1.284393  [  193/ 3200]\n",
            "loss: 1.325153  [  194/ 3200]\n",
            "loss: 1.170199  [  195/ 3200]\n",
            "loss: 1.221044  [  196/ 3200]\n",
            "loss: 1.248714  [  197/ 3200]\n",
            "loss: 1.270228  [  198/ 3200]\n",
            "loss: 1.271682  [  199/ 3200]\n",
            "Epoch:  10\n",
            "loss: 1.270378  [    0/ 3200]\n",
            "loss: 1.336314  [    1/ 3200]\n",
            "loss: 1.261019  [    2/ 3200]\n",
            "loss: 1.250056  [    3/ 3200]\n",
            "loss: 1.288590  [    4/ 3200]\n",
            "loss: 1.263878  [    5/ 3200]\n",
            "loss: 1.265257  [    6/ 3200]\n",
            "loss: 1.260592  [    7/ 3200]\n",
            "loss: 1.292049  [    8/ 3200]\n",
            "loss: 1.198453  [    9/ 3200]\n",
            "loss: 1.236363  [   10/ 3200]\n",
            "loss: 1.254416  [   11/ 3200]\n",
            "loss: 1.223837  [   12/ 3200]\n",
            "loss: 1.286929  [   13/ 3200]\n",
            "loss: 1.137974  [   14/ 3200]\n",
            "loss: 1.206776  [   15/ 3200]\n",
            "loss: 1.292339  [   16/ 3200]\n",
            "loss: 1.383320  [   17/ 3200]\n",
            "loss: 1.248855  [   18/ 3200]\n",
            "loss: 1.234612  [   19/ 3200]\n",
            "loss: 1.269173  [   20/ 3200]\n",
            "loss: 1.264411  [   21/ 3200]\n",
            "loss: 1.258729  [   22/ 3200]\n",
            "loss: 1.347645  [   23/ 3200]\n",
            "loss: 1.183575  [   24/ 3200]\n",
            "loss: 1.284964  [   25/ 3200]\n",
            "loss: 1.256701  [   26/ 3200]\n",
            "loss: 1.270803  [   27/ 3200]\n",
            "loss: 1.171874  [   28/ 3200]\n",
            "loss: 1.292457  [   29/ 3200]\n",
            "loss: 1.210669  [   30/ 3200]\n",
            "loss: 1.235601  [   31/ 3200]\n",
            "loss: 1.218828  [   32/ 3200]\n",
            "loss: 1.299121  [   33/ 3200]\n",
            "loss: 1.269055  [   34/ 3200]\n",
            "loss: 1.310210  [   35/ 3200]\n",
            "loss: 1.235058  [   36/ 3200]\n",
            "loss: 1.339209  [   37/ 3200]\n",
            "loss: 1.288333  [   38/ 3200]\n",
            "loss: 1.205528  [   39/ 3200]\n",
            "loss: 1.249966  [   40/ 3200]\n",
            "loss: 1.208205  [   41/ 3200]\n",
            "loss: 1.211763  [   42/ 3200]\n",
            "loss: 1.124187  [   43/ 3200]\n",
            "loss: 1.304749  [   44/ 3200]\n",
            "loss: 1.264723  [   45/ 3200]\n",
            "loss: 1.139023  [   46/ 3200]\n",
            "loss: 1.275571  [   47/ 3200]\n",
            "loss: 1.203913  [   48/ 3200]\n",
            "loss: 1.379571  [   49/ 3200]\n",
            "loss: 1.212213  [   50/ 3200]\n",
            "loss: 1.259235  [   51/ 3200]\n",
            "loss: 1.228860  [   52/ 3200]\n",
            "loss: 1.277201  [   53/ 3200]\n",
            "loss: 1.308591  [   54/ 3200]\n",
            "loss: 1.211728  [   55/ 3200]\n",
            "loss: 1.274987  [   56/ 3200]\n",
            "loss: 1.239842  [   57/ 3200]\n",
            "loss: 1.257111  [   58/ 3200]\n",
            "loss: 1.164835  [   59/ 3200]\n",
            "loss: 1.334379  [   60/ 3200]\n",
            "loss: 1.267423  [   61/ 3200]\n",
            "loss: 1.249602  [   62/ 3200]\n",
            "loss: 1.206386  [   63/ 3200]\n",
            "loss: 1.274096  [   64/ 3200]\n",
            "loss: 1.244344  [   65/ 3200]\n",
            "loss: 1.239987  [   66/ 3200]\n",
            "loss: 1.252368  [   67/ 3200]\n",
            "loss: 1.343133  [   68/ 3200]\n",
            "loss: 1.222684  [   69/ 3200]\n",
            "loss: 1.362947  [   70/ 3200]\n",
            "loss: 1.273367  [   71/ 3200]\n",
            "loss: 1.266402  [   72/ 3200]\n",
            "loss: 1.261409  [   73/ 3200]\n",
            "loss: 1.282970  [   74/ 3200]\n",
            "loss: 1.267808  [   75/ 3200]\n",
            "loss: 1.245158  [   76/ 3200]\n",
            "loss: 1.261592  [   77/ 3200]\n",
            "loss: 1.315324  [   78/ 3200]\n",
            "loss: 1.266007  [   79/ 3200]\n",
            "loss: 1.345183  [   80/ 3200]\n",
            "loss: 1.255325  [   81/ 3200]\n",
            "loss: 1.261530  [   82/ 3200]\n",
            "loss: 1.298180  [   83/ 3200]\n",
            "loss: 1.292030  [   84/ 3200]\n",
            "loss: 1.277616  [   85/ 3200]\n",
            "loss: 1.268338  [   86/ 3200]\n",
            "loss: 1.212618  [   87/ 3200]\n",
            "loss: 1.222786  [   88/ 3200]\n",
            "loss: 1.217882  [   89/ 3200]\n",
            "loss: 1.279754  [   90/ 3200]\n",
            "loss: 1.286550  [   91/ 3200]\n",
            "loss: 1.223886  [   92/ 3200]\n",
            "loss: 1.274908  [   93/ 3200]\n",
            "loss: 1.329170  [   94/ 3200]\n",
            "loss: 1.313826  [   95/ 3200]\n",
            "loss: 1.217732  [   96/ 3200]\n",
            "loss: 1.245492  [   97/ 3200]\n",
            "loss: 1.314758  [   98/ 3200]\n",
            "loss: 1.211073  [   99/ 3200]\n",
            "loss: 1.157361  [  100/ 3200]\n",
            "loss: 1.288714  [  101/ 3200]\n",
            "loss: 1.233030  [  102/ 3200]\n",
            "loss: 1.167261  [  103/ 3200]\n",
            "loss: 1.343006  [  104/ 3200]\n",
            "loss: 1.277678  [  105/ 3200]\n",
            "loss: 1.266628  [  106/ 3200]\n",
            "loss: 1.254831  [  107/ 3200]\n",
            "loss: 1.318195  [  108/ 3200]\n",
            "loss: 1.247442  [  109/ 3200]\n",
            "loss: 1.303318  [  110/ 3200]\n",
            "loss: 1.225679  [  111/ 3200]\n",
            "loss: 1.283637  [  112/ 3200]\n",
            "loss: 1.272379  [  113/ 3200]\n",
            "loss: 1.178502  [  114/ 3200]\n",
            "loss: 1.330747  [  115/ 3200]\n",
            "loss: 1.361464  [  116/ 3200]\n",
            "loss: 1.284811  [  117/ 3200]\n",
            "loss: 1.234232  [  118/ 3200]\n",
            "loss: 1.298138  [  119/ 3200]\n",
            "loss: 1.273445  [  120/ 3200]\n",
            "loss: 1.252344  [  121/ 3200]\n",
            "loss: 1.317879  [  122/ 3200]\n",
            "loss: 1.254897  [  123/ 3200]\n",
            "loss: 1.247428  [  124/ 3200]\n",
            "loss: 1.226444  [  125/ 3200]\n",
            "loss: 1.294189  [  126/ 3200]\n",
            "loss: 1.233719  [  127/ 3200]\n",
            "loss: 1.273009  [  128/ 3200]\n",
            "loss: 1.257577  [  129/ 3200]\n",
            "loss: 1.259276  [  130/ 3200]\n",
            "loss: 1.331664  [  131/ 3200]\n",
            "loss: 1.226713  [  132/ 3200]\n",
            "loss: 1.269670  [  133/ 3200]\n",
            "loss: 1.195466  [  134/ 3200]\n",
            "loss: 1.310552  [  135/ 3200]\n",
            "loss: 1.214103  [  136/ 3200]\n",
            "loss: 1.243730  [  137/ 3200]\n",
            "loss: 1.210599  [  138/ 3200]\n",
            "loss: 1.318597  [  139/ 3200]\n",
            "loss: 1.232555  [  140/ 3200]\n",
            "loss: 1.244588  [  141/ 3200]\n",
            "loss: 1.305469  [  142/ 3200]\n",
            "loss: 1.284941  [  143/ 3200]\n",
            "loss: 1.236279  [  144/ 3200]\n",
            "loss: 1.277912  [  145/ 3200]\n",
            "loss: 1.186177  [  146/ 3200]\n",
            "loss: 1.251772  [  147/ 3200]\n",
            "loss: 1.210339  [  148/ 3200]\n",
            "loss: 1.293344  [  149/ 3200]\n",
            "loss: 1.270405  [  150/ 3200]\n",
            "loss: 1.183815  [  151/ 3200]\n",
            "loss: 1.337697  [  152/ 3200]\n",
            "loss: 1.272735  [  153/ 3200]\n",
            "loss: 1.159681  [  154/ 3200]\n",
            "loss: 1.305051  [  155/ 3200]\n",
            "loss: 1.250429  [  156/ 3200]\n",
            "loss: 1.263673  [  157/ 3200]\n",
            "loss: 1.290485  [  158/ 3200]\n",
            "loss: 1.275493  [  159/ 3200]\n",
            "loss: 1.312319  [  160/ 3200]\n",
            "loss: 1.225172  [  161/ 3200]\n",
            "loss: 1.278385  [  162/ 3200]\n",
            "loss: 1.297519  [  163/ 3200]\n",
            "loss: 1.315082  [  164/ 3200]\n",
            "loss: 1.227520  [  165/ 3200]\n",
            "loss: 1.240999  [  166/ 3200]\n",
            "loss: 1.258034  [  167/ 3200]\n",
            "loss: 1.223830  [  168/ 3200]\n",
            "loss: 1.313834  [  169/ 3200]\n",
            "loss: 1.245701  [  170/ 3200]\n",
            "loss: 1.231687  [  171/ 3200]\n",
            "loss: 1.275747  [  172/ 3200]\n",
            "loss: 1.208189  [  173/ 3200]\n",
            "loss: 1.238667  [  174/ 3200]\n",
            "loss: 1.286468  [  175/ 3200]\n",
            "loss: 1.241479  [  176/ 3200]\n",
            "loss: 1.131902  [  177/ 3200]\n",
            "loss: 1.265682  [  178/ 3200]\n",
            "loss: 1.259635  [  179/ 3200]\n",
            "loss: 1.237713  [  180/ 3200]\n",
            "loss: 1.265826  [  181/ 3200]\n",
            "loss: 1.254824  [  182/ 3200]\n",
            "loss: 1.253024  [  183/ 3200]\n",
            "loss: 1.325942  [  184/ 3200]\n",
            "loss: 1.289713  [  185/ 3200]\n",
            "loss: 1.301369  [  186/ 3200]\n",
            "loss: 1.238622  [  187/ 3200]\n",
            "loss: 1.248466  [  188/ 3200]\n",
            "loss: 1.197872  [  189/ 3200]\n",
            "loss: 1.350929  [  190/ 3200]\n",
            "loss: 1.247325  [  191/ 3200]\n",
            "loss: 1.211964  [  192/ 3200]\n",
            "loss: 1.232376  [  193/ 3200]\n",
            "loss: 1.273888  [  194/ 3200]\n",
            "loss: 1.334278  [  195/ 3200]\n",
            "loss: 1.295447  [  196/ 3200]\n",
            "loss: 1.193590  [  197/ 3200]\n",
            "loss: 1.276909  [  198/ 3200]\n",
            "loss: 1.310281  [  199/ 3200]\n",
            "Epoch:  11\n",
            "loss: 1.260243  [    0/ 3200]\n",
            "loss: 1.203202  [    1/ 3200]\n",
            "loss: 1.261653  [    2/ 3200]\n",
            "loss: 1.268472  [    3/ 3200]\n",
            "loss: 1.215776  [    4/ 3200]\n",
            "loss: 1.445304  [    5/ 3200]\n",
            "loss: 1.224017  [    6/ 3200]\n",
            "loss: 1.292675  [    7/ 3200]\n",
            "loss: 1.214459  [    8/ 3200]\n",
            "loss: 1.290323  [    9/ 3200]\n",
            "loss: 1.303769  [   10/ 3200]\n",
            "loss: 1.293887  [   11/ 3200]\n",
            "loss: 1.284532  [   12/ 3200]\n",
            "loss: 1.237218  [   13/ 3200]\n",
            "loss: 1.231148  [   14/ 3200]\n",
            "loss: 1.260161  [   15/ 3200]\n",
            "loss: 1.228270  [   16/ 3200]\n",
            "loss: 1.129958  [   17/ 3200]\n",
            "loss: 1.183528  [   18/ 3200]\n",
            "loss: 1.502275  [   19/ 3200]\n",
            "loss: 1.331372  [   20/ 3200]\n",
            "loss: 1.344873  [   21/ 3200]\n",
            "loss: 1.271160  [   22/ 3200]\n",
            "loss: 1.161271  [   23/ 3200]\n",
            "loss: 1.233351  [   24/ 3200]\n",
            "loss: 1.269488  [   25/ 3200]\n",
            "loss: 1.239692  [   26/ 3200]\n",
            "loss: 1.284064  [   27/ 3200]\n",
            "loss: 1.176879  [   28/ 3200]\n",
            "loss: 1.268580  [   29/ 3200]\n",
            "loss: 1.182057  [   30/ 3200]\n",
            "loss: 1.262112  [   31/ 3200]\n",
            "loss: 1.173186  [   32/ 3200]\n",
            "loss: 1.295247  [   33/ 3200]\n",
            "loss: 1.308998  [   34/ 3200]\n",
            "loss: 1.239020  [   35/ 3200]\n",
            "loss: 1.310494  [   36/ 3200]\n",
            "loss: 1.304133  [   37/ 3200]\n",
            "loss: 1.188581  [   38/ 3200]\n",
            "loss: 1.222347  [   39/ 3200]\n",
            "loss: 1.253689  [   40/ 3200]\n",
            "loss: 1.294574  [   41/ 3200]\n",
            "loss: 1.203737  [   42/ 3200]\n",
            "loss: 1.231687  [   43/ 3200]\n",
            "loss: 1.214204  [   44/ 3200]\n",
            "loss: 1.297125  [   45/ 3200]\n",
            "loss: 1.226621  [   46/ 3200]\n",
            "loss: 1.242617  [   47/ 3200]\n",
            "loss: 1.268422  [   48/ 3200]\n",
            "loss: 1.214167  [   49/ 3200]\n",
            "loss: 1.193866  [   50/ 3200]\n",
            "loss: 1.250737  [   51/ 3200]\n",
            "loss: 1.260555  [   52/ 3200]\n",
            "loss: 1.294847  [   53/ 3200]\n",
            "loss: 1.193390  [   54/ 3200]\n",
            "loss: 1.322674  [   55/ 3200]\n",
            "loss: 1.161206  [   56/ 3200]\n",
            "loss: 1.267974  [   57/ 3200]\n",
            "loss: 1.232448  [   58/ 3200]\n",
            "loss: 1.205493  [   59/ 3200]\n",
            "loss: 1.248045  [   60/ 3200]\n",
            "loss: 1.289572  [   61/ 3200]\n",
            "loss: 1.200854  [   62/ 3200]\n",
            "loss: 1.171068  [   63/ 3200]\n",
            "loss: 1.191463  [   64/ 3200]\n",
            "loss: 1.250868  [   65/ 3200]\n",
            "loss: 1.251789  [   66/ 3200]\n",
            "loss: 1.266543  [   67/ 3200]\n",
            "loss: 1.296691  [   68/ 3200]\n",
            "loss: 1.183542  [   69/ 3200]\n",
            "loss: 1.327864  [   70/ 3200]\n",
            "loss: 1.248484  [   71/ 3200]\n",
            "loss: 1.289240  [   72/ 3200]\n",
            "loss: 1.172924  [   73/ 3200]\n",
            "loss: 1.184486  [   74/ 3200]\n",
            "loss: 1.268009  [   75/ 3200]\n",
            "loss: 1.139295  [   76/ 3200]\n",
            "loss: 1.235811  [   77/ 3200]\n",
            "loss: 1.243653  [   78/ 3200]\n",
            "loss: 1.260945  [   79/ 3200]\n",
            "loss: 1.160389  [   80/ 3200]\n",
            "loss: 1.211139  [   81/ 3200]\n",
            "loss: 1.276973  [   82/ 3200]\n",
            "loss: 1.254876  [   83/ 3200]\n",
            "loss: 1.343288  [   84/ 3200]\n",
            "loss: 1.234057  [   85/ 3200]\n",
            "loss: 1.178174  [   86/ 3200]\n",
            "loss: 1.280298  [   87/ 3200]\n",
            "loss: 1.289371  [   88/ 3200]\n",
            "loss: 1.283588  [   89/ 3200]\n",
            "loss: 1.189746  [   90/ 3200]\n",
            "loss: 1.205980  [   91/ 3200]\n",
            "loss: 1.220939  [   92/ 3200]\n",
            "loss: 1.251279  [   93/ 3200]\n",
            "loss: 1.239212  [   94/ 3200]\n",
            "loss: 1.243798  [   95/ 3200]\n",
            "loss: 1.197230  [   96/ 3200]\n",
            "loss: 1.238594  [   97/ 3200]\n",
            "loss: 1.255434  [   98/ 3200]\n",
            "loss: 1.317586  [   99/ 3200]\n",
            "loss: 1.213288  [  100/ 3200]\n",
            "loss: 1.226769  [  101/ 3200]\n",
            "loss: 1.248455  [  102/ 3200]\n",
            "loss: 1.177937  [  103/ 3200]\n",
            "loss: 1.227673  [  104/ 3200]\n",
            "loss: 1.284045  [  105/ 3200]\n",
            "loss: 1.261626  [  106/ 3200]\n",
            "loss: 1.334267  [  107/ 3200]\n",
            "loss: 1.231380  [  108/ 3200]\n",
            "loss: 1.282039  [  109/ 3200]\n",
            "loss: 1.222466  [  110/ 3200]\n",
            "loss: 1.250384  [  111/ 3200]\n",
            "loss: 1.302903  [  112/ 3200]\n",
            "loss: 1.212840  [  113/ 3200]\n",
            "loss: 1.190579  [  114/ 3200]\n",
            "loss: 1.380234  [  115/ 3200]\n",
            "loss: 1.229520  [  116/ 3200]\n",
            "loss: 1.239704  [  117/ 3200]\n",
            "loss: 1.221032  [  118/ 3200]\n",
            "loss: 1.200743  [  119/ 3200]\n",
            "loss: 1.276301  [  120/ 3200]\n",
            "loss: 1.237703  [  121/ 3200]\n",
            "loss: 1.289301  [  122/ 3200]\n",
            "loss: 1.237576  [  123/ 3200]\n",
            "loss: 1.248367  [  124/ 3200]\n",
            "loss: 1.240810  [  125/ 3200]\n",
            "loss: 1.109929  [  126/ 3200]\n",
            "loss: 1.242484  [  127/ 3200]\n",
            "loss: 1.233523  [  128/ 3200]\n",
            "loss: 1.275504  [  129/ 3200]\n",
            "loss: 1.291494  [  130/ 3200]\n",
            "loss: 1.317345  [  131/ 3200]\n",
            "loss: 1.210039  [  132/ 3200]\n",
            "loss: 1.219048  [  133/ 3200]\n",
            "loss: 1.144593  [  134/ 3200]\n",
            "loss: 1.323896  [  135/ 3200]\n",
            "loss: 1.291596  [  136/ 3200]\n",
            "loss: 1.263295  [  137/ 3200]\n",
            "loss: 1.262672  [  138/ 3200]\n",
            "loss: 1.276875  [  139/ 3200]\n",
            "loss: 1.213496  [  140/ 3200]\n",
            "loss: 1.212367  [  141/ 3200]\n",
            "loss: 1.241840  [  142/ 3200]\n",
            "loss: 1.230419  [  143/ 3200]\n",
            "loss: 1.097935  [  144/ 3200]\n",
            "loss: 1.227982  [  145/ 3200]\n",
            "loss: 1.298633  [  146/ 3200]\n",
            "loss: 1.163260  [  147/ 3200]\n",
            "loss: 1.207211  [  148/ 3200]\n",
            "loss: 1.194489  [  149/ 3200]\n",
            "loss: 1.232966  [  150/ 3200]\n",
            "loss: 1.167589  [  151/ 3200]\n",
            "loss: 1.255342  [  152/ 3200]\n",
            "loss: 1.270309  [  153/ 3200]\n",
            "loss: 1.115827  [  154/ 3200]\n",
            "loss: 1.240171  [  155/ 3200]\n",
            "loss: 1.346173  [  156/ 3200]\n",
            "loss: 1.244487  [  157/ 3200]\n",
            "loss: 1.181626  [  158/ 3200]\n",
            "loss: 1.307304  [  159/ 3200]\n",
            "loss: 1.185752  [  160/ 3200]\n",
            "loss: 1.206182  [  161/ 3200]\n",
            "loss: 1.270147  [  162/ 3200]\n",
            "loss: 1.243119  [  163/ 3200]\n",
            "loss: 1.243195  [  164/ 3200]\n",
            "loss: 1.266861  [  165/ 3200]\n",
            "loss: 1.251052  [  166/ 3200]\n",
            "loss: 1.276851  [  167/ 3200]\n",
            "loss: 1.192639  [  168/ 3200]\n",
            "loss: 1.230678  [  169/ 3200]\n",
            "loss: 1.217958  [  170/ 3200]\n",
            "loss: 1.230420  [  171/ 3200]\n",
            "loss: 1.246710  [  172/ 3200]\n",
            "loss: 1.357706  [  173/ 3200]\n",
            "loss: 1.243796  [  174/ 3200]\n",
            "loss: 1.346876  [  175/ 3200]\n",
            "loss: 1.180361  [  176/ 3200]\n",
            "loss: 1.255085  [  177/ 3200]\n",
            "loss: 1.184194  [  178/ 3200]\n",
            "loss: 1.273757  [  179/ 3200]\n",
            "loss: 1.231515  [  180/ 3200]\n",
            "loss: 1.254622  [  181/ 3200]\n",
            "loss: 1.319121  [  182/ 3200]\n",
            "loss: 1.256418  [  183/ 3200]\n",
            "loss: 1.262441  [  184/ 3200]\n",
            "loss: 1.357378  [  185/ 3200]\n",
            "loss: 1.246678  [  186/ 3200]\n",
            "loss: 1.270094  [  187/ 3200]\n",
            "loss: 1.217632  [  188/ 3200]\n",
            "loss: 1.225335  [  189/ 3200]\n",
            "loss: 1.284413  [  190/ 3200]\n",
            "loss: 1.232619  [  191/ 3200]\n",
            "loss: 1.216315  [  192/ 3200]\n",
            "loss: 1.284492  [  193/ 3200]\n",
            "loss: 1.236199  [  194/ 3200]\n",
            "loss: 1.194724  [  195/ 3200]\n",
            "loss: 1.222177  [  196/ 3200]\n",
            "loss: 1.194902  [  197/ 3200]\n",
            "loss: 1.110247  [  198/ 3200]\n",
            "loss: 1.109962  [  199/ 3200]\n",
            "Epoch:  12\n",
            "loss: 1.228900  [    0/ 3200]\n",
            "loss: 1.303996  [    1/ 3200]\n",
            "loss: 1.271127  [    2/ 3200]\n",
            "loss: 1.260367  [    3/ 3200]\n",
            "loss: 1.235533  [    4/ 3200]\n",
            "loss: 1.196700  [    5/ 3200]\n",
            "loss: 1.322818  [    6/ 3200]\n",
            "loss: 1.263698  [    7/ 3200]\n",
            "loss: 1.347662  [    8/ 3200]\n",
            "loss: 1.251033  [    9/ 3200]\n",
            "loss: 1.099829  [   10/ 3200]\n",
            "loss: 1.175165  [   11/ 3200]\n",
            "loss: 1.224172  [   12/ 3200]\n",
            "loss: 1.308873  [   13/ 3200]\n",
            "loss: 1.236689  [   14/ 3200]\n",
            "loss: 1.203013  [   15/ 3200]\n",
            "loss: 1.196952  [   16/ 3200]\n",
            "loss: 1.177615  [   17/ 3200]\n",
            "loss: 1.241406  [   18/ 3200]\n",
            "loss: 1.241611  [   19/ 3200]\n",
            "loss: 1.219269  [   20/ 3200]\n",
            "loss: 1.201440  [   21/ 3200]\n",
            "loss: 1.257071  [   22/ 3200]\n",
            "loss: 1.267731  [   23/ 3200]\n",
            "loss: 1.244668  [   24/ 3200]\n",
            "loss: 1.252809  [   25/ 3200]\n",
            "loss: 1.275368  [   26/ 3200]\n",
            "loss: 1.265398  [   27/ 3200]\n",
            "loss: 1.308367  [   28/ 3200]\n",
            "loss: 1.254833  [   29/ 3200]\n",
            "loss: 1.102014  [   30/ 3200]\n",
            "loss: 1.223586  [   31/ 3200]\n",
            "loss: 1.202932  [   32/ 3200]\n",
            "loss: 1.239822  [   33/ 3200]\n",
            "loss: 1.275369  [   34/ 3200]\n",
            "loss: 1.330421  [   35/ 3200]\n",
            "loss: 1.161869  [   36/ 3200]\n",
            "loss: 1.169169  [   37/ 3200]\n",
            "loss: 1.236471  [   38/ 3200]\n",
            "loss: 1.223569  [   39/ 3200]\n",
            "loss: 1.171228  [   40/ 3200]\n",
            "loss: 1.047069  [   41/ 3200]\n",
            "loss: 1.281695  [   42/ 3200]\n",
            "loss: 1.227312  [   43/ 3200]\n",
            "loss: 1.243475  [   44/ 3200]\n",
            "loss: 1.160532  [   45/ 3200]\n",
            "loss: 1.327733  [   46/ 3200]\n",
            "loss: 1.259028  [   47/ 3200]\n",
            "loss: 1.220693  [   48/ 3200]\n",
            "loss: 1.243732  [   49/ 3200]\n",
            "loss: 1.203574  [   50/ 3200]\n",
            "loss: 1.282456  [   51/ 3200]\n",
            "loss: 1.263238  [   52/ 3200]\n",
            "loss: 1.263280  [   53/ 3200]\n",
            "loss: 1.299508  [   54/ 3200]\n",
            "loss: 1.265045  [   55/ 3200]\n",
            "loss: 1.217011  [   56/ 3200]\n",
            "loss: 1.207569  [   57/ 3200]\n",
            "loss: 1.250391  [   58/ 3200]\n",
            "loss: 1.098675  [   59/ 3200]\n",
            "loss: 1.285687  [   60/ 3200]\n",
            "loss: 1.231660  [   61/ 3200]\n",
            "loss: 1.233764  [   62/ 3200]\n",
            "loss: 1.208626  [   63/ 3200]\n",
            "loss: 1.302001  [   64/ 3200]\n",
            "loss: 1.249749  [   65/ 3200]\n",
            "loss: 1.180688  [   66/ 3200]\n",
            "loss: 1.187101  [   67/ 3200]\n",
            "loss: 1.247271  [   68/ 3200]\n",
            "loss: 1.250751  [   69/ 3200]\n",
            "loss: 1.199922  [   70/ 3200]\n",
            "loss: 1.151221  [   71/ 3200]\n",
            "loss: 1.266689  [   72/ 3200]\n",
            "loss: 1.215327  [   73/ 3200]\n",
            "loss: 1.073518  [   74/ 3200]\n",
            "loss: 1.190186  [   75/ 3200]\n",
            "loss: 1.382290  [   76/ 3200]\n",
            "loss: 1.126963  [   77/ 3200]\n",
            "loss: 1.162734  [   78/ 3200]\n",
            "loss: 1.147079  [   79/ 3200]\n",
            "loss: 1.337440  [   80/ 3200]\n",
            "loss: 1.203977  [   81/ 3200]\n",
            "loss: 1.292683  [   82/ 3200]\n",
            "loss: 1.204428  [   83/ 3200]\n",
            "loss: 1.378525  [   84/ 3200]\n",
            "loss: 1.206309  [   85/ 3200]\n",
            "loss: 1.282309  [   86/ 3200]\n",
            "loss: 1.285815  [   87/ 3200]\n",
            "loss: 1.264562  [   88/ 3200]\n",
            "loss: 1.177464  [   89/ 3200]\n",
            "loss: 1.170246  [   90/ 3200]\n",
            "loss: 1.227760  [   91/ 3200]\n",
            "loss: 1.258401  [   92/ 3200]\n",
            "loss: 1.235696  [   93/ 3200]\n",
            "loss: 1.273198  [   94/ 3200]\n",
            "loss: 1.244894  [   95/ 3200]\n",
            "loss: 1.120672  [   96/ 3200]\n",
            "loss: 1.245360  [   97/ 3200]\n",
            "loss: 1.227720  [   98/ 3200]\n",
            "loss: 1.127100  [   99/ 3200]\n",
            "loss: 1.231476  [  100/ 3200]\n",
            "loss: 1.210607  [  101/ 3200]\n",
            "loss: 1.202228  [  102/ 3200]\n",
            "loss: 1.173752  [  103/ 3200]\n",
            "loss: 1.311749  [  104/ 3200]\n",
            "loss: 1.295311  [  105/ 3200]\n",
            "loss: 1.118316  [  106/ 3200]\n",
            "loss: 1.200773  [  107/ 3200]\n",
            "loss: 1.253188  [  108/ 3200]\n",
            "loss: 1.186631  [  109/ 3200]\n",
            "loss: 1.093709  [  110/ 3200]\n",
            "loss: 1.360995  [  111/ 3200]\n",
            "loss: 1.178265  [  112/ 3200]\n",
            "loss: 1.288731  [  113/ 3200]\n",
            "loss: 1.213639  [  114/ 3200]\n",
            "loss: 1.151898  [  115/ 3200]\n",
            "loss: 1.203120  [  116/ 3200]\n",
            "loss: 1.053367  [  117/ 3200]\n",
            "loss: 1.278398  [  118/ 3200]\n",
            "loss: 1.255097  [  119/ 3200]\n",
            "loss: 1.284946  [  120/ 3200]\n",
            "loss: 1.177038  [  121/ 3200]\n",
            "loss: 1.248200  [  122/ 3200]\n",
            "loss: 1.286706  [  123/ 3200]\n",
            "loss: 1.378992  [  124/ 3200]\n",
            "loss: 1.156965  [  125/ 3200]\n",
            "loss: 1.160431  [  126/ 3200]\n",
            "loss: 1.256008  [  127/ 3200]\n",
            "loss: 1.215700  [  128/ 3200]\n",
            "loss: 1.138088  [  129/ 3200]\n",
            "loss: 1.209367  [  130/ 3200]\n",
            "loss: 1.221739  [  131/ 3200]\n",
            "loss: 1.234729  [  132/ 3200]\n",
            "loss: 1.228661  [  133/ 3200]\n",
            "loss: 1.231046  [  134/ 3200]\n",
            "loss: 1.268002  [  135/ 3200]\n",
            "loss: 1.191383  [  136/ 3200]\n",
            "loss: 1.272567  [  137/ 3200]\n",
            "loss: 1.171404  [  138/ 3200]\n",
            "loss: 1.219934  [  139/ 3200]\n",
            "loss: 1.152700  [  140/ 3200]\n",
            "loss: 1.304870  [  141/ 3200]\n",
            "loss: 1.210809  [  142/ 3200]\n",
            "loss: 1.125857  [  143/ 3200]\n",
            "loss: 1.329135  [  144/ 3200]\n",
            "loss: 1.149933  [  145/ 3200]\n",
            "loss: 1.245046  [  146/ 3200]\n",
            "loss: 1.200822  [  147/ 3200]\n",
            "loss: 1.131411  [  148/ 3200]\n",
            "loss: 1.206314  [  149/ 3200]\n",
            "loss: 1.223925  [  150/ 3200]\n",
            "loss: 1.205101  [  151/ 3200]\n",
            "loss: 1.259390  [  152/ 3200]\n",
            "loss: 1.276886  [  153/ 3200]\n",
            "loss: 1.163274  [  154/ 3200]\n",
            "loss: 1.148163  [  155/ 3200]\n",
            "loss: 1.174000  [  156/ 3200]\n",
            "loss: 1.161962  [  157/ 3200]\n",
            "loss: 1.326822  [  158/ 3200]\n",
            "loss: 1.260821  [  159/ 3200]\n",
            "loss: 1.196162  [  160/ 3200]\n",
            "loss: 1.306399  [  161/ 3200]\n",
            "loss: 1.169912  [  162/ 3200]\n",
            "loss: 1.251807  [  163/ 3200]\n",
            "loss: 1.094686  [  164/ 3200]\n",
            "loss: 1.183279  [  165/ 3200]\n",
            "loss: 1.205516  [  166/ 3200]\n",
            "loss: 1.181523  [  167/ 3200]\n",
            "loss: 1.183397  [  168/ 3200]\n",
            "loss: 1.210821  [  169/ 3200]\n",
            "loss: 1.144729  [  170/ 3200]\n",
            "loss: 1.203859  [  171/ 3200]\n",
            "loss: 1.211464  [  172/ 3200]\n",
            "loss: 1.161949  [  173/ 3200]\n",
            "loss: 1.112387  [  174/ 3200]\n",
            "loss: 1.162973  [  175/ 3200]\n",
            "loss: 1.169407  [  176/ 3200]\n",
            "loss: 1.291550  [  177/ 3200]\n",
            "loss: 1.252981  [  178/ 3200]\n",
            "loss: 1.158736  [  179/ 3200]\n",
            "loss: 1.204338  [  180/ 3200]\n",
            "loss: 1.274376  [  181/ 3200]\n",
            "loss: 1.208500  [  182/ 3200]\n",
            "loss: 1.264785  [  183/ 3200]\n",
            "loss: 1.334039  [  184/ 3200]\n",
            "loss: 1.256559  [  185/ 3200]\n",
            "loss: 1.099394  [  186/ 3200]\n",
            "loss: 1.209874  [  187/ 3200]\n",
            "loss: 1.254786  [  188/ 3200]\n",
            "loss: 1.183117  [  189/ 3200]\n",
            "loss: 1.117057  [  190/ 3200]\n",
            "loss: 1.173915  [  191/ 3200]\n",
            "loss: 1.276630  [  192/ 3200]\n",
            "loss: 1.212215  [  193/ 3200]\n",
            "loss: 1.223138  [  194/ 3200]\n",
            "loss: 1.285960  [  195/ 3200]\n",
            "loss: 1.319650  [  196/ 3200]\n",
            "loss: 1.167106  [  197/ 3200]\n",
            "loss: 1.172217  [  198/ 3200]\n",
            "loss: 1.198140  [  199/ 3200]\n",
            "Epoch:  13\n",
            "loss: 1.201560  [    0/ 3200]\n",
            "loss: 1.192407  [    1/ 3200]\n",
            "loss: 1.138884  [    2/ 3200]\n",
            "loss: 0.976143  [    3/ 3200]\n",
            "loss: 1.211533  [    4/ 3200]\n",
            "loss: 1.221109  [    5/ 3200]\n",
            "loss: 1.244485  [    6/ 3200]\n",
            "loss: 1.165084  [    7/ 3200]\n",
            "loss: 1.247281  [    8/ 3200]\n",
            "loss: 1.199758  [    9/ 3200]\n",
            "loss: 1.268273  [   10/ 3200]\n",
            "loss: 1.252144  [   11/ 3200]\n",
            "loss: 1.149021  [   12/ 3200]\n",
            "loss: 1.216474  [   13/ 3200]\n",
            "loss: 1.281177  [   14/ 3200]\n",
            "loss: 1.133843  [   15/ 3200]\n",
            "loss: 1.098503  [   16/ 3200]\n",
            "loss: 1.179485  [   17/ 3200]\n",
            "loss: 1.276765  [   18/ 3200]\n",
            "loss: 1.060968  [   19/ 3200]\n",
            "loss: 1.202099  [   20/ 3200]\n",
            "loss: 1.204491  [   21/ 3200]\n",
            "loss: 1.151397  [   22/ 3200]\n",
            "loss: 1.245850  [   23/ 3200]\n",
            "loss: 1.212943  [   24/ 3200]\n",
            "loss: 1.147036  [   25/ 3200]\n",
            "loss: 1.286981  [   26/ 3200]\n",
            "loss: 1.200918  [   27/ 3200]\n",
            "loss: 1.114887  [   28/ 3200]\n",
            "loss: 1.283286  [   29/ 3200]\n",
            "loss: 1.149845  [   30/ 3200]\n",
            "loss: 1.207510  [   31/ 3200]\n",
            "loss: 1.242577  [   32/ 3200]\n",
            "loss: 1.192454  [   33/ 3200]\n",
            "loss: 1.300601  [   34/ 3200]\n",
            "loss: 1.171150  [   35/ 3200]\n",
            "loss: 1.140840  [   36/ 3200]\n",
            "loss: 1.174714  [   37/ 3200]\n",
            "loss: 1.284695  [   38/ 3200]\n",
            "loss: 1.046580  [   39/ 3200]\n",
            "loss: 1.250003  [   40/ 3200]\n",
            "loss: 1.366456  [   41/ 3200]\n",
            "loss: 1.267408  [   42/ 3200]\n",
            "loss: 1.279043  [   43/ 3200]\n",
            "loss: 1.212583  [   44/ 3200]\n",
            "loss: 1.165036  [   45/ 3200]\n",
            "loss: 1.271868  [   46/ 3200]\n",
            "loss: 1.198504  [   47/ 3200]\n",
            "loss: 1.263938  [   48/ 3200]\n",
            "loss: 1.225057  [   49/ 3200]\n",
            "loss: 1.200315  [   50/ 3200]\n",
            "loss: 1.213493  [   51/ 3200]\n",
            "loss: 1.108275  [   52/ 3200]\n",
            "loss: 1.084467  [   53/ 3200]\n",
            "loss: 1.180918  [   54/ 3200]\n",
            "loss: 1.292610  [   55/ 3200]\n",
            "loss: 1.137595  [   56/ 3200]\n",
            "loss: 1.177130  [   57/ 3200]\n",
            "loss: 1.205358  [   58/ 3200]\n",
            "loss: 1.260315  [   59/ 3200]\n",
            "loss: 1.218554  [   60/ 3200]\n",
            "loss: 1.217766  [   61/ 3200]\n",
            "loss: 1.207273  [   62/ 3200]\n",
            "loss: 1.290413  [   63/ 3200]\n",
            "loss: 1.178007  [   64/ 3200]\n",
            "loss: 1.031484  [   65/ 3200]\n",
            "loss: 1.223715  [   66/ 3200]\n",
            "loss: 1.190472  [   67/ 3200]\n",
            "loss: 1.233019  [   68/ 3200]\n",
            "loss: 1.109260  [   69/ 3200]\n",
            "loss: 1.240463  [   70/ 3200]\n",
            "loss: 1.194113  [   71/ 3200]\n",
            "loss: 1.121384  [   72/ 3200]\n",
            "loss: 1.249970  [   73/ 3200]\n",
            "loss: 1.228094  [   74/ 3200]\n",
            "loss: 1.192481  [   75/ 3200]\n",
            "loss: 1.278226  [   76/ 3200]\n",
            "loss: 1.256369  [   77/ 3200]\n",
            "loss: 1.261712  [   78/ 3200]\n",
            "loss: 1.230906  [   79/ 3200]\n",
            "loss: 1.135114  [   80/ 3200]\n",
            "loss: 1.092505  [   81/ 3200]\n",
            "loss: 1.099124  [   82/ 3200]\n",
            "loss: 0.983610  [   83/ 3200]\n",
            "loss: 1.294618  [   84/ 3200]\n",
            "loss: 1.165726  [   85/ 3200]\n",
            "loss: 1.241605  [   86/ 3200]\n",
            "loss: 1.177499  [   87/ 3200]\n",
            "loss: 1.198360  [   88/ 3200]\n",
            "loss: 1.130642  [   89/ 3200]\n",
            "loss: 1.231874  [   90/ 3200]\n",
            "loss: 1.173309  [   91/ 3200]\n",
            "loss: 1.215533  [   92/ 3200]\n",
            "loss: 1.062696  [   93/ 3200]\n",
            "loss: 1.222970  [   94/ 3200]\n",
            "loss: 1.239140  [   95/ 3200]\n",
            "loss: 1.273246  [   96/ 3200]\n",
            "loss: 1.136389  [   97/ 3200]\n",
            "loss: 1.239910  [   98/ 3200]\n",
            "loss: 1.118862  [   99/ 3200]\n",
            "loss: 1.232849  [  100/ 3200]\n",
            "loss: 1.271357  [  101/ 3200]\n",
            "loss: 1.235295  [  102/ 3200]\n",
            "loss: 1.139064  [  103/ 3200]\n",
            "loss: 1.179343  [  104/ 3200]\n",
            "loss: 1.134204  [  105/ 3200]\n",
            "loss: 1.246539  [  106/ 3200]\n",
            "loss: 1.196490  [  107/ 3200]\n",
            "loss: 1.195123  [  108/ 3200]\n",
            "loss: 1.072569  [  109/ 3200]\n",
            "loss: 1.272900  [  110/ 3200]\n",
            "loss: 1.112117  [  111/ 3200]\n",
            "loss: 1.111510  [  112/ 3200]\n",
            "loss: 1.310229  [  113/ 3200]\n",
            "loss: 1.257272  [  114/ 3200]\n",
            "loss: 1.211928  [  115/ 3200]\n",
            "loss: 1.293263  [  116/ 3200]\n",
            "loss: 1.416658  [  117/ 3200]\n",
            "loss: 1.184614  [  118/ 3200]\n",
            "loss: 1.134014  [  119/ 3200]\n",
            "loss: 1.390785  [  120/ 3200]\n",
            "loss: 1.364958  [  121/ 3200]\n",
            "loss: 1.235958  [  122/ 3200]\n",
            "loss: 1.150037  [  123/ 3200]\n",
            "loss: 1.202082  [  124/ 3200]\n",
            "loss: 1.191709  [  125/ 3200]\n",
            "loss: 1.076065  [  126/ 3200]\n",
            "loss: 1.191352  [  127/ 3200]\n",
            "loss: 1.232103  [  128/ 3200]\n",
            "loss: 1.232349  [  129/ 3200]\n",
            "loss: 1.244117  [  130/ 3200]\n",
            "loss: 1.156676  [  131/ 3200]\n",
            "loss: 1.231959  [  132/ 3200]\n",
            "loss: 1.140586  [  133/ 3200]\n",
            "loss: 1.165821  [  134/ 3200]\n",
            "loss: 1.294163  [  135/ 3200]\n",
            "loss: 1.140191  [  136/ 3200]\n",
            "loss: 1.110644  [  137/ 3200]\n",
            "loss: 1.082065  [  138/ 3200]\n",
            "loss: 1.180157  [  139/ 3200]\n",
            "loss: 1.251105  [  140/ 3200]\n",
            "loss: 1.126241  [  141/ 3200]\n",
            "loss: 1.256012  [  142/ 3200]\n",
            "loss: 1.156240  [  143/ 3200]\n",
            "loss: 1.209280  [  144/ 3200]\n",
            "loss: 1.142921  [  145/ 3200]\n",
            "loss: 1.419344  [  146/ 3200]\n",
            "loss: 1.162913  [  147/ 3200]\n",
            "loss: 1.282823  [  148/ 3200]\n",
            "loss: 1.281307  [  149/ 3200]\n",
            "loss: 1.301677  [  150/ 3200]\n",
            "loss: 1.133461  [  151/ 3200]\n",
            "loss: 1.145155  [  152/ 3200]\n",
            "loss: 1.155885  [  153/ 3200]\n",
            "loss: 1.241891  [  154/ 3200]\n",
            "loss: 1.177115  [  155/ 3200]\n",
            "loss: 1.293727  [  156/ 3200]\n",
            "loss: 1.305998  [  157/ 3200]\n",
            "loss: 1.192529  [  158/ 3200]\n",
            "loss: 1.184371  [  159/ 3200]\n",
            "loss: 1.124325  [  160/ 3200]\n",
            "loss: 1.235008  [  161/ 3200]\n",
            "loss: 1.207785  [  162/ 3200]\n",
            "loss: 1.228312  [  163/ 3200]\n",
            "loss: 1.277977  [  164/ 3200]\n",
            "loss: 1.215761  [  165/ 3200]\n",
            "loss: 1.208003  [  166/ 3200]\n",
            "loss: 1.276568  [  167/ 3200]\n",
            "loss: 1.096311  [  168/ 3200]\n",
            "loss: 1.196525  [  169/ 3200]\n",
            "loss: 1.231383  [  170/ 3200]\n",
            "loss: 1.170564  [  171/ 3200]\n",
            "loss: 1.065155  [  172/ 3200]\n",
            "loss: 1.151248  [  173/ 3200]\n",
            "loss: 1.232554  [  174/ 3200]\n",
            "loss: 1.131141  [  175/ 3200]\n",
            "loss: 1.097244  [  176/ 3200]\n",
            "loss: 1.259348  [  177/ 3200]\n",
            "loss: 1.164953  [  178/ 3200]\n",
            "loss: 1.191371  [  179/ 3200]\n",
            "loss: 1.067249  [  180/ 3200]\n",
            "loss: 1.203720  [  181/ 3200]\n",
            "loss: 1.243222  [  182/ 3200]\n",
            "loss: 1.230676  [  183/ 3200]\n",
            "loss: 1.209562  [  184/ 3200]\n",
            "loss: 1.147999  [  185/ 3200]\n",
            "loss: 1.271505  [  186/ 3200]\n",
            "loss: 1.363281  [  187/ 3200]\n",
            "loss: 1.107183  [  188/ 3200]\n",
            "loss: 1.122533  [  189/ 3200]\n",
            "loss: 1.213040  [  190/ 3200]\n",
            "loss: 1.123454  [  191/ 3200]\n",
            "loss: 1.222085  [  192/ 3200]\n",
            "loss: 1.149919  [  193/ 3200]\n",
            "loss: 1.281511  [  194/ 3200]\n",
            "loss: 1.177803  [  195/ 3200]\n",
            "loss: 1.215648  [  196/ 3200]\n",
            "loss: 1.156991  [  197/ 3200]\n",
            "loss: 1.261000  [  198/ 3200]\n",
            "loss: 1.075088  [  199/ 3200]\n",
            "Epoch:  14\n",
            "loss: 1.222940  [    0/ 3200]\n",
            "loss: 1.071866  [    1/ 3200]\n",
            "loss: 1.110044  [    2/ 3200]\n",
            "loss: 1.245409  [    3/ 3200]\n",
            "loss: 1.053404  [    4/ 3200]\n",
            "loss: 1.216810  [    5/ 3200]\n",
            "loss: 1.305734  [    6/ 3200]\n",
            "loss: 1.275838  [    7/ 3200]\n",
            "loss: 1.290154  [    8/ 3200]\n",
            "loss: 1.153300  [    9/ 3200]\n",
            "loss: 1.158332  [   10/ 3200]\n",
            "loss: 1.180538  [   11/ 3200]\n",
            "loss: 1.251019  [   12/ 3200]\n",
            "loss: 1.140286  [   13/ 3200]\n",
            "loss: 1.341415  [   14/ 3200]\n",
            "loss: 1.119902  [   15/ 3200]\n",
            "loss: 1.247316  [   16/ 3200]\n",
            "loss: 1.221335  [   17/ 3200]\n",
            "loss: 1.124440  [   18/ 3200]\n",
            "loss: 1.351861  [   19/ 3200]\n",
            "loss: 1.191763  [   20/ 3200]\n",
            "loss: 1.085472  [   21/ 3200]\n",
            "loss: 1.172343  [   22/ 3200]\n",
            "loss: 1.162121  [   23/ 3200]\n",
            "loss: 1.220937  [   24/ 3200]\n",
            "loss: 1.164855  [   25/ 3200]\n",
            "loss: 1.161316  [   26/ 3200]\n",
            "loss: 1.356565  [   27/ 3200]\n",
            "loss: 1.159287  [   28/ 3200]\n",
            "loss: 1.231576  [   29/ 3200]\n",
            "loss: 1.205003  [   30/ 3200]\n",
            "loss: 1.118131  [   31/ 3200]\n",
            "loss: 1.102905  [   32/ 3200]\n",
            "loss: 1.312209  [   33/ 3200]\n",
            "loss: 1.006880  [   34/ 3200]\n",
            "loss: 1.230585  [   35/ 3200]\n",
            "loss: 1.153309  [   36/ 3200]\n",
            "loss: 1.204661  [   37/ 3200]\n",
            "loss: 1.085143  [   38/ 3200]\n",
            "loss: 1.079228  [   39/ 3200]\n",
            "loss: 1.163787  [   40/ 3200]\n",
            "loss: 1.077807  [   41/ 3200]\n",
            "loss: 1.306191  [   42/ 3200]\n",
            "loss: 1.224550  [   43/ 3200]\n",
            "loss: 1.216436  [   44/ 3200]\n",
            "loss: 1.185154  [   45/ 3200]\n",
            "loss: 1.018770  [   46/ 3200]\n",
            "loss: 1.179968  [   47/ 3200]\n",
            "loss: 1.215850  [   48/ 3200]\n",
            "loss: 1.246561  [   49/ 3200]\n",
            "loss: 1.220307  [   50/ 3200]\n",
            "loss: 1.202064  [   51/ 3200]\n",
            "loss: 1.224072  [   52/ 3200]\n",
            "loss: 1.190380  [   53/ 3200]\n",
            "loss: 1.288723  [   54/ 3200]\n",
            "loss: 1.056122  [   55/ 3200]\n",
            "loss: 1.252168  [   56/ 3200]\n",
            "loss: 1.177409  [   57/ 3200]\n",
            "loss: 1.195785  [   58/ 3200]\n",
            "loss: 1.151579  [   59/ 3200]\n",
            "loss: 1.193735  [   60/ 3200]\n",
            "loss: 1.131421  [   61/ 3200]\n",
            "loss: 1.245105  [   62/ 3200]\n",
            "loss: 1.029614  [   63/ 3200]\n",
            "loss: 1.124242  [   64/ 3200]\n",
            "loss: 1.355621  [   65/ 3200]\n",
            "loss: 1.134350  [   66/ 3200]\n",
            "loss: 1.178981  [   67/ 3200]\n",
            "loss: 1.178701  [   68/ 3200]\n",
            "loss: 1.311778  [   69/ 3200]\n",
            "loss: 1.023778  [   70/ 3200]\n",
            "loss: 1.177946  [   71/ 3200]\n",
            "loss: 1.198102  [   72/ 3200]\n",
            "loss: 1.243968  [   73/ 3200]\n",
            "loss: 1.325463  [   74/ 3200]\n",
            "loss: 1.105862  [   75/ 3200]\n",
            "loss: 1.110571  [   76/ 3200]\n",
            "loss: 1.208524  [   77/ 3200]\n",
            "loss: 1.210479  [   78/ 3200]\n",
            "loss: 1.138660  [   79/ 3200]\n",
            "loss: 1.198514  [   80/ 3200]\n",
            "loss: 1.293018  [   81/ 3200]\n",
            "loss: 1.266928  [   82/ 3200]\n",
            "loss: 0.888819  [   83/ 3200]\n",
            "loss: 1.132153  [   84/ 3200]\n",
            "loss: 1.155131  [   85/ 3200]\n",
            "loss: 1.216720  [   86/ 3200]\n",
            "loss: 1.239795  [   87/ 3200]\n",
            "loss: 1.170062  [   88/ 3200]\n",
            "loss: 1.199525  [   89/ 3200]\n",
            "loss: 1.069225  [   90/ 3200]\n",
            "loss: 1.200726  [   91/ 3200]\n",
            "loss: 1.194585  [   92/ 3200]\n",
            "loss: 1.160854  [   93/ 3200]\n",
            "loss: 1.255218  [   94/ 3200]\n",
            "loss: 1.200362  [   95/ 3200]\n",
            "loss: 1.122224  [   96/ 3200]\n",
            "loss: 1.250213  [   97/ 3200]\n",
            "loss: 1.254740  [   98/ 3200]\n",
            "loss: 1.108674  [   99/ 3200]\n",
            "loss: 1.221800  [  100/ 3200]\n",
            "loss: 1.316808  [  101/ 3200]\n",
            "loss: 1.193376  [  102/ 3200]\n",
            "loss: 1.308345  [  103/ 3200]\n",
            "loss: 0.973056  [  104/ 3200]\n",
            "loss: 1.145844  [  105/ 3200]\n",
            "loss: 1.167350  [  106/ 3200]\n",
            "loss: 1.123312  [  107/ 3200]\n",
            "loss: 1.234559  [  108/ 3200]\n",
            "loss: 1.156400  [  109/ 3200]\n",
            "loss: 1.172404  [  110/ 3200]\n",
            "loss: 1.081436  [  111/ 3200]\n",
            "loss: 1.094080  [  112/ 3200]\n",
            "loss: 1.276502  [  113/ 3200]\n",
            "loss: 1.059033  [  114/ 3200]\n",
            "loss: 1.146981  [  115/ 3200]\n",
            "loss: 1.279659  [  116/ 3200]\n",
            "loss: 1.282795  [  117/ 3200]\n",
            "loss: 1.134834  [  118/ 3200]\n",
            "loss: 1.271214  [  119/ 3200]\n",
            "loss: 1.284555  [  120/ 3200]\n",
            "loss: 1.267045  [  121/ 3200]\n",
            "loss: 1.094496  [  122/ 3200]\n",
            "loss: 1.226641  [  123/ 3200]\n",
            "loss: 1.178025  [  124/ 3200]\n",
            "loss: 1.241836  [  125/ 3200]\n",
            "loss: 1.132037  [  126/ 3200]\n",
            "loss: 1.218415  [  127/ 3200]\n",
            "loss: 1.141010  [  128/ 3200]\n",
            "loss: 1.192044  [  129/ 3200]\n",
            "loss: 1.183549  [  130/ 3200]\n",
            "loss: 1.061911  [  131/ 3200]\n",
            "loss: 1.236154  [  132/ 3200]\n",
            "loss: 1.193920  [  133/ 3200]\n",
            "loss: 1.170375  [  134/ 3200]\n",
            "loss: 1.252928  [  135/ 3200]\n",
            "loss: 1.142282  [  136/ 3200]\n",
            "loss: 1.268835  [  137/ 3200]\n",
            "loss: 1.164749  [  138/ 3200]\n",
            "loss: 1.277188  [  139/ 3200]\n",
            "loss: 1.323357  [  140/ 3200]\n",
            "loss: 1.184037  [  141/ 3200]\n",
            "loss: 1.154553  [  142/ 3200]\n",
            "loss: 1.254654  [  143/ 3200]\n",
            "loss: 1.218771  [  144/ 3200]\n",
            "loss: 1.077518  [  145/ 3200]\n",
            "loss: 1.283474  [  146/ 3200]\n",
            "loss: 1.151748  [  147/ 3200]\n",
            "loss: 1.142758  [  148/ 3200]\n",
            "loss: 1.029049  [  149/ 3200]\n",
            "loss: 1.177503  [  150/ 3200]\n",
            "loss: 1.165952  [  151/ 3200]\n",
            "loss: 1.136583  [  152/ 3200]\n",
            "loss: 1.214406  [  153/ 3200]\n",
            "loss: 1.252719  [  154/ 3200]\n",
            "loss: 1.096659  [  155/ 3200]\n",
            "loss: 1.088499  [  156/ 3200]\n",
            "loss: 1.201675  [  157/ 3200]\n",
            "loss: 1.173088  [  158/ 3200]\n",
            "loss: 1.155335  [  159/ 3200]\n",
            "loss: 1.099468  [  160/ 3200]\n",
            "loss: 1.252836  [  161/ 3200]\n",
            "loss: 1.206507  [  162/ 3200]\n",
            "loss: 1.067215  [  163/ 3200]\n",
            "loss: 1.094936  [  164/ 3200]\n",
            "loss: 0.984604  [  165/ 3200]\n",
            "loss: 1.226735  [  166/ 3200]\n",
            "loss: 1.146491  [  167/ 3200]\n",
            "loss: 1.205194  [  168/ 3200]\n",
            "loss: 1.141389  [  169/ 3200]\n",
            "loss: 1.140729  [  170/ 3200]\n",
            "loss: 1.229421  [  171/ 3200]\n",
            "loss: 1.162273  [  172/ 3200]\n",
            "loss: 1.247429  [  173/ 3200]\n",
            "loss: 1.097444  [  174/ 3200]\n",
            "loss: 0.992939  [  175/ 3200]\n",
            "loss: 1.227928  [  176/ 3200]\n",
            "loss: 1.273934  [  177/ 3200]\n",
            "loss: 1.262363  [  178/ 3200]\n",
            "loss: 1.178971  [  179/ 3200]\n",
            "loss: 1.154265  [  180/ 3200]\n",
            "loss: 1.150467  [  181/ 3200]\n",
            "loss: 1.179121  [  182/ 3200]\n",
            "loss: 1.132433  [  183/ 3200]\n",
            "loss: 1.006217  [  184/ 3200]\n",
            "loss: 1.133335  [  185/ 3200]\n",
            "loss: 1.246604  [  186/ 3200]\n",
            "loss: 1.143648  [  187/ 3200]\n",
            "loss: 1.042358  [  188/ 3200]\n",
            "loss: 1.179682  [  189/ 3200]\n",
            "loss: 1.136415  [  190/ 3200]\n",
            "loss: 1.261815  [  191/ 3200]\n",
            "loss: 1.249437  [  192/ 3200]\n",
            "loss: 1.112755  [  193/ 3200]\n",
            "loss: 1.109052  [  194/ 3200]\n",
            "loss: 1.188449  [  195/ 3200]\n",
            "loss: 1.083354  [  196/ 3200]\n",
            "loss: 1.154130  [  197/ 3200]\n",
            "loss: 1.147342  [  198/ 3200]\n",
            "loss: 1.138733  [  199/ 3200]\n",
            "Epoch:  15\n",
            "loss: 1.168699  [    0/ 3200]\n",
            "loss: 1.148809  [    1/ 3200]\n",
            "loss: 1.168554  [    2/ 3200]\n",
            "loss: 1.171072  [    3/ 3200]\n",
            "loss: 1.113798  [    4/ 3200]\n",
            "loss: 1.023586  [    5/ 3200]\n",
            "loss: 1.159490  [    6/ 3200]\n",
            "loss: 1.226387  [    7/ 3200]\n",
            "loss: 1.166506  [    8/ 3200]\n",
            "loss: 1.178980  [    9/ 3200]\n",
            "loss: 1.064040  [   10/ 3200]\n",
            "loss: 1.185892  [   11/ 3200]\n",
            "loss: 1.304921  [   12/ 3200]\n",
            "loss: 1.050695  [   13/ 3200]\n",
            "loss: 1.186113  [   14/ 3200]\n",
            "loss: 0.992799  [   15/ 3200]\n",
            "loss: 1.206786  [   16/ 3200]\n",
            "loss: 1.122602  [   17/ 3200]\n",
            "loss: 1.289277  [   18/ 3200]\n",
            "loss: 1.056439  [   19/ 3200]\n",
            "loss: 1.234824  [   20/ 3200]\n",
            "loss: 1.186779  [   21/ 3200]\n",
            "loss: 1.132785  [   22/ 3200]\n",
            "loss: 1.178762  [   23/ 3200]\n",
            "loss: 1.039243  [   24/ 3200]\n",
            "loss: 1.158088  [   25/ 3200]\n",
            "loss: 1.037934  [   26/ 3200]\n",
            "loss: 1.108814  [   27/ 3200]\n",
            "loss: 1.128937  [   28/ 3200]\n",
            "loss: 1.194457  [   29/ 3200]\n",
            "loss: 1.128819  [   30/ 3200]\n",
            "loss: 1.100163  [   31/ 3200]\n",
            "loss: 1.213209  [   32/ 3200]\n",
            "loss: 1.175663  [   33/ 3200]\n",
            "loss: 1.108580  [   34/ 3200]\n",
            "loss: 1.076895  [   35/ 3200]\n",
            "loss: 1.213132  [   36/ 3200]\n",
            "loss: 1.232047  [   37/ 3200]\n",
            "loss: 1.241814  [   38/ 3200]\n",
            "loss: 1.326230  [   39/ 3200]\n",
            "loss: 1.016245  [   40/ 3200]\n",
            "loss: 1.198787  [   41/ 3200]\n",
            "loss: 1.266317  [   42/ 3200]\n",
            "loss: 1.356364  [   43/ 3200]\n",
            "loss: 1.085972  [   44/ 3200]\n",
            "loss: 1.057365  [   45/ 3200]\n",
            "loss: 1.203595  [   46/ 3200]\n",
            "loss: 1.252632  [   47/ 3200]\n",
            "loss: 1.216344  [   48/ 3200]\n",
            "loss: 1.205667  [   49/ 3200]\n",
            "loss: 1.114869  [   50/ 3200]\n",
            "loss: 1.188179  [   51/ 3200]\n",
            "loss: 1.118992  [   52/ 3200]\n",
            "loss: 1.248357  [   53/ 3200]\n",
            "loss: 1.207187  [   54/ 3200]\n",
            "loss: 1.349800  [   55/ 3200]\n",
            "loss: 1.169531  [   56/ 3200]\n",
            "loss: 1.285382  [   57/ 3200]\n",
            "loss: 1.149630  [   58/ 3200]\n",
            "loss: 1.290318  [   59/ 3200]\n",
            "loss: 1.075679  [   60/ 3200]\n",
            "loss: 1.192514  [   61/ 3200]\n",
            "loss: 1.027786  [   62/ 3200]\n",
            "loss: 1.098668  [   63/ 3200]\n",
            "loss: 1.274498  [   64/ 3200]\n",
            "loss: 1.199418  [   65/ 3200]\n",
            "loss: 1.009250  [   66/ 3200]\n",
            "loss: 1.074708  [   67/ 3200]\n",
            "loss: 1.148739  [   68/ 3200]\n",
            "loss: 1.005445  [   69/ 3200]\n",
            "loss: 1.212088  [   70/ 3200]\n",
            "loss: 1.171193  [   71/ 3200]\n",
            "loss: 1.167355  [   72/ 3200]\n",
            "loss: 1.150628  [   73/ 3200]\n",
            "loss: 1.211057  [   74/ 3200]\n",
            "loss: 1.173589  [   75/ 3200]\n",
            "loss: 1.246114  [   76/ 3200]\n",
            "loss: 0.989617  [   77/ 3200]\n",
            "loss: 1.039198  [   78/ 3200]\n",
            "loss: 1.173076  [   79/ 3200]\n",
            "loss: 1.133188  [   80/ 3200]\n",
            "loss: 1.171154  [   81/ 3200]\n",
            "loss: 1.271687  [   82/ 3200]\n",
            "loss: 1.230513  [   83/ 3200]\n",
            "loss: 1.128098  [   84/ 3200]\n",
            "loss: 1.032698  [   85/ 3200]\n",
            "loss: 1.078970  [   86/ 3200]\n",
            "loss: 1.130325  [   87/ 3200]\n",
            "loss: 1.155329  [   88/ 3200]\n",
            "loss: 1.269390  [   89/ 3200]\n",
            "loss: 1.206788  [   90/ 3200]\n",
            "loss: 1.060803  [   91/ 3200]\n",
            "loss: 1.257863  [   92/ 3200]\n",
            "loss: 1.093152  [   93/ 3200]\n",
            "loss: 1.188748  [   94/ 3200]\n",
            "loss: 1.115233  [   95/ 3200]\n",
            "loss: 1.225849  [   96/ 3200]\n",
            "loss: 1.183113  [   97/ 3200]\n",
            "loss: 1.226715  [   98/ 3200]\n",
            "loss: 1.269734  [   99/ 3200]\n",
            "loss: 1.104372  [  100/ 3200]\n",
            "loss: 1.168915  [  101/ 3200]\n",
            "loss: 0.996569  [  102/ 3200]\n",
            "loss: 1.171632  [  103/ 3200]\n",
            "loss: 1.356285  [  104/ 3200]\n",
            "loss: 1.127757  [  105/ 3200]\n",
            "loss: 1.121518  [  106/ 3200]\n",
            "loss: 1.187904  [  107/ 3200]\n",
            "loss: 1.207690  [  108/ 3200]\n",
            "loss: 1.274221  [  109/ 3200]\n",
            "loss: 1.204486  [  110/ 3200]\n",
            "loss: 1.227150  [  111/ 3200]\n",
            "loss: 1.167748  [  112/ 3200]\n",
            "loss: 1.104639  [  113/ 3200]\n",
            "loss: 1.316247  [  114/ 3200]\n",
            "loss: 1.256362  [  115/ 3200]\n",
            "loss: 1.019309  [  116/ 3200]\n",
            "loss: 1.180993  [  117/ 3200]\n",
            "loss: 1.197673  [  118/ 3200]\n",
            "loss: 1.029194  [  119/ 3200]\n",
            "loss: 1.048161  [  120/ 3200]\n",
            "loss: 1.057638  [  121/ 3200]\n",
            "loss: 1.014040  [  122/ 3200]\n",
            "loss: 1.152851  [  123/ 3200]\n",
            "loss: 1.198533  [  124/ 3200]\n",
            "loss: 1.214803  [  125/ 3200]\n",
            "loss: 1.303342  [  126/ 3200]\n",
            "loss: 1.189236  [  127/ 3200]\n",
            "loss: 1.230647  [  128/ 3200]\n",
            "loss: 1.292950  [  129/ 3200]\n",
            "loss: 1.057346  [  130/ 3200]\n",
            "loss: 1.059869  [  131/ 3200]\n",
            "loss: 1.219522  [  132/ 3200]\n",
            "loss: 1.127168  [  133/ 3200]\n",
            "loss: 1.192392  [  134/ 3200]\n",
            "loss: 1.284694  [  135/ 3200]\n",
            "loss: 1.086280  [  136/ 3200]\n",
            "loss: 1.391797  [  137/ 3200]\n",
            "loss: 1.102761  [  138/ 3200]\n",
            "loss: 1.213707  [  139/ 3200]\n",
            "loss: 1.124949  [  140/ 3200]\n",
            "loss: 1.246340  [  141/ 3200]\n",
            "loss: 1.100835  [  142/ 3200]\n",
            "loss: 1.079059  [  143/ 3200]\n",
            "loss: 1.106853  [  144/ 3200]\n",
            "loss: 1.218880  [  145/ 3200]\n",
            "loss: 1.108318  [  146/ 3200]\n",
            "loss: 1.224649  [  147/ 3200]\n",
            "loss: 0.975859  [  148/ 3200]\n",
            "loss: 1.201224  [  149/ 3200]\n",
            "loss: 1.191904  [  150/ 3200]\n",
            "loss: 1.046385  [  151/ 3200]\n",
            "loss: 1.125374  [  152/ 3200]\n",
            "loss: 1.172795  [  153/ 3200]\n",
            "loss: 1.138100  [  154/ 3200]\n",
            "loss: 1.206229  [  155/ 3200]\n",
            "loss: 1.103869  [  156/ 3200]\n",
            "loss: 1.060118  [  157/ 3200]\n",
            "loss: 1.087801  [  158/ 3200]\n",
            "loss: 1.120932  [  159/ 3200]\n",
            "loss: 1.045073  [  160/ 3200]\n",
            "loss: 1.261910  [  161/ 3200]\n",
            "loss: 1.088303  [  162/ 3200]\n",
            "loss: 1.111313  [  163/ 3200]\n",
            "loss: 1.221606  [  164/ 3200]\n",
            "loss: 1.147758  [  165/ 3200]\n",
            "loss: 1.271873  [  166/ 3200]\n",
            "loss: 1.201523  [  167/ 3200]\n",
            "loss: 1.296156  [  168/ 3200]\n",
            "loss: 1.078790  [  169/ 3200]\n",
            "loss: 1.160316  [  170/ 3200]\n",
            "loss: 1.147442  [  171/ 3200]\n",
            "loss: 1.040938  [  172/ 3200]\n",
            "loss: 1.081865  [  173/ 3200]\n",
            "loss: 1.244810  [  174/ 3200]\n",
            "loss: 1.319182  [  175/ 3200]\n",
            "loss: 1.132881  [  176/ 3200]\n",
            "loss: 1.062086  [  177/ 3200]\n",
            "loss: 1.124288  [  178/ 3200]\n",
            "loss: 1.052837  [  179/ 3200]\n",
            "loss: 1.131354  [  180/ 3200]\n",
            "loss: 1.247819  [  181/ 3200]\n",
            "loss: 1.112689  [  182/ 3200]\n",
            "loss: 1.171869  [  183/ 3200]\n",
            "loss: 1.069442  [  184/ 3200]\n",
            "loss: 1.104565  [  185/ 3200]\n",
            "loss: 1.164566  [  186/ 3200]\n",
            "loss: 1.259030  [  187/ 3200]\n",
            "loss: 1.035466  [  188/ 3200]\n",
            "loss: 1.119756  [  189/ 3200]\n",
            "loss: 1.278104  [  190/ 3200]\n",
            "loss: 1.155948  [  191/ 3200]\n",
            "loss: 1.133900  [  192/ 3200]\n",
            "loss: 1.072356  [  193/ 3200]\n",
            "loss: 1.150770  [  194/ 3200]\n",
            "loss: 1.276526  [  195/ 3200]\n",
            "loss: 1.032150  [  196/ 3200]\n",
            "loss: 1.255554  [  197/ 3200]\n",
            "loss: 1.217508  [  198/ 3200]\n",
            "loss: 1.240030  [  199/ 3200]\n",
            "Epoch:  16\n",
            "loss: 1.141544  [    0/ 3200]\n",
            "loss: 1.167370  [    1/ 3200]\n",
            "loss: 1.147832  [    2/ 3200]\n",
            "loss: 1.000847  [    3/ 3200]\n",
            "loss: 1.198399  [    4/ 3200]\n",
            "loss: 1.083811  [    5/ 3200]\n",
            "loss: 1.237426  [    6/ 3200]\n",
            "loss: 1.142304  [    7/ 3200]\n",
            "loss: 1.251879  [    8/ 3200]\n",
            "loss: 1.036848  [    9/ 3200]\n",
            "loss: 1.143165  [   10/ 3200]\n",
            "loss: 1.056442  [   11/ 3200]\n",
            "loss: 1.122377  [   12/ 3200]\n",
            "loss: 1.074535  [   13/ 3200]\n",
            "loss: 1.185001  [   14/ 3200]\n",
            "loss: 1.226774  [   15/ 3200]\n",
            "loss: 1.115456  [   16/ 3200]\n",
            "loss: 1.181809  [   17/ 3200]\n",
            "loss: 1.195250  [   18/ 3200]\n",
            "loss: 1.101360  [   19/ 3200]\n",
            "loss: 1.164183  [   20/ 3200]\n",
            "loss: 0.960377  [   21/ 3200]\n",
            "loss: 1.018053  [   22/ 3200]\n",
            "loss: 1.098445  [   23/ 3200]\n",
            "loss: 1.108071  [   24/ 3200]\n",
            "loss: 0.995109  [   25/ 3200]\n",
            "loss: 1.157225  [   26/ 3200]\n",
            "loss: 1.227854  [   27/ 3200]\n",
            "loss: 1.039235  [   28/ 3200]\n",
            "loss: 1.281491  [   29/ 3200]\n",
            "loss: 1.187024  [   30/ 3200]\n",
            "loss: 1.124189  [   31/ 3200]\n",
            "loss: 1.086311  [   32/ 3200]\n",
            "loss: 1.345012  [   33/ 3200]\n",
            "loss: 1.017770  [   34/ 3200]\n",
            "loss: 1.154221  [   35/ 3200]\n",
            "loss: 1.153643  [   36/ 3200]\n",
            "loss: 1.139248  [   37/ 3200]\n",
            "loss: 1.104648  [   38/ 3200]\n",
            "loss: 1.114859  [   39/ 3200]\n",
            "loss: 1.295640  [   40/ 3200]\n",
            "loss: 1.276076  [   41/ 3200]\n",
            "loss: 1.367594  [   42/ 3200]\n",
            "loss: 1.213675  [   43/ 3200]\n",
            "loss: 1.114486  [   44/ 3200]\n",
            "loss: 1.160779  [   45/ 3200]\n",
            "loss: 1.215758  [   46/ 3200]\n",
            "loss: 1.070306  [   47/ 3200]\n",
            "loss: 1.272465  [   48/ 3200]\n",
            "loss: 1.143607  [   49/ 3200]\n",
            "loss: 1.061617  [   50/ 3200]\n",
            "loss: 1.193547  [   51/ 3200]\n",
            "loss: 1.157178  [   52/ 3200]\n",
            "loss: 1.135148  [   53/ 3200]\n",
            "loss: 1.134643  [   54/ 3200]\n",
            "loss: 1.075334  [   55/ 3200]\n",
            "loss: 1.186303  [   56/ 3200]\n",
            "loss: 1.226679  [   57/ 3200]\n",
            "loss: 1.195048  [   58/ 3200]\n",
            "loss: 1.147936  [   59/ 3200]\n",
            "loss: 1.097404  [   60/ 3200]\n",
            "loss: 1.242138  [   61/ 3200]\n",
            "loss: 1.156809  [   62/ 3200]\n",
            "loss: 1.263728  [   63/ 3200]\n",
            "loss: 1.141173  [   64/ 3200]\n",
            "loss: 1.201010  [   65/ 3200]\n",
            "loss: 1.031241  [   66/ 3200]\n",
            "loss: 1.223955  [   67/ 3200]\n",
            "loss: 1.252327  [   68/ 3200]\n",
            "loss: 1.082493  [   69/ 3200]\n",
            "loss: 1.070507  [   70/ 3200]\n",
            "loss: 1.217851  [   71/ 3200]\n",
            "loss: 1.110621  [   72/ 3200]\n",
            "loss: 1.064797  [   73/ 3200]\n",
            "loss: 1.076756  [   74/ 3200]\n",
            "loss: 0.928361  [   75/ 3200]\n",
            "loss: 1.239590  [   76/ 3200]\n",
            "loss: 1.222998  [   77/ 3200]\n",
            "loss: 1.083472  [   78/ 3200]\n",
            "loss: 1.211526  [   79/ 3200]\n",
            "loss: 1.129859  [   80/ 3200]\n",
            "loss: 1.152096  [   81/ 3200]\n",
            "loss: 1.075806  [   82/ 3200]\n",
            "loss: 1.251554  [   83/ 3200]\n",
            "loss: 1.111222  [   84/ 3200]\n",
            "loss: 1.356533  [   85/ 3200]\n",
            "loss: 1.034023  [   86/ 3200]\n",
            "loss: 1.169871  [   87/ 3200]\n",
            "loss: 1.206820  [   88/ 3200]\n",
            "loss: 1.191435  [   89/ 3200]\n",
            "loss: 1.155364  [   90/ 3200]\n",
            "loss: 1.160820  [   91/ 3200]\n",
            "loss: 1.141445  [   92/ 3200]\n",
            "loss: 1.205053  [   93/ 3200]\n",
            "loss: 1.144371  [   94/ 3200]\n",
            "loss: 1.088238  [   95/ 3200]\n",
            "loss: 1.034951  [   96/ 3200]\n",
            "loss: 1.363242  [   97/ 3200]\n",
            "loss: 0.992070  [   98/ 3200]\n",
            "loss: 1.159075  [   99/ 3200]\n",
            "loss: 1.137349  [  100/ 3200]\n",
            "loss: 1.131452  [  101/ 3200]\n",
            "loss: 1.156835  [  102/ 3200]\n",
            "loss: 1.129351  [  103/ 3200]\n",
            "loss: 1.016299  [  104/ 3200]\n",
            "loss: 0.992333  [  105/ 3200]\n",
            "loss: 1.041982  [  106/ 3200]\n",
            "loss: 1.081531  [  107/ 3200]\n",
            "loss: 1.172753  [  108/ 3200]\n",
            "loss: 0.911811  [  109/ 3200]\n",
            "loss: 1.469626  [  110/ 3200]\n",
            "loss: 1.224623  [  111/ 3200]\n",
            "loss: 1.050503  [  112/ 3200]\n",
            "loss: 1.131044  [  113/ 3200]\n",
            "loss: 0.980545  [  114/ 3200]\n",
            "loss: 1.062050  [  115/ 3200]\n",
            "loss: 1.131930  [  116/ 3200]\n",
            "loss: 1.174038  [  117/ 3200]\n",
            "loss: 1.125326  [  118/ 3200]\n",
            "loss: 1.229438  [  119/ 3200]\n",
            "loss: 1.031730  [  120/ 3200]\n",
            "loss: 1.112843  [  121/ 3200]\n",
            "loss: 1.073724  [  122/ 3200]\n",
            "loss: 1.085692  [  123/ 3200]\n",
            "loss: 0.978392  [  124/ 3200]\n",
            "loss: 1.069921  [  125/ 3200]\n",
            "loss: 1.168220  [  126/ 3200]\n",
            "loss: 1.076444  [  127/ 3200]\n",
            "loss: 1.184890  [  128/ 3200]\n",
            "loss: 1.166831  [  129/ 3200]\n",
            "loss: 1.016916  [  130/ 3200]\n",
            "loss: 1.167104  [  131/ 3200]\n",
            "loss: 1.118702  [  132/ 3200]\n",
            "loss: 1.083917  [  133/ 3200]\n",
            "loss: 1.114505  [  134/ 3200]\n",
            "loss: 1.214354  [  135/ 3200]\n",
            "loss: 1.008479  [  136/ 3200]\n",
            "loss: 1.305969  [  137/ 3200]\n",
            "loss: 1.078986  [  138/ 3200]\n",
            "loss: 1.167162  [  139/ 3200]\n",
            "loss: 1.079779  [  140/ 3200]\n",
            "loss: 1.025200  [  141/ 3200]\n",
            "loss: 1.143276  [  142/ 3200]\n",
            "loss: 1.244000  [  143/ 3200]\n",
            "loss: 1.229867  [  144/ 3200]\n",
            "loss: 1.116379  [  145/ 3200]\n",
            "loss: 1.199428  [  146/ 3200]\n",
            "loss: 1.104757  [  147/ 3200]\n",
            "loss: 1.257565  [  148/ 3200]\n",
            "loss: 1.224193  [  149/ 3200]\n",
            "loss: 1.214549  [  150/ 3200]\n",
            "loss: 1.107452  [  151/ 3200]\n",
            "loss: 1.260262  [  152/ 3200]\n",
            "loss: 1.104162  [  153/ 3200]\n",
            "loss: 1.117313  [  154/ 3200]\n",
            "loss: 0.996446  [  155/ 3200]\n",
            "loss: 1.196677  [  156/ 3200]\n",
            "loss: 1.209030  [  157/ 3200]\n",
            "loss: 1.232140  [  158/ 3200]\n",
            "loss: 1.072290  [  159/ 3200]\n",
            "loss: 0.861071  [  160/ 3200]\n",
            "loss: 1.268285  [  161/ 3200]\n",
            "loss: 1.092777  [  162/ 3200]\n",
            "loss: 1.202990  [  163/ 3200]\n",
            "loss: 1.127845  [  164/ 3200]\n",
            "loss: 1.115984  [  165/ 3200]\n",
            "loss: 1.132481  [  166/ 3200]\n",
            "loss: 1.265923  [  167/ 3200]\n",
            "loss: 1.237898  [  168/ 3200]\n",
            "loss: 0.943304  [  169/ 3200]\n",
            "loss: 1.003425  [  170/ 3200]\n",
            "loss: 1.138351  [  171/ 3200]\n",
            "loss: 1.184210  [  172/ 3200]\n",
            "loss: 1.137164  [  173/ 3200]\n",
            "loss: 1.083640  [  174/ 3200]\n",
            "loss: 1.050959  [  175/ 3200]\n",
            "loss: 1.196672  [  176/ 3200]\n",
            "loss: 1.057295  [  177/ 3200]\n",
            "loss: 1.037674  [  178/ 3200]\n",
            "loss: 1.021724  [  179/ 3200]\n",
            "loss: 1.119231  [  180/ 3200]\n",
            "loss: 1.203388  [  181/ 3200]\n",
            "loss: 1.197497  [  182/ 3200]\n",
            "loss: 1.086820  [  183/ 3200]\n",
            "loss: 1.111761  [  184/ 3200]\n",
            "loss: 0.885049  [  185/ 3200]\n",
            "loss: 1.344830  [  186/ 3200]\n",
            "loss: 1.054615  [  187/ 3200]\n",
            "loss: 1.379697  [  188/ 3200]\n",
            "loss: 1.310067  [  189/ 3200]\n",
            "loss: 1.152014  [  190/ 3200]\n",
            "loss: 1.313125  [  191/ 3200]\n",
            "loss: 1.120483  [  192/ 3200]\n",
            "loss: 1.002422  [  193/ 3200]\n",
            "loss: 1.102738  [  194/ 3200]\n",
            "loss: 1.364908  [  195/ 3200]\n",
            "loss: 1.249480  [  196/ 3200]\n",
            "loss: 1.185179  [  197/ 3200]\n",
            "loss: 1.241810  [  198/ 3200]\n",
            "loss: 0.980201  [  199/ 3200]\n",
            "Epoch:  17\n",
            "loss: 1.062553  [    0/ 3200]\n",
            "loss: 1.204318  [    1/ 3200]\n",
            "loss: 1.073880  [    2/ 3200]\n",
            "loss: 1.186764  [    3/ 3200]\n",
            "loss: 1.143453  [    4/ 3200]\n",
            "loss: 1.171753  [    5/ 3200]\n",
            "loss: 1.186002  [    6/ 3200]\n",
            "loss: 1.182738  [    7/ 3200]\n",
            "loss: 1.051161  [    8/ 3200]\n",
            "loss: 1.268266  [    9/ 3200]\n",
            "loss: 1.117197  [   10/ 3200]\n",
            "loss: 1.095160  [   11/ 3200]\n",
            "loss: 1.155269  [   12/ 3200]\n",
            "loss: 1.182380  [   13/ 3200]\n",
            "loss: 1.010497  [   14/ 3200]\n",
            "loss: 1.049462  [   15/ 3200]\n",
            "loss: 1.093611  [   16/ 3200]\n",
            "loss: 1.145543  [   17/ 3200]\n",
            "loss: 1.169480  [   18/ 3200]\n",
            "loss: 1.171352  [   19/ 3200]\n",
            "loss: 1.193912  [   20/ 3200]\n",
            "loss: 1.123585  [   21/ 3200]\n",
            "loss: 1.016797  [   22/ 3200]\n",
            "loss: 1.290818  [   23/ 3200]\n",
            "loss: 1.249413  [   24/ 3200]\n",
            "loss: 1.112899  [   25/ 3200]\n",
            "loss: 1.102354  [   26/ 3200]\n",
            "loss: 1.140013  [   27/ 3200]\n",
            "loss: 1.197102  [   28/ 3200]\n",
            "loss: 0.936637  [   29/ 3200]\n",
            "loss: 1.228954  [   30/ 3200]\n",
            "loss: 1.128597  [   31/ 3200]\n",
            "loss: 1.171959  [   32/ 3200]\n",
            "loss: 1.013916  [   33/ 3200]\n",
            "loss: 0.941597  [   34/ 3200]\n",
            "loss: 1.215903  [   35/ 3200]\n",
            "loss: 1.144424  [   36/ 3200]\n",
            "loss: 1.044307  [   37/ 3200]\n",
            "loss: 1.067654  [   38/ 3200]\n",
            "loss: 1.177887  [   39/ 3200]\n",
            "loss: 1.113788  [   40/ 3200]\n",
            "loss: 1.111469  [   41/ 3200]\n",
            "loss: 1.110731  [   42/ 3200]\n",
            "loss: 1.170656  [   43/ 3200]\n",
            "loss: 0.921951  [   44/ 3200]\n",
            "loss: 1.028285  [   45/ 3200]\n",
            "loss: 1.109927  [   46/ 3200]\n",
            "loss: 0.926691  [   47/ 3200]\n",
            "loss: 1.020158  [   48/ 3200]\n",
            "loss: 1.111300  [   49/ 3200]\n",
            "loss: 1.254481  [   50/ 3200]\n",
            "loss: 1.188311  [   51/ 3200]\n",
            "loss: 1.224318  [   52/ 3200]\n",
            "loss: 1.183724  [   53/ 3200]\n",
            "loss: 1.098288  [   54/ 3200]\n",
            "loss: 1.243860  [   55/ 3200]\n",
            "loss: 1.219012  [   56/ 3200]\n",
            "loss: 1.025402  [   57/ 3200]\n",
            "loss: 1.057732  [   58/ 3200]\n",
            "loss: 1.121305  [   59/ 3200]\n",
            "loss: 1.112359  [   60/ 3200]\n",
            "loss: 1.316584  [   61/ 3200]\n",
            "loss: 1.199072  [   62/ 3200]\n",
            "loss: 1.079912  [   63/ 3200]\n",
            "loss: 1.025848  [   64/ 3200]\n",
            "loss: 1.106944  [   65/ 3200]\n",
            "loss: 1.174414  [   66/ 3200]\n",
            "loss: 1.226195  [   67/ 3200]\n",
            "loss: 1.094901  [   68/ 3200]\n",
            "loss: 0.967899  [   69/ 3200]\n",
            "loss: 1.147991  [   70/ 3200]\n",
            "loss: 1.197580  [   71/ 3200]\n",
            "loss: 1.227606  [   72/ 3200]\n",
            "loss: 1.297295  [   73/ 3200]\n",
            "loss: 1.028084  [   74/ 3200]\n",
            "loss: 1.071956  [   75/ 3200]\n",
            "loss: 1.136421  [   76/ 3200]\n",
            "loss: 1.097406  [   77/ 3200]\n",
            "loss: 1.248696  [   78/ 3200]\n",
            "loss: 1.223840  [   79/ 3200]\n",
            "loss: 1.123852  [   80/ 3200]\n",
            "loss: 0.989773  [   81/ 3200]\n",
            "loss: 0.993932  [   82/ 3200]\n",
            "loss: 1.184690  [   83/ 3200]\n",
            "loss: 1.006739  [   84/ 3200]\n",
            "loss: 1.173726  [   85/ 3200]\n",
            "loss: 1.241655  [   86/ 3200]\n",
            "loss: 1.103272  [   87/ 3200]\n",
            "loss: 1.070979  [   88/ 3200]\n",
            "loss: 1.020957  [   89/ 3200]\n",
            "loss: 1.194990  [   90/ 3200]\n",
            "loss: 1.050095  [   91/ 3200]\n",
            "loss: 1.275064  [   92/ 3200]\n",
            "loss: 1.104483  [   93/ 3200]\n",
            "loss: 1.139215  [   94/ 3200]\n",
            "loss: 1.252206  [   95/ 3200]\n",
            "loss: 1.189437  [   96/ 3200]\n",
            "loss: 1.166198  [   97/ 3200]\n",
            "loss: 1.063427  [   98/ 3200]\n",
            "loss: 1.115212  [   99/ 3200]\n",
            "loss: 1.149734  [  100/ 3200]\n",
            "loss: 1.034213  [  101/ 3200]\n",
            "loss: 1.196026  [  102/ 3200]\n",
            "loss: 0.868690  [  103/ 3200]\n",
            "loss: 0.904586  [  104/ 3200]\n",
            "loss: 1.213162  [  105/ 3200]\n",
            "loss: 1.175488  [  106/ 3200]\n",
            "loss: 1.075263  [  107/ 3200]\n",
            "loss: 1.163064  [  108/ 3200]\n",
            "loss: 1.158197  [  109/ 3200]\n",
            "loss: 1.029873  [  110/ 3200]\n",
            "loss: 1.086797  [  111/ 3200]\n",
            "loss: 1.271993  [  112/ 3200]\n",
            "loss: 1.191780  [  113/ 3200]\n",
            "loss: 1.076021  [  114/ 3200]\n",
            "loss: 1.011747  [  115/ 3200]\n",
            "loss: 0.971494  [  116/ 3200]\n",
            "loss: 1.142301  [  117/ 3200]\n",
            "loss: 1.046495  [  118/ 3200]\n",
            "loss: 1.208606  [  119/ 3200]\n",
            "loss: 1.315093  [  120/ 3200]\n",
            "loss: 1.124647  [  121/ 3200]\n",
            "loss: 1.124098  [  122/ 3200]\n",
            "loss: 1.184567  [  123/ 3200]\n",
            "loss: 1.040915  [  124/ 3200]\n",
            "loss: 1.094084  [  125/ 3200]\n",
            "loss: 1.141444  [  126/ 3200]\n",
            "loss: 1.071543  [  127/ 3200]\n",
            "loss: 1.139670  [  128/ 3200]\n",
            "loss: 1.081543  [  129/ 3200]\n",
            "loss: 1.013419  [  130/ 3200]\n",
            "loss: 1.252158  [  131/ 3200]\n",
            "loss: 1.158169  [  132/ 3200]\n",
            "loss: 1.287980  [  133/ 3200]\n",
            "loss: 1.155334  [  134/ 3200]\n",
            "loss: 0.979626  [  135/ 3200]\n",
            "loss: 1.012378  [  136/ 3200]\n",
            "loss: 0.836478  [  137/ 3200]\n",
            "loss: 1.121138  [  138/ 3200]\n",
            "loss: 1.212174  [  139/ 3200]\n",
            "loss: 1.060420  [  140/ 3200]\n",
            "loss: 1.392238  [  141/ 3200]\n",
            "loss: 1.279066  [  142/ 3200]\n",
            "loss: 1.051812  [  143/ 3200]\n",
            "loss: 1.003653  [  144/ 3200]\n",
            "loss: 0.949368  [  145/ 3200]\n",
            "loss: 1.054189  [  146/ 3200]\n",
            "loss: 0.872101  [  147/ 3200]\n",
            "loss: 1.017621  [  148/ 3200]\n",
            "loss: 1.082912  [  149/ 3200]\n",
            "loss: 1.125576  [  150/ 3200]\n",
            "loss: 1.151436  [  151/ 3200]\n",
            "loss: 1.195259  [  152/ 3200]\n",
            "loss: 1.034506  [  153/ 3200]\n",
            "loss: 1.169048  [  154/ 3200]\n",
            "loss: 1.206521  [  155/ 3200]\n",
            "loss: 1.199651  [  156/ 3200]\n",
            "loss: 1.204867  [  157/ 3200]\n",
            "loss: 0.927681  [  158/ 3200]\n",
            "loss: 1.174478  [  159/ 3200]\n",
            "loss: 1.338230  [  160/ 3200]\n",
            "loss: 1.073983  [  161/ 3200]\n",
            "loss: 1.121105  [  162/ 3200]\n",
            "loss: 1.381672  [  163/ 3200]\n",
            "loss: 1.251255  [  164/ 3200]\n",
            "loss: 1.112381  [  165/ 3200]\n",
            "loss: 1.042916  [  166/ 3200]\n",
            "loss: 0.986296  [  167/ 3200]\n",
            "loss: 1.141623  [  168/ 3200]\n",
            "loss: 0.963988  [  169/ 3200]\n",
            "loss: 1.062891  [  170/ 3200]\n",
            "loss: 1.233985  [  171/ 3200]\n",
            "loss: 1.147316  [  172/ 3200]\n",
            "loss: 1.147514  [  173/ 3200]\n",
            "loss: 1.167340  [  174/ 3200]\n",
            "loss: 1.127235  [  175/ 3200]\n",
            "loss: 1.196645  [  176/ 3200]\n",
            "loss: 1.002210  [  177/ 3200]\n",
            "loss: 1.113537  [  178/ 3200]\n",
            "loss: 0.955042  [  179/ 3200]\n",
            "loss: 1.261709  [  180/ 3200]\n",
            "loss: 1.247615  [  181/ 3200]\n",
            "loss: 1.122521  [  182/ 3200]\n",
            "loss: 1.024182  [  183/ 3200]\n",
            "loss: 1.171692  [  184/ 3200]\n",
            "loss: 1.048764  [  185/ 3200]\n",
            "loss: 1.089993  [  186/ 3200]\n",
            "loss: 1.194360  [  187/ 3200]\n",
            "loss: 1.083272  [  188/ 3200]\n",
            "loss: 1.085386  [  189/ 3200]\n",
            "loss: 1.153041  [  190/ 3200]\n",
            "loss: 1.137822  [  191/ 3200]\n",
            "loss: 1.008186  [  192/ 3200]\n",
            "loss: 1.167157  [  193/ 3200]\n",
            "loss: 1.077226  [  194/ 3200]\n",
            "loss: 1.155877  [  195/ 3200]\n",
            "loss: 1.095436  [  196/ 3200]\n",
            "loss: 1.193281  [  197/ 3200]\n",
            "loss: 1.109054  [  198/ 3200]\n",
            "loss: 0.925468  [  199/ 3200]\n",
            "Epoch:  18\n",
            "loss: 0.962496  [    0/ 3200]\n",
            "loss: 1.139192  [    1/ 3200]\n",
            "loss: 1.196992  [    2/ 3200]\n",
            "loss: 1.298677  [    3/ 3200]\n",
            "loss: 1.039095  [    4/ 3200]\n",
            "loss: 1.028221  [    5/ 3200]\n",
            "loss: 1.075613  [    6/ 3200]\n",
            "loss: 1.096821  [    7/ 3200]\n",
            "loss: 1.056198  [    8/ 3200]\n",
            "loss: 1.114922  [    9/ 3200]\n",
            "loss: 1.105795  [   10/ 3200]\n",
            "loss: 1.037417  [   11/ 3200]\n",
            "loss: 1.067683  [   12/ 3200]\n",
            "loss: 0.965628  [   13/ 3200]\n",
            "loss: 1.118559  [   14/ 3200]\n",
            "loss: 1.180297  [   15/ 3200]\n",
            "loss: 1.144008  [   16/ 3200]\n",
            "loss: 1.125942  [   17/ 3200]\n",
            "loss: 1.118736  [   18/ 3200]\n",
            "loss: 1.102182  [   19/ 3200]\n",
            "loss: 1.103439  [   20/ 3200]\n",
            "loss: 0.960695  [   21/ 3200]\n",
            "loss: 0.835681  [   22/ 3200]\n",
            "loss: 1.135310  [   23/ 3200]\n",
            "loss: 1.063496  [   24/ 3200]\n",
            "loss: 0.979298  [   25/ 3200]\n",
            "loss: 0.923546  [   26/ 3200]\n",
            "loss: 1.044873  [   27/ 3200]\n",
            "loss: 0.938318  [   28/ 3200]\n",
            "loss: 1.060788  [   29/ 3200]\n",
            "loss: 0.830294  [   30/ 3200]\n",
            "loss: 0.959348  [   31/ 3200]\n",
            "loss: 1.160010  [   32/ 3200]\n",
            "loss: 1.155678  [   33/ 3200]\n",
            "loss: 1.094040  [   34/ 3200]\n",
            "loss: 1.196843  [   35/ 3200]\n",
            "loss: 1.160334  [   36/ 3200]\n",
            "loss: 1.226323  [   37/ 3200]\n",
            "loss: 1.031632  [   38/ 3200]\n",
            "loss: 1.328017  [   39/ 3200]\n",
            "loss: 1.155435  [   40/ 3200]\n",
            "loss: 1.275408  [   41/ 3200]\n",
            "loss: 0.950825  [   42/ 3200]\n",
            "loss: 1.155713  [   43/ 3200]\n",
            "loss: 1.290722  [   44/ 3200]\n",
            "loss: 1.104745  [   45/ 3200]\n",
            "loss: 1.107266  [   46/ 3200]\n",
            "loss: 1.152711  [   47/ 3200]\n",
            "loss: 1.003049  [   48/ 3200]\n",
            "loss: 1.113653  [   49/ 3200]\n",
            "loss: 1.089366  [   50/ 3200]\n",
            "loss: 1.027256  [   51/ 3200]\n",
            "loss: 1.107929  [   52/ 3200]\n",
            "loss: 1.156924  [   53/ 3200]\n",
            "loss: 1.039604  [   54/ 3200]\n",
            "loss: 1.161295  [   55/ 3200]\n",
            "loss: 1.083547  [   56/ 3200]\n",
            "loss: 1.203542  [   57/ 3200]\n",
            "loss: 0.994939  [   58/ 3200]\n",
            "loss: 1.114644  [   59/ 3200]\n",
            "loss: 1.199789  [   60/ 3200]\n",
            "loss: 1.056525  [   61/ 3200]\n",
            "loss: 0.995236  [   62/ 3200]\n",
            "loss: 1.264792  [   63/ 3200]\n",
            "loss: 1.224905  [   64/ 3200]\n",
            "loss: 1.166038  [   65/ 3200]\n",
            "loss: 1.065007  [   66/ 3200]\n",
            "loss: 1.190564  [   67/ 3200]\n",
            "loss: 1.222335  [   68/ 3200]\n",
            "loss: 1.126896  [   69/ 3200]\n",
            "loss: 1.100516  [   70/ 3200]\n",
            "loss: 1.071272  [   71/ 3200]\n",
            "loss: 1.147124  [   72/ 3200]\n",
            "loss: 1.029841  [   73/ 3200]\n",
            "loss: 1.121176  [   74/ 3200]\n",
            "loss: 1.093676  [   75/ 3200]\n",
            "loss: 1.004761  [   76/ 3200]\n",
            "loss: 1.057616  [   77/ 3200]\n",
            "loss: 1.380255  [   78/ 3200]\n",
            "loss: 1.088251  [   79/ 3200]\n",
            "loss: 0.987986  [   80/ 3200]\n",
            "loss: 1.244910  [   81/ 3200]\n",
            "loss: 1.090227  [   82/ 3200]\n",
            "loss: 1.060510  [   83/ 3200]\n",
            "loss: 1.147008  [   84/ 3200]\n",
            "loss: 1.170004  [   85/ 3200]\n",
            "loss: 1.031268  [   86/ 3200]\n",
            "loss: 1.193995  [   87/ 3200]\n",
            "loss: 1.056680  [   88/ 3200]\n",
            "loss: 1.226587  [   89/ 3200]\n",
            "loss: 1.176145  [   90/ 3200]\n",
            "loss: 1.067550  [   91/ 3200]\n",
            "loss: 1.027419  [   92/ 3200]\n",
            "loss: 1.088883  [   93/ 3200]\n",
            "loss: 1.043617  [   94/ 3200]\n",
            "loss: 1.263954  [   95/ 3200]\n",
            "loss: 1.072326  [   96/ 3200]\n",
            "loss: 1.171252  [   97/ 3200]\n",
            "loss: 1.228268  [   98/ 3200]\n",
            "loss: 1.315716  [   99/ 3200]\n",
            "loss: 1.103459  [  100/ 3200]\n",
            "loss: 1.006333  [  101/ 3200]\n",
            "loss: 0.938261  [  102/ 3200]\n",
            "loss: 0.885827  [  103/ 3200]\n",
            "loss: 1.036582  [  104/ 3200]\n",
            "loss: 1.075345  [  105/ 3200]\n",
            "loss: 1.259713  [  106/ 3200]\n",
            "loss: 1.161531  [  107/ 3200]\n",
            "loss: 1.148594  [  108/ 3200]\n",
            "loss: 1.218353  [  109/ 3200]\n",
            "loss: 1.013479  [  110/ 3200]\n",
            "loss: 1.118580  [  111/ 3200]\n",
            "loss: 1.082250  [  112/ 3200]\n",
            "loss: 1.128243  [  113/ 3200]\n",
            "loss: 0.911253  [  114/ 3200]\n",
            "loss: 1.063462  [  115/ 3200]\n",
            "loss: 1.339654  [  116/ 3200]\n",
            "loss: 0.953625  [  117/ 3200]\n",
            "loss: 1.305521  [  118/ 3200]\n",
            "loss: 1.023426  [  119/ 3200]\n",
            "loss: 1.197676  [  120/ 3200]\n",
            "loss: 1.001709  [  121/ 3200]\n",
            "loss: 1.070970  [  122/ 3200]\n",
            "loss: 1.180565  [  123/ 3200]\n",
            "loss: 1.239153  [  124/ 3200]\n",
            "loss: 1.127081  [  125/ 3200]\n",
            "loss: 1.146748  [  126/ 3200]\n",
            "loss: 1.015993  [  127/ 3200]\n",
            "loss: 0.763984  [  128/ 3200]\n",
            "loss: 1.083164  [  129/ 3200]\n",
            "loss: 1.065004  [  130/ 3200]\n",
            "loss: 0.981759  [  131/ 3200]\n",
            "loss: 1.135776  [  132/ 3200]\n",
            "loss: 1.042852  [  133/ 3200]\n",
            "loss: 1.037146  [  134/ 3200]\n",
            "loss: 1.538745  [  135/ 3200]\n",
            "loss: 0.945531  [  136/ 3200]\n",
            "loss: 1.063792  [  137/ 3200]\n",
            "loss: 1.088457  [  138/ 3200]\n",
            "loss: 1.269789  [  139/ 3200]\n",
            "loss: 1.094530  [  140/ 3200]\n",
            "loss: 1.181806  [  141/ 3200]\n",
            "loss: 1.314806  [  142/ 3200]\n",
            "loss: 1.286958  [  143/ 3200]\n",
            "loss: 1.232884  [  144/ 3200]\n",
            "loss: 1.109577  [  145/ 3200]\n",
            "loss: 1.064860  [  146/ 3200]\n",
            "loss: 0.991881  [  147/ 3200]\n",
            "loss: 1.151479  [  148/ 3200]\n",
            "loss: 1.155097  [  149/ 3200]\n",
            "loss: 1.003227  [  150/ 3200]\n",
            "loss: 1.125607  [  151/ 3200]\n",
            "loss: 1.104993  [  152/ 3200]\n",
            "loss: 1.197724  [  153/ 3200]\n",
            "loss: 1.138261  [  154/ 3200]\n",
            "loss: 1.069868  [  155/ 3200]\n",
            "loss: 1.158075  [  156/ 3200]\n",
            "loss: 1.093808  [  157/ 3200]\n",
            "loss: 1.093311  [  158/ 3200]\n",
            "loss: 1.050474  [  159/ 3200]\n",
            "loss: 0.996481  [  160/ 3200]\n",
            "loss: 1.199483  [  161/ 3200]\n",
            "loss: 1.141117  [  162/ 3200]\n",
            "loss: 1.084684  [  163/ 3200]\n",
            "loss: 1.107087  [  164/ 3200]\n",
            "loss: 1.037092  [  165/ 3200]\n",
            "loss: 0.986695  [  166/ 3200]\n",
            "loss: 1.296064  [  167/ 3200]\n",
            "loss: 1.176154  [  168/ 3200]\n",
            "loss: 1.081880  [  169/ 3200]\n",
            "loss: 1.135991  [  170/ 3200]\n",
            "loss: 0.946038  [  171/ 3200]\n",
            "loss: 1.221766  [  172/ 3200]\n",
            "loss: 1.112607  [  173/ 3200]\n",
            "loss: 1.000772  [  174/ 3200]\n",
            "loss: 1.212288  [  175/ 3200]\n",
            "loss: 1.022748  [  176/ 3200]\n",
            "loss: 0.885319  [  177/ 3200]\n",
            "loss: 1.110959  [  178/ 3200]\n",
            "loss: 1.205574  [  179/ 3200]\n",
            "loss: 1.275638  [  180/ 3200]\n",
            "loss: 1.119531  [  181/ 3200]\n",
            "loss: 1.097295  [  182/ 3200]\n",
            "loss: 1.192863  [  183/ 3200]\n",
            "loss: 1.062142  [  184/ 3200]\n",
            "loss: 1.192333  [  185/ 3200]\n",
            "loss: 1.117006  [  186/ 3200]\n",
            "loss: 1.092914  [  187/ 3200]\n",
            "loss: 1.093556  [  188/ 3200]\n",
            "loss: 1.022716  [  189/ 3200]\n",
            "loss: 1.148276  [  190/ 3200]\n",
            "loss: 1.071361  [  191/ 3200]\n",
            "loss: 1.112715  [  192/ 3200]\n",
            "loss: 0.955401  [  193/ 3200]\n",
            "loss: 1.202465  [  194/ 3200]\n",
            "loss: 1.089665  [  195/ 3200]\n",
            "loss: 0.987715  [  196/ 3200]\n",
            "loss: 1.132552  [  197/ 3200]\n",
            "loss: 1.020140  [  198/ 3200]\n",
            "loss: 1.167114  [  199/ 3200]\n",
            "Epoch:  19\n",
            "loss: 0.964114  [    0/ 3200]\n",
            "loss: 1.047178  [    1/ 3200]\n",
            "loss: 1.112559  [    2/ 3200]\n",
            "loss: 1.144743  [    3/ 3200]\n",
            "loss: 1.007645  [    4/ 3200]\n",
            "loss: 1.132721  [    5/ 3200]\n",
            "loss: 1.042678  [    6/ 3200]\n",
            "loss: 0.962095  [    7/ 3200]\n",
            "loss: 1.140360  [    8/ 3200]\n",
            "loss: 1.142201  [    9/ 3200]\n",
            "loss: 1.078285  [   10/ 3200]\n",
            "loss: 1.025802  [   11/ 3200]\n",
            "loss: 0.916151  [   12/ 3200]\n",
            "loss: 1.158883  [   13/ 3200]\n",
            "loss: 0.993204  [   14/ 3200]\n",
            "loss: 1.021170  [   15/ 3200]\n",
            "loss: 1.189560  [   16/ 3200]\n",
            "loss: 1.031514  [   17/ 3200]\n",
            "loss: 0.952881  [   18/ 3200]\n",
            "loss: 1.136724  [   19/ 3200]\n",
            "loss: 1.260067  [   20/ 3200]\n",
            "loss: 1.115232  [   21/ 3200]\n",
            "loss: 1.309317  [   22/ 3200]\n",
            "loss: 1.005146  [   23/ 3200]\n",
            "loss: 0.925749  [   24/ 3200]\n",
            "loss: 1.121612  [   25/ 3200]\n",
            "loss: 1.028845  [   26/ 3200]\n",
            "loss: 1.111884  [   27/ 3200]\n",
            "loss: 1.106050  [   28/ 3200]\n",
            "loss: 1.057699  [   29/ 3200]\n",
            "loss: 1.135864  [   30/ 3200]\n",
            "loss: 1.176428  [   31/ 3200]\n",
            "loss: 1.123223  [   32/ 3200]\n",
            "loss: 1.225936  [   33/ 3200]\n",
            "loss: 1.082860  [   34/ 3200]\n",
            "loss: 0.964849  [   35/ 3200]\n",
            "loss: 0.985944  [   36/ 3200]\n",
            "loss: 1.147323  [   37/ 3200]\n",
            "loss: 1.034590  [   38/ 3200]\n",
            "loss: 1.174141  [   39/ 3200]\n",
            "loss: 0.968497  [   40/ 3200]\n",
            "loss: 1.151141  [   41/ 3200]\n",
            "loss: 1.282342  [   42/ 3200]\n",
            "loss: 0.913066  [   43/ 3200]\n",
            "loss: 1.031135  [   44/ 3200]\n",
            "loss: 1.136141  [   45/ 3200]\n",
            "loss: 1.125911  [   46/ 3200]\n",
            "loss: 1.104387  [   47/ 3200]\n",
            "loss: 1.212073  [   48/ 3200]\n",
            "loss: 1.184221  [   49/ 3200]\n",
            "loss: 1.182000  [   50/ 3200]\n",
            "loss: 1.089280  [   51/ 3200]\n",
            "loss: 1.039363  [   52/ 3200]\n",
            "loss: 0.983263  [   53/ 3200]\n",
            "loss: 1.068659  [   54/ 3200]\n",
            "loss: 1.160312  [   55/ 3200]\n",
            "loss: 1.086064  [   56/ 3200]\n",
            "loss: 1.030435  [   57/ 3200]\n",
            "loss: 1.044508  [   58/ 3200]\n",
            "loss: 1.005440  [   59/ 3200]\n",
            "loss: 1.274127  [   60/ 3200]\n",
            "loss: 1.097211  [   61/ 3200]\n",
            "loss: 0.988304  [   62/ 3200]\n",
            "loss: 1.105701  [   63/ 3200]\n",
            "loss: 0.922580  [   64/ 3200]\n",
            "loss: 1.029058  [   65/ 3200]\n",
            "loss: 0.967158  [   66/ 3200]\n",
            "loss: 1.065142  [   67/ 3200]\n",
            "loss: 1.014785  [   68/ 3200]\n",
            "loss: 1.185590  [   69/ 3200]\n",
            "loss: 0.824357  [   70/ 3200]\n",
            "loss: 1.014757  [   71/ 3200]\n",
            "loss: 0.886291  [   72/ 3200]\n",
            "loss: 1.111750  [   73/ 3200]\n",
            "loss: 0.983137  [   74/ 3200]\n",
            "loss: 1.043877  [   75/ 3200]\n",
            "loss: 1.279506  [   76/ 3200]\n",
            "loss: 0.952397  [   77/ 3200]\n",
            "loss: 1.237812  [   78/ 3200]\n",
            "loss: 1.096994  [   79/ 3200]\n",
            "loss: 0.959943  [   80/ 3200]\n",
            "loss: 1.086542  [   81/ 3200]\n",
            "loss: 1.017094  [   82/ 3200]\n",
            "loss: 1.435392  [   83/ 3200]\n",
            "loss: 0.956071  [   84/ 3200]\n",
            "loss: 1.282785  [   85/ 3200]\n",
            "loss: 1.005494  [   86/ 3200]\n",
            "loss: 0.994762  [   87/ 3200]\n",
            "loss: 1.109349  [   88/ 3200]\n",
            "loss: 1.041929  [   89/ 3200]\n",
            "loss: 1.164983  [   90/ 3200]\n",
            "loss: 1.009828  [   91/ 3200]\n",
            "loss: 1.103596  [   92/ 3200]\n",
            "loss: 1.024997  [   93/ 3200]\n",
            "loss: 1.159439  [   94/ 3200]\n",
            "loss: 1.030148  [   95/ 3200]\n",
            "loss: 1.179822  [   96/ 3200]\n",
            "loss: 1.083672  [   97/ 3200]\n",
            "loss: 1.337153  [   98/ 3200]\n",
            "loss: 1.199669  [   99/ 3200]\n",
            "loss: 1.049670  [  100/ 3200]\n",
            "loss: 0.945946  [  101/ 3200]\n",
            "loss: 1.221821  [  102/ 3200]\n",
            "loss: 1.079884  [  103/ 3200]\n",
            "loss: 0.969035  [  104/ 3200]\n",
            "loss: 1.387239  [  105/ 3200]\n",
            "loss: 1.268831  [  106/ 3200]\n",
            "loss: 1.193904  [  107/ 3200]\n",
            "loss: 1.023770  [  108/ 3200]\n",
            "loss: 1.162491  [  109/ 3200]\n",
            "loss: 1.112717  [  110/ 3200]\n",
            "loss: 1.167595  [  111/ 3200]\n",
            "loss: 1.114784  [  112/ 3200]\n",
            "loss: 0.976785  [  113/ 3200]\n",
            "loss: 0.861005  [  114/ 3200]\n",
            "loss: 0.915554  [  115/ 3200]\n",
            "loss: 1.370144  [  116/ 3200]\n",
            "loss: 1.050750  [  117/ 3200]\n",
            "loss: 1.075464  [  118/ 3200]\n",
            "loss: 1.033751  [  119/ 3200]\n",
            "loss: 1.122315  [  120/ 3200]\n",
            "loss: 1.132962  [  121/ 3200]\n",
            "loss: 0.871397  [  122/ 3200]\n",
            "loss: 1.045862  [  123/ 3200]\n",
            "loss: 1.087654  [  124/ 3200]\n",
            "loss: 1.097121  [  125/ 3200]\n",
            "loss: 0.984571  [  126/ 3200]\n",
            "loss: 1.134166  [  127/ 3200]\n",
            "loss: 0.922524  [  128/ 3200]\n",
            "loss: 1.173364  [  129/ 3200]\n",
            "loss: 1.157641  [  130/ 3200]\n",
            "loss: 1.294428  [  131/ 3200]\n",
            "loss: 1.065870  [  132/ 3200]\n",
            "loss: 0.965544  [  133/ 3200]\n",
            "loss: 1.031180  [  134/ 3200]\n",
            "loss: 1.038313  [  135/ 3200]\n",
            "loss: 1.269155  [  136/ 3200]\n",
            "loss: 1.032765  [  137/ 3200]\n",
            "loss: 1.182044  [  138/ 3200]\n",
            "loss: 1.224707  [  139/ 3200]\n",
            "loss: 0.969616  [  140/ 3200]\n",
            "loss: 1.258313  [  141/ 3200]\n",
            "loss: 1.270110  [  142/ 3200]\n",
            "loss: 1.104145  [  143/ 3200]\n",
            "loss: 1.034765  [  144/ 3200]\n",
            "loss: 1.230240  [  145/ 3200]\n",
            "loss: 1.338788  [  146/ 3200]\n",
            "loss: 1.102446  [  147/ 3200]\n",
            "loss: 0.992614  [  148/ 3200]\n",
            "loss: 1.261441  [  149/ 3200]\n",
            "loss: 1.342340  [  150/ 3200]\n",
            "loss: 1.012130  [  151/ 3200]\n",
            "loss: 1.195377  [  152/ 3200]\n",
            "loss: 0.915534  [  153/ 3200]\n",
            "loss: 1.013548  [  154/ 3200]\n",
            "loss: 1.311656  [  155/ 3200]\n",
            "loss: 1.106882  [  156/ 3200]\n",
            "loss: 1.013096  [  157/ 3200]\n",
            "loss: 1.065391  [  158/ 3200]\n",
            "loss: 1.133981  [  159/ 3200]\n",
            "loss: 0.950371  [  160/ 3200]\n",
            "loss: 1.013787  [  161/ 3200]\n",
            "loss: 1.160158  [  162/ 3200]\n",
            "loss: 1.117463  [  163/ 3200]\n",
            "loss: 1.051265  [  164/ 3200]\n",
            "loss: 0.993031  [  165/ 3200]\n",
            "loss: 1.285730  [  166/ 3200]\n",
            "loss: 1.114987  [  167/ 3200]\n",
            "loss: 1.226981  [  168/ 3200]\n",
            "loss: 1.221400  [  169/ 3200]\n",
            "loss: 1.089598  [  170/ 3200]\n",
            "loss: 1.286645  [  171/ 3200]\n",
            "loss: 1.036503  [  172/ 3200]\n",
            "loss: 1.060522  [  173/ 3200]\n",
            "loss: 1.015265  [  174/ 3200]\n",
            "loss: 1.092635  [  175/ 3200]\n",
            "loss: 0.936185  [  176/ 3200]\n",
            "loss: 1.192736  [  177/ 3200]\n",
            "loss: 1.017025  [  178/ 3200]\n",
            "loss: 1.109487  [  179/ 3200]\n",
            "loss: 1.023720  [  180/ 3200]\n",
            "loss: 1.149637  [  181/ 3200]\n",
            "loss: 0.976720  [  182/ 3200]\n",
            "loss: 0.952279  [  183/ 3200]\n",
            "loss: 1.114745  [  184/ 3200]\n",
            "loss: 0.900337  [  185/ 3200]\n",
            "loss: 0.968965  [  186/ 3200]\n",
            "loss: 1.058964  [  187/ 3200]\n",
            "loss: 1.146859  [  188/ 3200]\n",
            "loss: 1.016995  [  189/ 3200]\n",
            "loss: 1.049804  [  190/ 3200]\n",
            "loss: 1.154671  [  191/ 3200]\n",
            "loss: 1.168994  [  192/ 3200]\n",
            "loss: 1.052774  [  193/ 3200]\n",
            "loss: 1.212906  [  194/ 3200]\n",
            "loss: 1.321086  [  195/ 3200]\n",
            "loss: 0.962193  [  196/ 3200]\n",
            "loss: 1.064308  [  197/ 3200]\n",
            "loss: 1.178638  [  198/ 3200]\n",
            "loss: 1.084510  [  199/ 3200]\n",
            "Epoch:  20\n",
            "loss: 1.309344  [    0/ 3200]\n",
            "loss: 0.848606  [    1/ 3200]\n",
            "loss: 0.865759  [    2/ 3200]\n",
            "loss: 1.103582  [    3/ 3200]\n",
            "loss: 1.123464  [    4/ 3200]\n",
            "loss: 1.056366  [    5/ 3200]\n",
            "loss: 1.141248  [    6/ 3200]\n",
            "loss: 1.184081  [    7/ 3200]\n",
            "loss: 0.996893  [    8/ 3200]\n",
            "loss: 1.114049  [    9/ 3200]\n",
            "loss: 0.915102  [   10/ 3200]\n",
            "loss: 1.039128  [   11/ 3200]\n",
            "loss: 1.136470  [   12/ 3200]\n",
            "loss: 1.139252  [   13/ 3200]\n",
            "loss: 1.162504  [   14/ 3200]\n",
            "loss: 0.950870  [   15/ 3200]\n",
            "loss: 1.028699  [   16/ 3200]\n",
            "loss: 1.138603  [   17/ 3200]\n",
            "loss: 0.995400  [   18/ 3200]\n",
            "loss: 1.123696  [   19/ 3200]\n",
            "loss: 1.017400  [   20/ 3200]\n",
            "loss: 0.921587  [   21/ 3200]\n",
            "loss: 1.059134  [   22/ 3200]\n",
            "loss: 1.290217  [   23/ 3200]\n",
            "loss: 1.009225  [   24/ 3200]\n",
            "loss: 1.293717  [   25/ 3200]\n",
            "loss: 1.088208  [   26/ 3200]\n",
            "loss: 0.995426  [   27/ 3200]\n",
            "loss: 0.967287  [   28/ 3200]\n",
            "loss: 1.157435  [   29/ 3200]\n",
            "loss: 0.929028  [   30/ 3200]\n",
            "loss: 1.100696  [   31/ 3200]\n",
            "loss: 0.996335  [   32/ 3200]\n",
            "loss: 1.046368  [   33/ 3200]\n",
            "loss: 1.102032  [   34/ 3200]\n",
            "loss: 1.013343  [   35/ 3200]\n",
            "loss: 1.102819  [   36/ 3200]\n",
            "loss: 1.085064  [   37/ 3200]\n",
            "loss: 0.990202  [   38/ 3200]\n",
            "loss: 1.304353  [   39/ 3200]\n",
            "loss: 1.374509  [   40/ 3200]\n",
            "loss: 1.129380  [   41/ 3200]\n",
            "loss: 1.115730  [   42/ 3200]\n",
            "loss: 1.264654  [   43/ 3200]\n",
            "loss: 0.964027  [   44/ 3200]\n",
            "loss: 1.215873  [   45/ 3200]\n",
            "loss: 1.048765  [   46/ 3200]\n",
            "loss: 0.979319  [   47/ 3200]\n",
            "loss: 1.082295  [   48/ 3200]\n",
            "loss: 1.103963  [   49/ 3200]\n",
            "loss: 1.222222  [   50/ 3200]\n",
            "loss: 1.325886  [   51/ 3200]\n",
            "loss: 1.072103  [   52/ 3200]\n",
            "loss: 0.989525  [   53/ 3200]\n",
            "loss: 1.033279  [   54/ 3200]\n",
            "loss: 1.018165  [   55/ 3200]\n",
            "loss: 1.341032  [   56/ 3200]\n",
            "loss: 0.866029  [   57/ 3200]\n",
            "loss: 1.211863  [   58/ 3200]\n",
            "loss: 0.986848  [   59/ 3200]\n",
            "loss: 1.183255  [   60/ 3200]\n",
            "loss: 1.224676  [   61/ 3200]\n",
            "loss: 0.919147  [   62/ 3200]\n",
            "loss: 1.140402  [   63/ 3200]\n",
            "loss: 0.963648  [   64/ 3200]\n",
            "loss: 1.036412  [   65/ 3200]\n",
            "loss: 1.200823  [   66/ 3200]\n",
            "loss: 0.729831  [   67/ 3200]\n",
            "loss: 1.153048  [   68/ 3200]\n",
            "loss: 1.124080  [   69/ 3200]\n",
            "loss: 1.288115  [   70/ 3200]\n",
            "loss: 1.069673  [   71/ 3200]\n",
            "loss: 1.080538  [   72/ 3200]\n",
            "loss: 1.413438  [   73/ 3200]\n",
            "loss: 1.049432  [   74/ 3200]\n",
            "loss: 1.065461  [   75/ 3200]\n",
            "loss: 1.084410  [   76/ 3200]\n",
            "loss: 0.944381  [   77/ 3200]\n",
            "loss: 1.104434  [   78/ 3200]\n",
            "loss: 1.133245  [   79/ 3200]\n",
            "loss: 1.045526  [   80/ 3200]\n",
            "loss: 1.096653  [   81/ 3200]\n",
            "loss: 1.121889  [   82/ 3200]\n",
            "loss: 1.070071  [   83/ 3200]\n",
            "loss: 1.089885  [   84/ 3200]\n",
            "loss: 1.020875  [   85/ 3200]\n",
            "loss: 1.068347  [   86/ 3200]\n",
            "loss: 1.032819  [   87/ 3200]\n",
            "loss: 0.888641  [   88/ 3200]\n",
            "loss: 1.132506  [   89/ 3200]\n",
            "loss: 1.245807  [   90/ 3200]\n",
            "loss: 1.128289  [   91/ 3200]\n",
            "loss: 1.076419  [   92/ 3200]\n",
            "loss: 1.031427  [   93/ 3200]\n",
            "loss: 1.045708  [   94/ 3200]\n",
            "loss: 1.169278  [   95/ 3200]\n",
            "loss: 0.975964  [   96/ 3200]\n",
            "loss: 1.231703  [   97/ 3200]\n",
            "loss: 1.083737  [   98/ 3200]\n",
            "loss: 1.042388  [   99/ 3200]\n",
            "loss: 0.913579  [  100/ 3200]\n",
            "loss: 1.149467  [  101/ 3200]\n",
            "loss: 0.946046  [  102/ 3200]\n",
            "loss: 0.953770  [  103/ 3200]\n",
            "loss: 1.172264  [  104/ 3200]\n",
            "loss: 1.193360  [  105/ 3200]\n",
            "loss: 1.088902  [  106/ 3200]\n",
            "loss: 1.182592  [  107/ 3200]\n",
            "loss: 0.925391  [  108/ 3200]\n",
            "loss: 1.137826  [  109/ 3200]\n",
            "loss: 1.239905  [  110/ 3200]\n",
            "loss: 1.062926  [  111/ 3200]\n",
            "loss: 0.872023  [  112/ 3200]\n",
            "loss: 0.979576  [  113/ 3200]\n",
            "loss: 1.163841  [  114/ 3200]\n",
            "loss: 1.023731  [  115/ 3200]\n",
            "loss: 0.932541  [  116/ 3200]\n",
            "loss: 1.186791  [  117/ 3200]\n",
            "loss: 1.067726  [  118/ 3200]\n",
            "loss: 1.066592  [  119/ 3200]\n",
            "loss: 1.069052  [  120/ 3200]\n",
            "loss: 1.102971  [  121/ 3200]\n",
            "loss: 1.036235  [  122/ 3200]\n",
            "loss: 0.869525  [  123/ 3200]\n",
            "loss: 1.141079  [  124/ 3200]\n",
            "loss: 1.063580  [  125/ 3200]\n",
            "loss: 1.057745  [  126/ 3200]\n",
            "loss: 1.206052  [  127/ 3200]\n",
            "loss: 0.957646  [  128/ 3200]\n",
            "loss: 1.042261  [  129/ 3200]\n",
            "loss: 1.241392  [  130/ 3200]\n",
            "loss: 1.141577  [  131/ 3200]\n",
            "loss: 1.209915  [  132/ 3200]\n",
            "loss: 1.224153  [  133/ 3200]\n",
            "loss: 1.057989  [  134/ 3200]\n",
            "loss: 1.072819  [  135/ 3200]\n",
            "loss: 0.885180  [  136/ 3200]\n",
            "loss: 0.919209  [  137/ 3200]\n",
            "loss: 1.035684  [  138/ 3200]\n",
            "loss: 1.045910  [  139/ 3200]\n",
            "loss: 1.071838  [  140/ 3200]\n",
            "loss: 0.968289  [  141/ 3200]\n",
            "loss: 1.003444  [  142/ 3200]\n",
            "loss: 1.168733  [  143/ 3200]\n",
            "loss: 0.988757  [  144/ 3200]\n",
            "loss: 1.215311  [  145/ 3200]\n",
            "loss: 0.816775  [  146/ 3200]\n",
            "loss: 1.055022  [  147/ 3200]\n",
            "loss: 1.074494  [  148/ 3200]\n",
            "loss: 1.131088  [  149/ 3200]\n",
            "loss: 0.902153  [  150/ 3200]\n",
            "loss: 0.987466  [  151/ 3200]\n",
            "loss: 0.906297  [  152/ 3200]\n",
            "loss: 1.048917  [  153/ 3200]\n",
            "loss: 1.164665  [  154/ 3200]\n",
            "loss: 0.971248  [  155/ 3200]\n",
            "loss: 1.156711  [  156/ 3200]\n",
            "loss: 0.940910  [  157/ 3200]\n",
            "loss: 1.045053  [  158/ 3200]\n",
            "loss: 1.058735  [  159/ 3200]\n",
            "loss: 1.023920  [  160/ 3200]\n",
            "loss: 1.116316  [  161/ 3200]\n",
            "loss: 1.101232  [  162/ 3200]\n",
            "loss: 0.969260  [  163/ 3200]\n",
            "loss: 1.237185  [  164/ 3200]\n",
            "loss: 0.986713  [  165/ 3200]\n",
            "loss: 0.921608  [  166/ 3200]\n",
            "loss: 1.136932  [  167/ 3200]\n",
            "loss: 1.079237  [  168/ 3200]\n",
            "loss: 0.934546  [  169/ 3200]\n",
            "loss: 0.995994  [  170/ 3200]\n",
            "loss: 1.013762  [  171/ 3200]\n",
            "loss: 1.031791  [  172/ 3200]\n",
            "loss: 0.890131  [  173/ 3200]\n",
            "loss: 1.045969  [  174/ 3200]\n",
            "loss: 1.084557  [  175/ 3200]\n",
            "loss: 1.147177  [  176/ 3200]\n",
            "loss: 0.890597  [  177/ 3200]\n",
            "loss: 0.887280  [  178/ 3200]\n",
            "loss: 1.089008  [  179/ 3200]\n",
            "loss: 1.055955  [  180/ 3200]\n",
            "loss: 1.029768  [  181/ 3200]\n",
            "loss: 1.081719  [  182/ 3200]\n",
            "loss: 1.171145  [  183/ 3200]\n",
            "loss: 1.015134  [  184/ 3200]\n",
            "loss: 0.997659  [  185/ 3200]\n",
            "loss: 1.312272  [  186/ 3200]\n",
            "loss: 1.003470  [  187/ 3200]\n",
            "loss: 0.896417  [  188/ 3200]\n",
            "loss: 0.997500  [  189/ 3200]\n",
            "loss: 1.040082  [  190/ 3200]\n",
            "loss: 1.194047  [  191/ 3200]\n",
            "loss: 1.015130  [  192/ 3200]\n",
            "loss: 1.344664  [  193/ 3200]\n",
            "loss: 1.089191  [  194/ 3200]\n",
            "loss: 1.290766  [  195/ 3200]\n",
            "loss: 1.057043  [  196/ 3200]\n",
            "loss: 1.080584  [  197/ 3200]\n",
            "loss: 1.002756  [  198/ 3200]\n",
            "loss: 1.185733  [  199/ 3200]\n",
            "Epoch:  21\n",
            "loss: 0.998111  [    0/ 3200]\n",
            "loss: 0.839244  [    1/ 3200]\n",
            "loss: 0.978772  [    2/ 3200]\n",
            "loss: 1.165053  [    3/ 3200]\n",
            "loss: 0.780378  [    4/ 3200]\n",
            "loss: 1.083374  [    5/ 3200]\n",
            "loss: 1.223818  [    6/ 3200]\n",
            "loss: 0.836708  [    7/ 3200]\n",
            "loss: 1.105232  [    8/ 3200]\n",
            "loss: 1.004106  [    9/ 3200]\n",
            "loss: 0.940167  [   10/ 3200]\n",
            "loss: 1.210044  [   11/ 3200]\n",
            "loss: 1.047297  [   12/ 3200]\n",
            "loss: 1.064190  [   13/ 3200]\n",
            "loss: 1.035747  [   14/ 3200]\n",
            "loss: 0.972270  [   15/ 3200]\n",
            "loss: 1.057423  [   16/ 3200]\n",
            "loss: 0.948139  [   17/ 3200]\n",
            "loss: 1.215709  [   18/ 3200]\n",
            "loss: 1.013854  [   19/ 3200]\n",
            "loss: 1.191184  [   20/ 3200]\n",
            "loss: 0.933710  [   21/ 3200]\n",
            "loss: 1.017176  [   22/ 3200]\n",
            "loss: 1.014183  [   23/ 3200]\n",
            "loss: 1.146230  [   24/ 3200]\n",
            "loss: 0.970607  [   25/ 3200]\n",
            "loss: 1.061133  [   26/ 3200]\n",
            "loss: 1.057461  [   27/ 3200]\n",
            "loss: 1.096872  [   28/ 3200]\n",
            "loss: 1.085652  [   29/ 3200]\n",
            "loss: 1.086809  [   30/ 3200]\n",
            "loss: 0.982685  [   31/ 3200]\n",
            "loss: 1.003144  [   32/ 3200]\n",
            "loss: 1.091560  [   33/ 3200]\n",
            "loss: 1.050041  [   34/ 3200]\n",
            "loss: 1.182593  [   35/ 3200]\n",
            "loss: 0.880486  [   36/ 3200]\n",
            "loss: 0.971577  [   37/ 3200]\n",
            "loss: 1.120197  [   38/ 3200]\n",
            "loss: 1.201171  [   39/ 3200]\n",
            "loss: 1.007610  [   40/ 3200]\n",
            "loss: 1.140586  [   41/ 3200]\n",
            "loss: 1.095519  [   42/ 3200]\n",
            "loss: 0.984614  [   43/ 3200]\n",
            "loss: 0.957006  [   44/ 3200]\n",
            "loss: 1.249223  [   45/ 3200]\n",
            "loss: 0.949699  [   46/ 3200]\n",
            "loss: 1.018262  [   47/ 3200]\n",
            "loss: 1.240795  [   48/ 3200]\n",
            "loss: 1.160475  [   49/ 3200]\n",
            "loss: 1.174332  [   50/ 3200]\n",
            "loss: 0.877039  [   51/ 3200]\n",
            "loss: 1.217602  [   52/ 3200]\n",
            "loss: 1.090919  [   53/ 3200]\n",
            "loss: 0.978892  [   54/ 3200]\n",
            "loss: 1.127538  [   55/ 3200]\n",
            "loss: 1.047264  [   56/ 3200]\n",
            "loss: 1.030976  [   57/ 3200]\n",
            "loss: 1.198738  [   58/ 3200]\n",
            "loss: 1.337402  [   59/ 3200]\n",
            "loss: 1.135499  [   60/ 3200]\n",
            "loss: 1.212068  [   61/ 3200]\n",
            "loss: 0.991139  [   62/ 3200]\n",
            "loss: 0.795938  [   63/ 3200]\n",
            "loss: 1.069654  [   64/ 3200]\n",
            "loss: 1.122054  [   65/ 3200]\n",
            "loss: 0.975943  [   66/ 3200]\n",
            "loss: 1.156650  [   67/ 3200]\n",
            "loss: 1.181329  [   68/ 3200]\n",
            "loss: 1.035225  [   69/ 3200]\n",
            "loss: 1.145087  [   70/ 3200]\n",
            "loss: 1.045220  [   71/ 3200]\n",
            "loss: 0.923920  [   72/ 3200]\n",
            "loss: 1.034346  [   73/ 3200]\n",
            "loss: 1.086459  [   74/ 3200]\n",
            "loss: 1.419259  [   75/ 3200]\n",
            "loss: 1.182883  [   76/ 3200]\n",
            "loss: 1.161696  [   77/ 3200]\n",
            "loss: 1.105564  [   78/ 3200]\n",
            "loss: 1.160405  [   79/ 3200]\n",
            "loss: 1.033666  [   80/ 3200]\n",
            "loss: 1.133669  [   81/ 3200]\n",
            "loss: 1.132525  [   82/ 3200]\n",
            "loss: 0.946296  [   83/ 3200]\n",
            "loss: 1.131462  [   84/ 3200]\n",
            "loss: 0.888658  [   85/ 3200]\n",
            "loss: 1.053993  [   86/ 3200]\n",
            "loss: 1.008493  [   87/ 3200]\n",
            "loss: 1.002145  [   88/ 3200]\n",
            "loss: 1.058059  [   89/ 3200]\n",
            "loss: 1.145211  [   90/ 3200]\n",
            "loss: 0.938306  [   91/ 3200]\n",
            "loss: 1.119984  [   92/ 3200]\n",
            "loss: 1.000744  [   93/ 3200]\n",
            "loss: 1.211213  [   94/ 3200]\n",
            "loss: 1.022750  [   95/ 3200]\n",
            "loss: 1.300858  [   96/ 3200]\n",
            "loss: 1.072934  [   97/ 3200]\n",
            "loss: 0.959257  [   98/ 3200]\n",
            "loss: 0.983208  [   99/ 3200]\n",
            "loss: 1.157077  [  100/ 3200]\n",
            "loss: 1.150066  [  101/ 3200]\n",
            "loss: 1.104122  [  102/ 3200]\n",
            "loss: 1.157426  [  103/ 3200]\n",
            "loss: 0.978297  [  104/ 3200]\n",
            "loss: 1.083685  [  105/ 3200]\n",
            "loss: 1.090477  [  106/ 3200]\n",
            "loss: 1.098246  [  107/ 3200]\n",
            "loss: 1.103873  [  108/ 3200]\n",
            "loss: 0.861652  [  109/ 3200]\n",
            "loss: 1.204211  [  110/ 3200]\n",
            "loss: 1.257088  [  111/ 3200]\n",
            "loss: 1.245172  [  112/ 3200]\n",
            "loss: 0.854741  [  113/ 3200]\n",
            "loss: 0.997746  [  114/ 3200]\n",
            "loss: 1.016870  [  115/ 3200]\n",
            "loss: 1.210418  [  116/ 3200]\n",
            "loss: 1.042290  [  117/ 3200]\n",
            "loss: 1.097417  [  118/ 3200]\n",
            "loss: 1.130952  [  119/ 3200]\n",
            "loss: 1.214717  [  120/ 3200]\n",
            "loss: 1.158559  [  121/ 3200]\n",
            "loss: 1.114925  [  122/ 3200]\n",
            "loss: 1.031785  [  123/ 3200]\n",
            "loss: 0.844366  [  124/ 3200]\n",
            "loss: 1.020665  [  125/ 3200]\n",
            "loss: 1.062608  [  126/ 3200]\n",
            "loss: 1.101922  [  127/ 3200]\n",
            "loss: 1.052697  [  128/ 3200]\n",
            "loss: 0.904885  [  129/ 3200]\n",
            "loss: 0.896874  [  130/ 3200]\n",
            "loss: 1.006084  [  131/ 3200]\n",
            "loss: 1.255530  [  132/ 3200]\n",
            "loss: 0.935444  [  133/ 3200]\n",
            "loss: 0.948846  [  134/ 3200]\n",
            "loss: 0.704302  [  135/ 3200]\n",
            "loss: 0.991124  [  136/ 3200]\n",
            "loss: 1.318977  [  137/ 3200]\n",
            "loss: 1.073099  [  138/ 3200]\n",
            "loss: 1.112363  [  139/ 3200]\n",
            "loss: 1.071507  [  140/ 3200]\n",
            "loss: 1.310043  [  141/ 3200]\n",
            "loss: 1.143619  [  142/ 3200]\n",
            "loss: 0.867427  [  143/ 3200]\n",
            "loss: 0.908718  [  144/ 3200]\n",
            "loss: 1.000310  [  145/ 3200]\n",
            "loss: 1.234996  [  146/ 3200]\n",
            "loss: 1.140088  [  147/ 3200]\n",
            "loss: 1.081362  [  148/ 3200]\n",
            "loss: 1.105404  [  149/ 3200]\n",
            "loss: 0.998212  [  150/ 3200]\n",
            "loss: 0.950292  [  151/ 3200]\n",
            "loss: 1.262170  [  152/ 3200]\n",
            "loss: 0.961784  [  153/ 3200]\n",
            "loss: 0.883102  [  154/ 3200]\n",
            "loss: 1.222649  [  155/ 3200]\n",
            "loss: 1.173544  [  156/ 3200]\n",
            "loss: 1.066837  [  157/ 3200]\n",
            "loss: 0.992577  [  158/ 3200]\n",
            "loss: 1.129453  [  159/ 3200]\n",
            "loss: 1.164230  [  160/ 3200]\n",
            "loss: 1.035118  [  161/ 3200]\n",
            "loss: 1.138779  [  162/ 3200]\n",
            "loss: 1.175406  [  163/ 3200]\n",
            "loss: 1.024344  [  164/ 3200]\n",
            "loss: 1.040230  [  165/ 3200]\n",
            "loss: 0.926215  [  166/ 3200]\n",
            "loss: 0.804407  [  167/ 3200]\n",
            "loss: 0.832857  [  168/ 3200]\n",
            "loss: 1.273029  [  169/ 3200]\n",
            "loss: 1.087507  [  170/ 3200]\n",
            "loss: 1.016804  [  171/ 3200]\n",
            "loss: 1.058220  [  172/ 3200]\n",
            "loss: 1.137114  [  173/ 3200]\n",
            "loss: 1.022773  [  174/ 3200]\n",
            "loss: 1.000612  [  175/ 3200]\n",
            "loss: 1.283618  [  176/ 3200]\n",
            "loss: 0.899737  [  177/ 3200]\n",
            "loss: 1.164959  [  178/ 3200]\n",
            "loss: 1.128544  [  179/ 3200]\n",
            "loss: 0.660689  [  180/ 3200]\n",
            "loss: 1.244984  [  181/ 3200]\n",
            "loss: 1.053298  [  182/ 3200]\n",
            "loss: 1.187817  [  183/ 3200]\n",
            "loss: 0.830836  [  184/ 3200]\n",
            "loss: 1.226512  [  185/ 3200]\n",
            "loss: 1.196185  [  186/ 3200]\n",
            "loss: 1.214830  [  187/ 3200]\n",
            "loss: 1.202479  [  188/ 3200]\n",
            "loss: 0.951243  [  189/ 3200]\n",
            "loss: 1.019384  [  190/ 3200]\n",
            "loss: 1.220012  [  191/ 3200]\n",
            "loss: 0.892220  [  192/ 3200]\n",
            "loss: 0.953995  [  193/ 3200]\n",
            "loss: 0.932167  [  194/ 3200]\n",
            "loss: 0.976633  [  195/ 3200]\n",
            "loss: 1.009256  [  196/ 3200]\n",
            "loss: 1.004332  [  197/ 3200]\n",
            "loss: 1.119950  [  198/ 3200]\n",
            "loss: 1.029927  [  199/ 3200]\n",
            "Epoch:  22\n",
            "loss: 0.908258  [    0/ 3200]\n",
            "loss: 1.010532  [    1/ 3200]\n",
            "loss: 1.128510  [    2/ 3200]\n",
            "loss: 1.012740  [    3/ 3200]\n",
            "loss: 1.015058  [    4/ 3200]\n",
            "loss: 1.098908  [    5/ 3200]\n",
            "loss: 1.075040  [    6/ 3200]\n",
            "loss: 1.055918  [    7/ 3200]\n",
            "loss: 1.036298  [    8/ 3200]\n",
            "loss: 1.108346  [    9/ 3200]\n",
            "loss: 0.963082  [   10/ 3200]\n",
            "loss: 0.965923  [   11/ 3200]\n",
            "loss: 1.025770  [   12/ 3200]\n",
            "loss: 0.847273  [   13/ 3200]\n",
            "loss: 0.953913  [   14/ 3200]\n",
            "loss: 1.141574  [   15/ 3200]\n",
            "loss: 1.071899  [   16/ 3200]\n",
            "loss: 1.313359  [   17/ 3200]\n",
            "loss: 0.890805  [   18/ 3200]\n",
            "loss: 0.943218  [   19/ 3200]\n",
            "loss: 1.165523  [   20/ 3200]\n",
            "loss: 1.131993  [   21/ 3200]\n",
            "loss: 1.014243  [   22/ 3200]\n",
            "loss: 1.130121  [   23/ 3200]\n",
            "loss: 1.315660  [   24/ 3200]\n",
            "loss: 0.987790  [   25/ 3200]\n",
            "loss: 0.900599  [   26/ 3200]\n",
            "loss: 0.947442  [   27/ 3200]\n",
            "loss: 0.938834  [   28/ 3200]\n",
            "loss: 1.026735  [   29/ 3200]\n",
            "loss: 1.071016  [   30/ 3200]\n",
            "loss: 0.807662  [   31/ 3200]\n",
            "loss: 0.945253  [   32/ 3200]\n",
            "loss: 1.015584  [   33/ 3200]\n",
            "loss: 1.164775  [   34/ 3200]\n",
            "loss: 1.022070  [   35/ 3200]\n",
            "loss: 1.323622  [   36/ 3200]\n",
            "loss: 1.099339  [   37/ 3200]\n",
            "loss: 0.896693  [   38/ 3200]\n",
            "loss: 1.056627  [   39/ 3200]\n",
            "loss: 1.129979  [   40/ 3200]\n",
            "loss: 1.053753  [   41/ 3200]\n",
            "loss: 1.083894  [   42/ 3200]\n",
            "loss: 1.128310  [   43/ 3200]\n",
            "loss: 1.081438  [   44/ 3200]\n",
            "loss: 1.343139  [   45/ 3200]\n",
            "loss: 0.948073  [   46/ 3200]\n",
            "loss: 1.041427  [   47/ 3200]\n",
            "loss: 1.100974  [   48/ 3200]\n",
            "loss: 0.923356  [   49/ 3200]\n",
            "loss: 1.090354  [   50/ 3200]\n",
            "loss: 0.974204  [   51/ 3200]\n",
            "loss: 1.255481  [   52/ 3200]\n",
            "loss: 1.069037  [   53/ 3200]\n",
            "loss: 1.092192  [   54/ 3200]\n",
            "loss: 1.003976  [   55/ 3200]\n",
            "loss: 0.872865  [   56/ 3200]\n",
            "loss: 0.847564  [   57/ 3200]\n",
            "loss: 0.963895  [   58/ 3200]\n",
            "loss: 0.951205  [   59/ 3200]\n",
            "loss: 0.869541  [   60/ 3200]\n",
            "loss: 0.923132  [   61/ 3200]\n",
            "loss: 1.012075  [   62/ 3200]\n",
            "loss: 1.028189  [   63/ 3200]\n",
            "loss: 1.032896  [   64/ 3200]\n",
            "loss: 0.960558  [   65/ 3200]\n",
            "loss: 1.165359  [   66/ 3200]\n",
            "loss: 1.139697  [   67/ 3200]\n",
            "loss: 1.026874  [   68/ 3200]\n",
            "loss: 1.160597  [   69/ 3200]\n",
            "loss: 0.812339  [   70/ 3200]\n",
            "loss: 0.820098  [   71/ 3200]\n",
            "loss: 0.906178  [   72/ 3200]\n",
            "loss: 1.381572  [   73/ 3200]\n",
            "loss: 0.951581  [   74/ 3200]\n",
            "loss: 1.157140  [   75/ 3200]\n",
            "loss: 1.108967  [   76/ 3200]\n",
            "loss: 0.980853  [   77/ 3200]\n",
            "loss: 1.140443  [   78/ 3200]\n",
            "loss: 1.017803  [   79/ 3200]\n",
            "loss: 1.149499  [   80/ 3200]\n",
            "loss: 0.902990  [   81/ 3200]\n",
            "loss: 1.445248  [   82/ 3200]\n",
            "loss: 0.929070  [   83/ 3200]\n",
            "loss: 1.122334  [   84/ 3200]\n",
            "loss: 0.951202  [   85/ 3200]\n",
            "loss: 1.185381  [   86/ 3200]\n",
            "loss: 1.191170  [   87/ 3200]\n",
            "loss: 1.090085  [   88/ 3200]\n",
            "loss: 0.960124  [   89/ 3200]\n",
            "loss: 1.049546  [   90/ 3200]\n",
            "loss: 0.870255  [   91/ 3200]\n",
            "loss: 0.818091  [   92/ 3200]\n",
            "loss: 1.267058  [   93/ 3200]\n",
            "loss: 1.123068  [   94/ 3200]\n",
            "loss: 0.997256  [   95/ 3200]\n",
            "loss: 1.089520  [   96/ 3200]\n",
            "loss: 1.220740  [   97/ 3200]\n",
            "loss: 0.993618  [   98/ 3200]\n",
            "loss: 1.082004  [   99/ 3200]\n",
            "loss: 1.037100  [  100/ 3200]\n",
            "loss: 1.001885  [  101/ 3200]\n",
            "loss: 1.029906  [  102/ 3200]\n",
            "loss: 1.063306  [  103/ 3200]\n",
            "loss: 1.005950  [  104/ 3200]\n",
            "loss: 1.174137  [  105/ 3200]\n",
            "loss: 1.147965  [  106/ 3200]\n",
            "loss: 0.836452  [  107/ 3200]\n",
            "loss: 1.156455  [  108/ 3200]\n",
            "loss: 1.040825  [  109/ 3200]\n",
            "loss: 1.117541  [  110/ 3200]\n",
            "loss: 0.980582  [  111/ 3200]\n",
            "loss: 1.125520  [  112/ 3200]\n",
            "loss: 0.821728  [  113/ 3200]\n",
            "loss: 0.964692  [  114/ 3200]\n",
            "loss: 1.150737  [  115/ 3200]\n",
            "loss: 0.981719  [  116/ 3200]\n",
            "loss: 1.092653  [  117/ 3200]\n",
            "loss: 0.997484  [  118/ 3200]\n",
            "loss: 1.100411  [  119/ 3200]\n",
            "loss: 1.108742  [  120/ 3200]\n",
            "loss: 1.012398  [  121/ 3200]\n",
            "loss: 1.024571  [  122/ 3200]\n",
            "loss: 1.017380  [  123/ 3200]\n",
            "loss: 1.172052  [  124/ 3200]\n",
            "loss: 0.829592  [  125/ 3200]\n",
            "loss: 1.216221  [  126/ 3200]\n",
            "loss: 0.973096  [  127/ 3200]\n",
            "loss: 1.130363  [  128/ 3200]\n",
            "loss: 0.952943  [  129/ 3200]\n",
            "loss: 1.170859  [  130/ 3200]\n",
            "loss: 1.100315  [  131/ 3200]\n",
            "loss: 1.007120  [  132/ 3200]\n",
            "loss: 0.990889  [  133/ 3200]\n",
            "loss: 0.982881  [  134/ 3200]\n",
            "loss: 0.924477  [  135/ 3200]\n",
            "loss: 1.106484  [  136/ 3200]\n",
            "loss: 0.835929  [  137/ 3200]\n",
            "loss: 1.158318  [  138/ 3200]\n",
            "loss: 0.983354  [  139/ 3200]\n",
            "loss: 0.972239  [  140/ 3200]\n",
            "loss: 1.188268  [  141/ 3200]\n",
            "loss: 0.961887  [  142/ 3200]\n",
            "loss: 0.886091  [  143/ 3200]\n",
            "loss: 1.021972  [  144/ 3200]\n",
            "loss: 0.956475  [  145/ 3200]\n",
            "loss: 1.159120  [  146/ 3200]\n",
            "loss: 1.012317  [  147/ 3200]\n",
            "loss: 0.897496  [  148/ 3200]\n",
            "loss: 1.035133  [  149/ 3200]\n",
            "loss: 0.837593  [  150/ 3200]\n",
            "loss: 1.283302  [  151/ 3200]\n",
            "loss: 1.136149  [  152/ 3200]\n",
            "loss: 1.121842  [  153/ 3200]\n",
            "loss: 1.006791  [  154/ 3200]\n",
            "loss: 0.876890  [  155/ 3200]\n",
            "loss: 0.823550  [  156/ 3200]\n",
            "loss: 0.951952  [  157/ 3200]\n",
            "loss: 1.097058  [  158/ 3200]\n",
            "loss: 1.050549  [  159/ 3200]\n",
            "loss: 0.936346  [  160/ 3200]\n",
            "loss: 0.959895  [  161/ 3200]\n",
            "loss: 1.026632  [  162/ 3200]\n",
            "loss: 1.018365  [  163/ 3200]\n",
            "loss: 1.196058  [  164/ 3200]\n",
            "loss: 1.172863  [  165/ 3200]\n",
            "loss: 1.165077  [  166/ 3200]\n",
            "loss: 1.020748  [  167/ 3200]\n",
            "loss: 0.782831  [  168/ 3200]\n",
            "loss: 1.327096  [  169/ 3200]\n",
            "loss: 0.999534  [  170/ 3200]\n",
            "loss: 1.010945  [  171/ 3200]\n",
            "loss: 1.002580  [  172/ 3200]\n",
            "loss: 0.952886  [  173/ 3200]\n",
            "loss: 0.944546  [  174/ 3200]\n",
            "loss: 0.983733  [  175/ 3200]\n",
            "loss: 1.142131  [  176/ 3200]\n",
            "loss: 1.189596  [  177/ 3200]\n",
            "loss: 0.969330  [  178/ 3200]\n",
            "loss: 1.189961  [  179/ 3200]\n",
            "loss: 1.012288  [  180/ 3200]\n",
            "loss: 1.026599  [  181/ 3200]\n",
            "loss: 1.188926  [  182/ 3200]\n",
            "loss: 1.094951  [  183/ 3200]\n",
            "loss: 1.123305  [  184/ 3200]\n",
            "loss: 1.016974  [  185/ 3200]\n",
            "loss: 1.207013  [  186/ 3200]\n",
            "loss: 1.037110  [  187/ 3200]\n",
            "loss: 1.049951  [  188/ 3200]\n",
            "loss: 1.148371  [  189/ 3200]\n",
            "loss: 1.060523  [  190/ 3200]\n",
            "loss: 1.000084  [  191/ 3200]\n",
            "loss: 1.051868  [  192/ 3200]\n",
            "loss: 1.089117  [  193/ 3200]\n",
            "loss: 1.008092  [  194/ 3200]\n",
            "loss: 1.342128  [  195/ 3200]\n",
            "loss: 1.188062  [  196/ 3200]\n",
            "loss: 1.005784  [  197/ 3200]\n",
            "loss: 1.073966  [  198/ 3200]\n",
            "loss: 1.138072  [  199/ 3200]\n",
            "Epoch:  23\n",
            "loss: 0.960391  [    0/ 3200]\n",
            "loss: 1.017477  [    1/ 3200]\n",
            "loss: 1.061630  [    2/ 3200]\n",
            "loss: 1.073918  [    3/ 3200]\n",
            "loss: 0.770427  [    4/ 3200]\n",
            "loss: 0.947837  [    5/ 3200]\n",
            "loss: 0.911458  [    6/ 3200]\n",
            "loss: 0.982730  [    7/ 3200]\n",
            "loss: 0.892140  [    8/ 3200]\n",
            "loss: 1.108948  [    9/ 3200]\n",
            "loss: 0.979040  [   10/ 3200]\n",
            "loss: 0.981790  [   11/ 3200]\n",
            "loss: 1.606062  [   12/ 3200]\n",
            "loss: 0.876370  [   13/ 3200]\n",
            "loss: 1.074230  [   14/ 3200]\n",
            "loss: 1.199050  [   15/ 3200]\n",
            "loss: 1.160471  [   16/ 3200]\n",
            "loss: 0.921407  [   17/ 3200]\n",
            "loss: 1.053654  [   18/ 3200]\n",
            "loss: 1.027653  [   19/ 3200]\n",
            "loss: 1.004014  [   20/ 3200]\n",
            "loss: 1.260824  [   21/ 3200]\n",
            "loss: 1.114926  [   22/ 3200]\n",
            "loss: 1.181605  [   23/ 3200]\n",
            "loss: 0.982941  [   24/ 3200]\n",
            "loss: 0.961363  [   25/ 3200]\n",
            "loss: 0.985396  [   26/ 3200]\n",
            "loss: 0.952767  [   27/ 3200]\n",
            "loss: 1.028764  [   28/ 3200]\n",
            "loss: 1.005938  [   29/ 3200]\n",
            "loss: 0.940198  [   30/ 3200]\n",
            "loss: 1.219061  [   31/ 3200]\n",
            "loss: 1.373472  [   32/ 3200]\n",
            "loss: 1.015499  [   33/ 3200]\n",
            "loss: 0.951412  [   34/ 3200]\n",
            "loss: 0.962814  [   35/ 3200]\n",
            "loss: 0.984325  [   36/ 3200]\n",
            "loss: 0.861133  [   37/ 3200]\n",
            "loss: 0.870446  [   38/ 3200]\n",
            "loss: 0.922988  [   39/ 3200]\n",
            "loss: 1.069113  [   40/ 3200]\n",
            "loss: 0.939246  [   41/ 3200]\n",
            "loss: 1.189068  [   42/ 3200]\n",
            "loss: 1.011458  [   43/ 3200]\n",
            "loss: 1.263433  [   44/ 3200]\n",
            "loss: 1.144253  [   45/ 3200]\n",
            "loss: 0.949745  [   46/ 3200]\n",
            "loss: 0.873498  [   47/ 3200]\n",
            "loss: 0.933036  [   48/ 3200]\n",
            "loss: 1.074700  [   49/ 3200]\n",
            "loss: 1.152449  [   50/ 3200]\n",
            "loss: 0.845905  [   51/ 3200]\n",
            "loss: 0.977338  [   52/ 3200]\n",
            "loss: 1.216973  [   53/ 3200]\n",
            "loss: 1.192469  [   54/ 3200]\n",
            "loss: 1.058513  [   55/ 3200]\n",
            "loss: 1.142043  [   56/ 3200]\n",
            "loss: 1.258449  [   57/ 3200]\n",
            "loss: 1.091968  [   58/ 3200]\n",
            "loss: 1.399578  [   59/ 3200]\n",
            "loss: 1.158111  [   60/ 3200]\n",
            "loss: 0.770215  [   61/ 3200]\n",
            "loss: 1.188926  [   62/ 3200]\n",
            "loss: 1.114164  [   63/ 3200]\n",
            "loss: 0.978210  [   64/ 3200]\n",
            "loss: 1.147168  [   65/ 3200]\n",
            "loss: 1.194824  [   66/ 3200]\n",
            "loss: 0.770944  [   67/ 3200]\n",
            "loss: 1.213492  [   68/ 3200]\n",
            "loss: 0.978204  [   69/ 3200]\n",
            "loss: 0.858658  [   70/ 3200]\n",
            "loss: 0.940543  [   71/ 3200]\n",
            "loss: 0.953722  [   72/ 3200]\n",
            "loss: 1.091140  [   73/ 3200]\n",
            "loss: 0.790043  [   74/ 3200]\n",
            "loss: 1.012856  [   75/ 3200]\n",
            "loss: 1.044005  [   76/ 3200]\n",
            "loss: 0.941187  [   77/ 3200]\n",
            "loss: 1.117509  [   78/ 3200]\n",
            "loss: 1.067057  [   79/ 3200]\n",
            "loss: 1.018976  [   80/ 3200]\n",
            "loss: 1.105831  [   81/ 3200]\n",
            "loss: 1.360154  [   82/ 3200]\n",
            "loss: 0.985461  [   83/ 3200]\n",
            "loss: 1.194585  [   84/ 3200]\n",
            "loss: 1.050332  [   85/ 3200]\n",
            "loss: 0.812601  [   86/ 3200]\n",
            "loss: 1.301775  [   87/ 3200]\n",
            "loss: 1.112509  [   88/ 3200]\n",
            "loss: 0.923841  [   89/ 3200]\n",
            "loss: 0.991348  [   90/ 3200]\n",
            "loss: 1.111604  [   91/ 3200]\n",
            "loss: 0.876371  [   92/ 3200]\n",
            "loss: 1.084062  [   93/ 3200]\n",
            "loss: 1.166216  [   94/ 3200]\n",
            "loss: 1.089069  [   95/ 3200]\n",
            "loss: 1.066404  [   96/ 3200]\n",
            "loss: 0.984782  [   97/ 3200]\n",
            "loss: 1.029301  [   98/ 3200]\n",
            "loss: 1.094986  [   99/ 3200]\n",
            "loss: 0.780751  [  100/ 3200]\n",
            "loss: 0.962922  [  101/ 3200]\n",
            "loss: 0.853742  [  102/ 3200]\n",
            "loss: 0.889124  [  103/ 3200]\n",
            "loss: 1.268948  [  104/ 3200]\n",
            "loss: 1.095968  [  105/ 3200]\n",
            "loss: 1.148315  [  106/ 3200]\n",
            "loss: 1.052746  [  107/ 3200]\n",
            "loss: 1.061521  [  108/ 3200]\n",
            "loss: 0.921370  [  109/ 3200]\n",
            "loss: 0.759506  [  110/ 3200]\n",
            "loss: 0.834588  [  111/ 3200]\n",
            "loss: 1.081886  [  112/ 3200]\n",
            "loss: 1.014084  [  113/ 3200]\n",
            "loss: 0.942622  [  114/ 3200]\n",
            "loss: 1.044478  [  115/ 3200]\n",
            "loss: 1.033245  [  116/ 3200]\n",
            "loss: 1.126726  [  117/ 3200]\n",
            "loss: 1.086807  [  118/ 3200]\n",
            "loss: 1.029843  [  119/ 3200]\n",
            "loss: 0.849418  [  120/ 3200]\n",
            "loss: 1.120360  [  121/ 3200]\n",
            "loss: 0.868889  [  122/ 3200]\n",
            "loss: 0.922940  [  123/ 3200]\n",
            "loss: 0.859447  [  124/ 3200]\n",
            "loss: 1.118074  [  125/ 3200]\n",
            "loss: 0.785005  [  126/ 3200]\n",
            "loss: 0.976707  [  127/ 3200]\n",
            "loss: 1.078165  [  128/ 3200]\n",
            "loss: 0.886003  [  129/ 3200]\n",
            "loss: 0.930675  [  130/ 3200]\n",
            "loss: 1.324948  [  131/ 3200]\n",
            "loss: 1.084079  [  132/ 3200]\n",
            "loss: 0.946185  [  133/ 3200]\n",
            "loss: 0.964503  [  134/ 3200]\n",
            "loss: 0.895755  [  135/ 3200]\n",
            "loss: 0.906970  [  136/ 3200]\n",
            "loss: 1.144434  [  137/ 3200]\n",
            "loss: 0.911554  [  138/ 3200]\n",
            "loss: 1.101042  [  139/ 3200]\n",
            "loss: 0.836950  [  140/ 3200]\n",
            "loss: 1.186167  [  141/ 3200]\n",
            "loss: 1.011392  [  142/ 3200]\n",
            "loss: 1.169310  [  143/ 3200]\n",
            "loss: 1.181042  [  144/ 3200]\n",
            "loss: 1.028935  [  145/ 3200]\n",
            "loss: 1.269415  [  146/ 3200]\n",
            "loss: 0.983314  [  147/ 3200]\n",
            "loss: 0.933619  [  148/ 3200]\n",
            "loss: 0.954340  [  149/ 3200]\n",
            "loss: 1.169284  [  150/ 3200]\n",
            "loss: 1.058210  [  151/ 3200]\n",
            "loss: 0.790709  [  152/ 3200]\n",
            "loss: 0.854291  [  153/ 3200]\n",
            "loss: 1.092565  [  154/ 3200]\n",
            "loss: 0.951274  [  155/ 3200]\n",
            "loss: 1.278551  [  156/ 3200]\n",
            "loss: 0.803047  [  157/ 3200]\n",
            "loss: 1.093876  [  158/ 3200]\n",
            "loss: 1.024555  [  159/ 3200]\n",
            "loss: 0.986097  [  160/ 3200]\n",
            "loss: 1.003096  [  161/ 3200]\n",
            "loss: 0.915314  [  162/ 3200]\n",
            "loss: 1.035528  [  163/ 3200]\n",
            "loss: 1.092317  [  164/ 3200]\n",
            "loss: 1.209642  [  165/ 3200]\n",
            "loss: 1.184130  [  166/ 3200]\n",
            "loss: 1.106167  [  167/ 3200]\n",
            "loss: 0.934656  [  168/ 3200]\n",
            "loss: 0.899365  [  169/ 3200]\n",
            "loss: 1.084927  [  170/ 3200]\n",
            "loss: 1.110390  [  171/ 3200]\n",
            "loss: 1.431544  [  172/ 3200]\n",
            "loss: 1.074700  [  173/ 3200]\n",
            "loss: 1.015994  [  174/ 3200]\n",
            "loss: 0.978441  [  175/ 3200]\n",
            "loss: 0.835349  [  176/ 3200]\n",
            "loss: 1.110292  [  177/ 3200]\n",
            "loss: 1.007956  [  178/ 3200]\n",
            "loss: 1.037872  [  179/ 3200]\n",
            "loss: 0.924575  [  180/ 3200]\n",
            "loss: 1.046056  [  181/ 3200]\n",
            "loss: 1.435670  [  182/ 3200]\n",
            "loss: 0.956297  [  183/ 3200]\n",
            "loss: 1.144303  [  184/ 3200]\n",
            "loss: 0.964038  [  185/ 3200]\n",
            "loss: 1.115962  [  186/ 3200]\n",
            "loss: 1.137568  [  187/ 3200]\n",
            "loss: 1.119282  [  188/ 3200]\n",
            "loss: 0.823975  [  189/ 3200]\n",
            "loss: 0.842665  [  190/ 3200]\n",
            "loss: 1.095846  [  191/ 3200]\n",
            "loss: 0.926787  [  192/ 3200]\n",
            "loss: 1.303390  [  193/ 3200]\n",
            "loss: 1.155973  [  194/ 3200]\n",
            "loss: 1.022718  [  195/ 3200]\n",
            "loss: 1.046544  [  196/ 3200]\n",
            "loss: 0.879363  [  197/ 3200]\n",
            "loss: 1.113369  [  198/ 3200]\n",
            "loss: 0.998977  [  199/ 3200]\n",
            "Epoch:  24\n",
            "loss: 1.055331  [    0/ 3200]\n",
            "loss: 0.982844  [    1/ 3200]\n",
            "loss: 1.070712  [    2/ 3200]\n",
            "loss: 0.979373  [    3/ 3200]\n",
            "loss: 1.019616  [    4/ 3200]\n",
            "loss: 1.106909  [    5/ 3200]\n",
            "loss: 1.124347  [    6/ 3200]\n",
            "loss: 0.960028  [    7/ 3200]\n",
            "loss: 1.205054  [    8/ 3200]\n",
            "loss: 1.042175  [    9/ 3200]\n",
            "loss: 1.102580  [   10/ 3200]\n",
            "loss: 0.945054  [   11/ 3200]\n",
            "loss: 1.052568  [   12/ 3200]\n",
            "loss: 1.065572  [   13/ 3200]\n",
            "loss: 0.760554  [   14/ 3200]\n",
            "loss: 1.077603  [   15/ 3200]\n",
            "loss: 0.971076  [   16/ 3200]\n",
            "loss: 1.104520  [   17/ 3200]\n",
            "loss: 1.057705  [   18/ 3200]\n",
            "loss: 1.096793  [   19/ 3200]\n",
            "loss: 0.993935  [   20/ 3200]\n",
            "loss: 1.273924  [   21/ 3200]\n",
            "loss: 1.204532  [   22/ 3200]\n",
            "loss: 0.966709  [   23/ 3200]\n",
            "loss: 1.124834  [   24/ 3200]\n",
            "loss: 0.895561  [   25/ 3200]\n",
            "loss: 1.100063  [   26/ 3200]\n",
            "loss: 1.053182  [   27/ 3200]\n",
            "loss: 0.899264  [   28/ 3200]\n",
            "loss: 0.985300  [   29/ 3200]\n",
            "loss: 1.083083  [   30/ 3200]\n",
            "loss: 1.019539  [   31/ 3200]\n",
            "loss: 0.928258  [   32/ 3200]\n",
            "loss: 0.953361  [   33/ 3200]\n",
            "loss: 1.262613  [   34/ 3200]\n",
            "loss: 1.121241  [   35/ 3200]\n",
            "loss: 1.091106  [   36/ 3200]\n",
            "loss: 0.986736  [   37/ 3200]\n",
            "loss: 1.047008  [   38/ 3200]\n",
            "loss: 0.892099  [   39/ 3200]\n",
            "loss: 1.155533  [   40/ 3200]\n",
            "loss: 1.062073  [   41/ 3200]\n",
            "loss: 1.007923  [   42/ 3200]\n",
            "loss: 1.014253  [   43/ 3200]\n",
            "loss: 0.846900  [   44/ 3200]\n",
            "loss: 1.134121  [   45/ 3200]\n",
            "loss: 1.085873  [   46/ 3200]\n",
            "loss: 0.928252  [   47/ 3200]\n",
            "loss: 1.081132  [   48/ 3200]\n",
            "loss: 1.184724  [   49/ 3200]\n",
            "loss: 0.885491  [   50/ 3200]\n",
            "loss: 0.983495  [   51/ 3200]\n",
            "loss: 1.138933  [   52/ 3200]\n",
            "loss: 1.142379  [   53/ 3200]\n",
            "loss: 1.289898  [   54/ 3200]\n",
            "loss: 0.946126  [   55/ 3200]\n",
            "loss: 1.012782  [   56/ 3200]\n",
            "loss: 1.019452  [   57/ 3200]\n",
            "loss: 0.789042  [   58/ 3200]\n",
            "loss: 0.964248  [   59/ 3200]\n",
            "loss: 0.908914  [   60/ 3200]\n",
            "loss: 1.176580  [   61/ 3200]\n",
            "loss: 0.793500  [   62/ 3200]\n",
            "loss: 0.979999  [   63/ 3200]\n",
            "loss: 0.874220  [   64/ 3200]\n",
            "loss: 1.012852  [   65/ 3200]\n",
            "loss: 1.160190  [   66/ 3200]\n",
            "loss: 1.040262  [   67/ 3200]\n",
            "loss: 1.124098  [   68/ 3200]\n",
            "loss: 1.064906  [   69/ 3200]\n",
            "loss: 0.988429  [   70/ 3200]\n",
            "loss: 0.771423  [   71/ 3200]\n",
            "loss: 1.066916  [   72/ 3200]\n",
            "loss: 1.028087  [   73/ 3200]\n",
            "loss: 1.011646  [   74/ 3200]\n",
            "loss: 1.394782  [   75/ 3200]\n",
            "loss: 1.005349  [   76/ 3200]\n",
            "loss: 0.857463  [   77/ 3200]\n",
            "loss: 0.795587  [   78/ 3200]\n",
            "loss: 0.993529  [   79/ 3200]\n",
            "loss: 1.253776  [   80/ 3200]\n",
            "loss: 0.870668  [   81/ 3200]\n",
            "loss: 0.981051  [   82/ 3200]\n",
            "loss: 0.864700  [   83/ 3200]\n",
            "loss: 0.913280  [   84/ 3200]\n",
            "loss: 0.766156  [   85/ 3200]\n",
            "loss: 0.911421  [   86/ 3200]\n",
            "loss: 0.893047  [   87/ 3200]\n",
            "loss: 1.264021  [   88/ 3200]\n",
            "loss: 1.012670  [   89/ 3200]\n",
            "loss: 1.013170  [   90/ 3200]\n",
            "loss: 0.793821  [   91/ 3200]\n",
            "loss: 0.856801  [   92/ 3200]\n",
            "loss: 0.956317  [   93/ 3200]\n",
            "loss: 0.890343  [   94/ 3200]\n",
            "loss: 0.853720  [   95/ 3200]\n",
            "loss: 1.063834  [   96/ 3200]\n",
            "loss: 1.475090  [   97/ 3200]\n",
            "loss: 1.104832  [   98/ 3200]\n",
            "loss: 1.213633  [   99/ 3200]\n",
            "loss: 1.020582  [  100/ 3200]\n",
            "loss: 1.169766  [  101/ 3200]\n",
            "loss: 0.970643  [  102/ 3200]\n",
            "loss: 0.918835  [  103/ 3200]\n",
            "loss: 0.974295  [  104/ 3200]\n",
            "loss: 1.062423  [  105/ 3200]\n",
            "loss: 0.967047  [  106/ 3200]\n",
            "loss: 1.009025  [  107/ 3200]\n",
            "loss: 1.066379  [  108/ 3200]\n",
            "loss: 0.893708  [  109/ 3200]\n",
            "loss: 1.034030  [  110/ 3200]\n",
            "loss: 1.092814  [  111/ 3200]\n",
            "loss: 1.005659  [  112/ 3200]\n",
            "loss: 1.084573  [  113/ 3200]\n",
            "loss: 1.277571  [  114/ 3200]\n",
            "loss: 1.186807  [  115/ 3200]\n",
            "loss: 0.932043  [  116/ 3200]\n",
            "loss: 1.039050  [  117/ 3200]\n",
            "loss: 0.887017  [  118/ 3200]\n",
            "loss: 1.008228  [  119/ 3200]\n",
            "loss: 0.691483  [  120/ 3200]\n",
            "loss: 0.813389  [  121/ 3200]\n",
            "loss: 1.190882  [  122/ 3200]\n",
            "loss: 0.794679  [  123/ 3200]\n",
            "loss: 0.901284  [  124/ 3200]\n",
            "loss: 1.188496  [  125/ 3200]\n",
            "loss: 0.880555  [  126/ 3200]\n",
            "loss: 1.035481  [  127/ 3200]\n",
            "loss: 1.081935  [  128/ 3200]\n",
            "loss: 1.011763  [  129/ 3200]\n",
            "loss: 1.374309  [  130/ 3200]\n",
            "loss: 1.090494  [  131/ 3200]\n",
            "loss: 1.220370  [  132/ 3200]\n",
            "loss: 1.066882  [  133/ 3200]\n",
            "loss: 1.259388  [  134/ 3200]\n",
            "loss: 0.976018  [  135/ 3200]\n",
            "loss: 0.973674  [  136/ 3200]\n",
            "loss: 0.986943  [  137/ 3200]\n",
            "loss: 0.739603  [  138/ 3200]\n",
            "loss: 0.862504  [  139/ 3200]\n",
            "loss: 1.045699  [  140/ 3200]\n",
            "loss: 0.849604  [  141/ 3200]\n",
            "loss: 1.042468  [  142/ 3200]\n",
            "loss: 0.928298  [  143/ 3200]\n",
            "loss: 0.969920  [  144/ 3200]\n",
            "loss: 1.182567  [  145/ 3200]\n",
            "loss: 1.138453  [  146/ 3200]\n",
            "loss: 1.084004  [  147/ 3200]\n",
            "loss: 1.255971  [  148/ 3200]\n",
            "loss: 1.009341  [  149/ 3200]\n",
            "loss: 1.158682  [  150/ 3200]\n",
            "loss: 0.984881  [  151/ 3200]\n",
            "loss: 1.010558  [  152/ 3200]\n",
            "loss: 1.148834  [  153/ 3200]\n",
            "loss: 1.209844  [  154/ 3200]\n",
            "loss: 1.006642  [  155/ 3200]\n",
            "loss: 0.892460  [  156/ 3200]\n",
            "loss: 1.142537  [  157/ 3200]\n",
            "loss: 0.946373  [  158/ 3200]\n",
            "loss: 0.888566  [  159/ 3200]\n",
            "loss: 0.955983  [  160/ 3200]\n",
            "loss: 0.907309  [  161/ 3200]\n",
            "loss: 1.076106  [  162/ 3200]\n",
            "loss: 1.028835  [  163/ 3200]\n",
            "loss: 1.180313  [  164/ 3200]\n",
            "loss: 1.086825  [  165/ 3200]\n",
            "loss: 0.857593  [  166/ 3200]\n",
            "loss: 1.044064  [  167/ 3200]\n",
            "loss: 1.142385  [  168/ 3200]\n",
            "loss: 0.857744  [  169/ 3200]\n",
            "loss: 1.182949  [  170/ 3200]\n",
            "loss: 0.964300  [  171/ 3200]\n",
            "loss: 0.974233  [  172/ 3200]\n",
            "loss: 0.861336  [  173/ 3200]\n",
            "loss: 0.968685  [  174/ 3200]\n",
            "loss: 1.264907  [  175/ 3200]\n",
            "loss: 0.897435  [  176/ 3200]\n",
            "loss: 1.169017  [  177/ 3200]\n",
            "loss: 0.812996  [  178/ 3200]\n",
            "loss: 1.087135  [  179/ 3200]\n",
            "loss: 0.976626  [  180/ 3200]\n",
            "loss: 1.257351  [  181/ 3200]\n",
            "loss: 1.124599  [  182/ 3200]\n",
            "loss: 0.925964  [  183/ 3200]\n",
            "loss: 1.079622  [  184/ 3200]\n",
            "loss: 1.021894  [  185/ 3200]\n",
            "loss: 0.898181  [  186/ 3200]\n",
            "loss: 1.257165  [  187/ 3200]\n",
            "loss: 0.925455  [  188/ 3200]\n",
            "loss: 0.965522  [  189/ 3200]\n",
            "loss: 0.766698  [  190/ 3200]\n",
            "loss: 1.055011  [  191/ 3200]\n",
            "loss: 1.046373  [  192/ 3200]\n",
            "loss: 0.800135  [  193/ 3200]\n",
            "loss: 1.489156  [  194/ 3200]\n",
            "loss: 1.067711  [  195/ 3200]\n",
            "loss: 1.013441  [  196/ 3200]\n",
            "loss: 0.971093  [  197/ 3200]\n",
            "loss: 1.413544  [  198/ 3200]\n",
            "loss: 1.135796  [  199/ 3200]\n",
            "Epoch:  25\n",
            "loss: 1.066479  [    0/ 3200]\n",
            "loss: 0.875062  [    1/ 3200]\n",
            "loss: 0.877082  [    2/ 3200]\n",
            "loss: 0.964681  [    3/ 3200]\n",
            "loss: 0.998080  [    4/ 3200]\n",
            "loss: 1.004705  [    5/ 3200]\n",
            "loss: 1.000175  [    6/ 3200]\n",
            "loss: 0.989342  [    7/ 3200]\n",
            "loss: 1.066330  [    8/ 3200]\n",
            "loss: 0.999305  [    9/ 3200]\n",
            "loss: 1.107938  [   10/ 3200]\n",
            "loss: 1.000083  [   11/ 3200]\n",
            "loss: 1.181664  [   12/ 3200]\n",
            "loss: 1.083270  [   13/ 3200]\n",
            "loss: 1.066840  [   14/ 3200]\n",
            "loss: 1.000880  [   15/ 3200]\n",
            "loss: 0.889085  [   16/ 3200]\n",
            "loss: 0.953159  [   17/ 3200]\n",
            "loss: 1.033344  [   18/ 3200]\n",
            "loss: 0.973516  [   19/ 3200]\n",
            "loss: 0.910299  [   20/ 3200]\n",
            "loss: 0.874561  [   21/ 3200]\n",
            "loss: 1.069634  [   22/ 3200]\n",
            "loss: 0.894656  [   23/ 3200]\n",
            "loss: 1.163089  [   24/ 3200]\n",
            "loss: 0.946007  [   25/ 3200]\n",
            "loss: 0.940197  [   26/ 3200]\n",
            "loss: 0.800289  [   27/ 3200]\n",
            "loss: 0.907443  [   28/ 3200]\n",
            "loss: 1.389690  [   29/ 3200]\n",
            "loss: 1.154103  [   30/ 3200]\n",
            "loss: 0.954458  [   31/ 3200]\n",
            "loss: 0.788759  [   32/ 3200]\n",
            "loss: 0.904008  [   33/ 3200]\n",
            "loss: 1.152282  [   34/ 3200]\n",
            "loss: 1.243072  [   35/ 3200]\n",
            "loss: 1.185936  [   36/ 3200]\n",
            "loss: 1.067991  [   37/ 3200]\n",
            "loss: 0.962552  [   38/ 3200]\n",
            "loss: 1.041686  [   39/ 3200]\n",
            "loss: 0.992500  [   40/ 3200]\n",
            "loss: 0.870626  [   41/ 3200]\n",
            "loss: 1.150776  [   42/ 3200]\n",
            "loss: 1.045575  [   43/ 3200]\n",
            "loss: 1.238517  [   44/ 3200]\n",
            "loss: 1.164597  [   45/ 3200]\n",
            "loss: 1.287822  [   46/ 3200]\n",
            "loss: 1.074299  [   47/ 3200]\n",
            "loss: 1.082894  [   48/ 3200]\n",
            "loss: 1.043437  [   49/ 3200]\n",
            "loss: 0.991432  [   50/ 3200]\n",
            "loss: 0.991202  [   51/ 3200]\n",
            "loss: 0.849238  [   52/ 3200]\n",
            "loss: 0.938542  [   53/ 3200]\n",
            "loss: 1.175735  [   54/ 3200]\n",
            "loss: 0.815614  [   55/ 3200]\n",
            "loss: 1.028679  [   56/ 3200]\n",
            "loss: 1.003385  [   57/ 3200]\n",
            "loss: 0.863013  [   58/ 3200]\n",
            "loss: 1.060368  [   59/ 3200]\n",
            "loss: 1.132382  [   60/ 3200]\n",
            "loss: 1.075097  [   61/ 3200]\n",
            "loss: 1.248287  [   62/ 3200]\n",
            "loss: 1.083643  [   63/ 3200]\n",
            "loss: 1.124999  [   64/ 3200]\n",
            "loss: 1.135915  [   65/ 3200]\n",
            "loss: 1.084502  [   66/ 3200]\n",
            "loss: 0.808850  [   67/ 3200]\n",
            "loss: 1.082977  [   68/ 3200]\n",
            "loss: 0.801831  [   69/ 3200]\n",
            "loss: 1.107269  [   70/ 3200]\n",
            "loss: 1.147304  [   71/ 3200]\n",
            "loss: 1.190621  [   72/ 3200]\n",
            "loss: 1.043380  [   73/ 3200]\n",
            "loss: 0.822862  [   74/ 3200]\n",
            "loss: 1.055405  [   75/ 3200]\n",
            "loss: 0.978507  [   76/ 3200]\n",
            "loss: 1.161151  [   77/ 3200]\n",
            "loss: 0.962023  [   78/ 3200]\n",
            "loss: 0.955845  [   79/ 3200]\n",
            "loss: 1.009127  [   80/ 3200]\n",
            "loss: 1.037235  [   81/ 3200]\n",
            "loss: 1.000234  [   82/ 3200]\n",
            "loss: 0.867210  [   83/ 3200]\n",
            "loss: 1.088404  [   84/ 3200]\n",
            "loss: 1.081433  [   85/ 3200]\n",
            "loss: 0.938329  [   86/ 3200]\n",
            "loss: 1.015517  [   87/ 3200]\n",
            "loss: 0.860070  [   88/ 3200]\n",
            "loss: 0.898145  [   89/ 3200]\n",
            "loss: 0.970262  [   90/ 3200]\n",
            "loss: 0.942951  [   91/ 3200]\n",
            "loss: 1.295144  [   92/ 3200]\n",
            "loss: 1.018981  [   93/ 3200]\n",
            "loss: 0.931023  [   94/ 3200]\n",
            "loss: 1.051789  [   95/ 3200]\n",
            "loss: 0.987905  [   96/ 3200]\n",
            "loss: 1.121032  [   97/ 3200]\n",
            "loss: 1.219435  [   98/ 3200]\n",
            "loss: 1.002864  [   99/ 3200]\n",
            "loss: 0.951889  [  100/ 3200]\n",
            "loss: 1.081829  [  101/ 3200]\n",
            "loss: 1.029496  [  102/ 3200]\n",
            "loss: 0.901423  [  103/ 3200]\n",
            "loss: 1.069420  [  104/ 3200]\n",
            "loss: 0.620774  [  105/ 3200]\n",
            "loss: 0.891698  [  106/ 3200]\n",
            "loss: 1.036421  [  107/ 3200]\n",
            "loss: 1.059018  [  108/ 3200]\n",
            "loss: 1.111196  [  109/ 3200]\n",
            "loss: 1.011803  [  110/ 3200]\n",
            "loss: 1.026911  [  111/ 3200]\n",
            "loss: 0.919878  [  112/ 3200]\n",
            "loss: 0.933466  [  113/ 3200]\n",
            "loss: 1.138601  [  114/ 3200]\n",
            "loss: 0.898514  [  115/ 3200]\n",
            "loss: 1.014816  [  116/ 3200]\n",
            "loss: 1.082589  [  117/ 3200]\n",
            "loss: 1.092827  [  118/ 3200]\n",
            "loss: 1.038336  [  119/ 3200]\n",
            "loss: 0.982298  [  120/ 3200]\n",
            "loss: 1.025116  [  121/ 3200]\n",
            "loss: 1.060365  [  122/ 3200]\n",
            "loss: 0.911268  [  123/ 3200]\n",
            "loss: 1.173395  [  124/ 3200]\n",
            "loss: 1.331840  [  125/ 3200]\n",
            "loss: 0.874886  [  126/ 3200]\n",
            "loss: 0.962377  [  127/ 3200]\n",
            "loss: 0.916438  [  128/ 3200]\n",
            "loss: 1.054826  [  129/ 3200]\n",
            "loss: 0.938676  [  130/ 3200]\n",
            "loss: 1.082707  [  131/ 3200]\n",
            "loss: 0.856391  [  132/ 3200]\n",
            "loss: 0.947882  [  133/ 3200]\n",
            "loss: 1.230699  [  134/ 3200]\n",
            "loss: 0.988158  [  135/ 3200]\n",
            "loss: 1.023466  [  136/ 3200]\n",
            "loss: 1.048565  [  137/ 3200]\n",
            "loss: 1.035908  [  138/ 3200]\n",
            "loss: 0.757238  [  139/ 3200]\n",
            "loss: 1.309864  [  140/ 3200]\n",
            "loss: 1.162540  [  141/ 3200]\n",
            "loss: 1.342624  [  142/ 3200]\n",
            "loss: 0.839982  [  143/ 3200]\n",
            "loss: 0.954460  [  144/ 3200]\n",
            "loss: 0.970287  [  145/ 3200]\n",
            "loss: 1.017476  [  146/ 3200]\n",
            "loss: 1.075406  [  147/ 3200]\n",
            "loss: 0.969778  [  148/ 3200]\n",
            "loss: 1.038076  [  149/ 3200]\n",
            "loss: 1.048578  [  150/ 3200]\n",
            "loss: 0.901957  [  151/ 3200]\n",
            "loss: 0.952575  [  152/ 3200]\n",
            "loss: 0.759752  [  153/ 3200]\n",
            "loss: 0.753109  [  154/ 3200]\n",
            "loss: 0.906994  [  155/ 3200]\n",
            "loss: 1.079780  [  156/ 3200]\n",
            "loss: 0.978874  [  157/ 3200]\n",
            "loss: 1.009870  [  158/ 3200]\n",
            "loss: 0.960039  [  159/ 3200]\n",
            "loss: 0.978038  [  160/ 3200]\n",
            "loss: 0.837620  [  161/ 3200]\n",
            "loss: 1.119529  [  162/ 3200]\n",
            "loss: 0.810034  [  163/ 3200]\n",
            "loss: 1.373846  [  164/ 3200]\n",
            "loss: 1.063787  [  165/ 3200]\n",
            "loss: 1.007754  [  166/ 3200]\n",
            "loss: 1.086205  [  167/ 3200]\n",
            "loss: 1.028068  [  168/ 3200]\n",
            "loss: 0.994123  [  169/ 3200]\n",
            "loss: 1.021111  [  170/ 3200]\n",
            "loss: 1.026379  [  171/ 3200]\n",
            "loss: 0.843329  [  172/ 3200]\n",
            "loss: 0.962947  [  173/ 3200]\n",
            "loss: 0.799331  [  174/ 3200]\n",
            "loss: 1.167733  [  175/ 3200]\n",
            "loss: 1.219993  [  176/ 3200]\n",
            "loss: 0.851041  [  177/ 3200]\n",
            "loss: 0.897955  [  178/ 3200]\n",
            "loss: 1.022103  [  179/ 3200]\n",
            "loss: 0.862194  [  180/ 3200]\n",
            "loss: 1.280842  [  181/ 3200]\n",
            "loss: 1.048227  [  182/ 3200]\n",
            "loss: 0.826425  [  183/ 3200]\n",
            "loss: 1.109516  [  184/ 3200]\n",
            "loss: 0.825497  [  185/ 3200]\n",
            "loss: 0.947708  [  186/ 3200]\n",
            "loss: 0.911217  [  187/ 3200]\n",
            "loss: 0.958864  [  188/ 3200]\n",
            "loss: 0.985921  [  189/ 3200]\n",
            "loss: 1.080228  [  190/ 3200]\n",
            "loss: 0.994369  [  191/ 3200]\n",
            "loss: 1.091832  [  192/ 3200]\n",
            "loss: 0.923488  [  193/ 3200]\n",
            "loss: 1.236447  [  194/ 3200]\n",
            "loss: 1.134672  [  195/ 3200]\n",
            "loss: 1.127619  [  196/ 3200]\n",
            "loss: 0.931597  [  197/ 3200]\n",
            "loss: 0.826758  [  198/ 3200]\n",
            "loss: 1.245005  [  199/ 3200]\n",
            "Epoch:  26\n",
            "loss: 0.904071  [    0/ 3200]\n",
            "loss: 0.970961  [    1/ 3200]\n",
            "loss: 1.100147  [    2/ 3200]\n",
            "loss: 1.028904  [    3/ 3200]\n",
            "loss: 0.860029  [    4/ 3200]\n",
            "loss: 1.059616  [    5/ 3200]\n",
            "loss: 1.176176  [    6/ 3200]\n",
            "loss: 1.015836  [    7/ 3200]\n",
            "loss: 0.911459  [    8/ 3200]\n",
            "loss: 1.046010  [    9/ 3200]\n",
            "loss: 1.171567  [   10/ 3200]\n",
            "loss: 1.045443  [   11/ 3200]\n",
            "loss: 1.177013  [   12/ 3200]\n",
            "loss: 0.977624  [   13/ 3200]\n",
            "loss: 0.974850  [   14/ 3200]\n",
            "loss: 1.135724  [   15/ 3200]\n",
            "loss: 0.922384  [   16/ 3200]\n",
            "loss: 0.833620  [   17/ 3200]\n",
            "loss: 1.087071  [   18/ 3200]\n",
            "loss: 1.106712  [   19/ 3200]\n",
            "loss: 1.101791  [   20/ 3200]\n",
            "loss: 1.110981  [   21/ 3200]\n",
            "loss: 1.078977  [   22/ 3200]\n",
            "loss: 1.030845  [   23/ 3200]\n",
            "loss: 0.969996  [   24/ 3200]\n",
            "loss: 1.011374  [   25/ 3200]\n",
            "loss: 1.168830  [   26/ 3200]\n",
            "loss: 1.023748  [   27/ 3200]\n",
            "loss: 1.193090  [   28/ 3200]\n",
            "loss: 0.967698  [   29/ 3200]\n",
            "loss: 0.961770  [   30/ 3200]\n",
            "loss: 0.853611  [   31/ 3200]\n",
            "loss: 1.018096  [   32/ 3200]\n",
            "loss: 1.037273  [   33/ 3200]\n",
            "loss: 1.235446  [   34/ 3200]\n",
            "loss: 1.145192  [   35/ 3200]\n",
            "loss: 1.052583  [   36/ 3200]\n",
            "loss: 0.799679  [   37/ 3200]\n",
            "loss: 0.970476  [   38/ 3200]\n",
            "loss: 1.093707  [   39/ 3200]\n",
            "loss: 1.190676  [   40/ 3200]\n",
            "loss: 0.898724  [   41/ 3200]\n",
            "loss: 0.806486  [   42/ 3200]\n",
            "loss: 0.793318  [   43/ 3200]\n",
            "loss: 0.952572  [   44/ 3200]\n",
            "loss: 0.885833  [   45/ 3200]\n",
            "loss: 1.083060  [   46/ 3200]\n",
            "loss: 0.930129  [   47/ 3200]\n",
            "loss: 0.923566  [   48/ 3200]\n",
            "loss: 1.022066  [   49/ 3200]\n",
            "loss: 0.934474  [   50/ 3200]\n",
            "loss: 0.988831  [   51/ 3200]\n",
            "loss: 1.002418  [   52/ 3200]\n",
            "loss: 1.171582  [   53/ 3200]\n",
            "loss: 0.851818  [   54/ 3200]\n",
            "loss: 0.918188  [   55/ 3200]\n",
            "loss: 0.959632  [   56/ 3200]\n",
            "loss: 1.209239  [   57/ 3200]\n",
            "loss: 0.741452  [   58/ 3200]\n",
            "loss: 1.184957  [   59/ 3200]\n",
            "loss: 0.831587  [   60/ 3200]\n",
            "loss: 0.901286  [   61/ 3200]\n",
            "loss: 1.044049  [   62/ 3200]\n",
            "loss: 0.969337  [   63/ 3200]\n",
            "loss: 1.249403  [   64/ 3200]\n",
            "loss: 0.941783  [   65/ 3200]\n",
            "loss: 1.135845  [   66/ 3200]\n",
            "loss: 1.010727  [   67/ 3200]\n",
            "loss: 0.996203  [   68/ 3200]\n",
            "loss: 1.262508  [   69/ 3200]\n",
            "loss: 0.845057  [   70/ 3200]\n",
            "loss: 0.958258  [   71/ 3200]\n",
            "loss: 0.856496  [   72/ 3200]\n",
            "loss: 0.933596  [   73/ 3200]\n",
            "loss: 0.962329  [   74/ 3200]\n",
            "loss: 0.946116  [   75/ 3200]\n",
            "loss: 1.073137  [   76/ 3200]\n",
            "loss: 0.979879  [   77/ 3200]\n",
            "loss: 0.758347  [   78/ 3200]\n",
            "loss: 1.059175  [   79/ 3200]\n",
            "loss: 0.604494  [   80/ 3200]\n",
            "loss: 0.962418  [   81/ 3200]\n",
            "loss: 0.866465  [   82/ 3200]\n",
            "loss: 0.885131  [   83/ 3200]\n",
            "loss: 1.020086  [   84/ 3200]\n",
            "loss: 1.089466  [   85/ 3200]\n",
            "loss: 1.014596  [   86/ 3200]\n",
            "loss: 1.013531  [   87/ 3200]\n",
            "loss: 0.963743  [   88/ 3200]\n",
            "loss: 1.006174  [   89/ 3200]\n",
            "loss: 1.131463  [   90/ 3200]\n",
            "loss: 0.981317  [   91/ 3200]\n",
            "loss: 1.067526  [   92/ 3200]\n",
            "loss: 0.976662  [   93/ 3200]\n",
            "loss: 1.304360  [   94/ 3200]\n",
            "loss: 0.834478  [   95/ 3200]\n",
            "loss: 0.938994  [   96/ 3200]\n",
            "loss: 1.034547  [   97/ 3200]\n",
            "loss: 0.981965  [   98/ 3200]\n",
            "loss: 0.884142  [   99/ 3200]\n",
            "loss: 1.230302  [  100/ 3200]\n",
            "loss: 1.032307  [  101/ 3200]\n",
            "loss: 1.103972  [  102/ 3200]\n",
            "loss: 0.852760  [  103/ 3200]\n",
            "loss: 0.923523  [  104/ 3200]\n",
            "loss: 0.895718  [  105/ 3200]\n",
            "loss: 0.941979  [  106/ 3200]\n",
            "loss: 0.934203  [  107/ 3200]\n",
            "loss: 0.994100  [  108/ 3200]\n",
            "loss: 0.994420  [  109/ 3200]\n",
            "loss: 0.969361  [  110/ 3200]\n",
            "loss: 1.059586  [  111/ 3200]\n",
            "loss: 1.069112  [  112/ 3200]\n",
            "loss: 0.957873  [  113/ 3200]\n",
            "loss: 0.980209  [  114/ 3200]\n",
            "loss: 1.082590  [  115/ 3200]\n",
            "loss: 1.015916  [  116/ 3200]\n",
            "loss: 1.018161  [  117/ 3200]\n",
            "loss: 0.901211  [  118/ 3200]\n",
            "loss: 0.955175  [  119/ 3200]\n",
            "loss: 1.639255  [  120/ 3200]\n",
            "loss: 1.168756  [  121/ 3200]\n",
            "loss: 1.300457  [  122/ 3200]\n",
            "loss: 1.211725  [  123/ 3200]\n",
            "loss: 1.027733  [  124/ 3200]\n",
            "loss: 1.104526  [  125/ 3200]\n",
            "loss: 1.072159  [  126/ 3200]\n",
            "loss: 1.265980  [  127/ 3200]\n",
            "loss: 1.133562  [  128/ 3200]\n",
            "loss: 0.972894  [  129/ 3200]\n",
            "loss: 1.070442  [  130/ 3200]\n",
            "loss: 0.946412  [  131/ 3200]\n",
            "loss: 1.137891  [  132/ 3200]\n",
            "loss: 0.943321  [  133/ 3200]\n",
            "loss: 0.786952  [  134/ 3200]\n",
            "loss: 0.714685  [  135/ 3200]\n",
            "loss: 0.813173  [  136/ 3200]\n",
            "loss: 0.952003  [  137/ 3200]\n",
            "loss: 0.881922  [  138/ 3200]\n",
            "loss: 1.127092  [  139/ 3200]\n",
            "loss: 1.266410  [  140/ 3200]\n",
            "loss: 1.160989  [  141/ 3200]\n",
            "loss: 0.846085  [  142/ 3200]\n",
            "loss: 1.063864  [  143/ 3200]\n",
            "loss: 0.953787  [  144/ 3200]\n",
            "loss: 1.044740  [  145/ 3200]\n",
            "loss: 0.848168  [  146/ 3200]\n",
            "loss: 0.945025  [  147/ 3200]\n",
            "loss: 0.846642  [  148/ 3200]\n",
            "loss: 0.990529  [  149/ 3200]\n",
            "loss: 0.849533  [  150/ 3200]\n",
            "loss: 1.027905  [  151/ 3200]\n",
            "loss: 1.006303  [  152/ 3200]\n",
            "loss: 1.056739  [  153/ 3200]\n",
            "loss: 1.044602  [  154/ 3200]\n",
            "loss: 1.081590  [  155/ 3200]\n",
            "loss: 0.967864  [  156/ 3200]\n",
            "loss: 0.973968  [  157/ 3200]\n",
            "loss: 1.185353  [  158/ 3200]\n",
            "loss: 1.029896  [  159/ 3200]\n",
            "loss: 1.124592  [  160/ 3200]\n",
            "loss: 1.021551  [  161/ 3200]\n",
            "loss: 1.041639  [  162/ 3200]\n",
            "loss: 0.955540  [  163/ 3200]\n",
            "loss: 0.996957  [  164/ 3200]\n",
            "loss: 1.080721  [  165/ 3200]\n",
            "loss: 1.344034  [  166/ 3200]\n",
            "loss: 1.245381  [  167/ 3200]\n",
            "loss: 0.798156  [  168/ 3200]\n",
            "loss: 0.873886  [  169/ 3200]\n",
            "loss: 1.231097  [  170/ 3200]\n",
            "loss: 1.071856  [  171/ 3200]\n",
            "loss: 1.035235  [  172/ 3200]\n",
            "loss: 1.010438  [  173/ 3200]\n",
            "loss: 1.010986  [  174/ 3200]\n",
            "loss: 1.228970  [  175/ 3200]\n",
            "loss: 1.130200  [  176/ 3200]\n",
            "loss: 0.918331  [  177/ 3200]\n",
            "loss: 1.023856  [  178/ 3200]\n",
            "loss: 1.022495  [  179/ 3200]\n",
            "loss: 1.271090  [  180/ 3200]\n",
            "loss: 0.955286  [  181/ 3200]\n",
            "loss: 1.240124  [  182/ 3200]\n",
            "loss: 1.100633  [  183/ 3200]\n",
            "loss: 0.951030  [  184/ 3200]\n",
            "loss: 0.818558  [  185/ 3200]\n",
            "loss: 1.079314  [  186/ 3200]\n",
            "loss: 0.988182  [  187/ 3200]\n",
            "loss: 0.746617  [  188/ 3200]\n",
            "loss: 0.956112  [  189/ 3200]\n",
            "loss: 0.943530  [  190/ 3200]\n",
            "loss: 0.830132  [  191/ 3200]\n",
            "loss: 0.948332  [  192/ 3200]\n",
            "loss: 0.995706  [  193/ 3200]\n",
            "loss: 0.764120  [  194/ 3200]\n",
            "loss: 0.752255  [  195/ 3200]\n",
            "loss: 1.041980  [  196/ 3200]\n",
            "loss: 1.037005  [  197/ 3200]\n",
            "loss: 0.842123  [  198/ 3200]\n",
            "loss: 0.806152  [  199/ 3200]\n",
            "Epoch:  27\n",
            "loss: 1.075284  [    0/ 3200]\n",
            "loss: 0.748202  [    1/ 3200]\n",
            "loss: 0.932519  [    2/ 3200]\n",
            "loss: 1.122356  [    3/ 3200]\n",
            "loss: 0.981968  [    4/ 3200]\n",
            "loss: 1.002084  [    5/ 3200]\n",
            "loss: 1.068361  [    6/ 3200]\n",
            "loss: 1.005642  [    7/ 3200]\n",
            "loss: 1.184245  [    8/ 3200]\n",
            "loss: 1.381528  [    9/ 3200]\n",
            "loss: 0.828530  [   10/ 3200]\n",
            "loss: 1.070095  [   11/ 3200]\n",
            "loss: 0.966061  [   12/ 3200]\n",
            "loss: 0.854561  [   13/ 3200]\n",
            "loss: 1.225180  [   14/ 3200]\n",
            "loss: 0.729465  [   15/ 3200]\n",
            "loss: 1.094394  [   16/ 3200]\n",
            "loss: 1.201616  [   17/ 3200]\n",
            "loss: 0.893473  [   18/ 3200]\n",
            "loss: 1.167844  [   19/ 3200]\n",
            "loss: 0.981421  [   20/ 3200]\n",
            "loss: 0.776448  [   21/ 3200]\n",
            "loss: 0.752560  [   22/ 3200]\n",
            "loss: 1.145823  [   23/ 3200]\n",
            "loss: 0.873684  [   24/ 3200]\n",
            "loss: 0.903281  [   25/ 3200]\n",
            "loss: 1.045972  [   26/ 3200]\n",
            "loss: 1.071525  [   27/ 3200]\n",
            "loss: 0.837654  [   28/ 3200]\n",
            "loss: 0.901649  [   29/ 3200]\n",
            "loss: 0.921858  [   30/ 3200]\n",
            "loss: 1.028905  [   31/ 3200]\n",
            "loss: 1.019055  [   32/ 3200]\n",
            "loss: 1.051027  [   33/ 3200]\n",
            "loss: 1.344422  [   34/ 3200]\n",
            "loss: 0.762029  [   35/ 3200]\n",
            "loss: 1.113246  [   36/ 3200]\n",
            "loss: 1.119608  [   37/ 3200]\n",
            "loss: 1.084665  [   38/ 3200]\n",
            "loss: 0.960260  [   39/ 3200]\n",
            "loss: 1.298955  [   40/ 3200]\n",
            "loss: 1.014111  [   41/ 3200]\n",
            "loss: 0.964035  [   42/ 3200]\n",
            "loss: 1.240424  [   43/ 3200]\n",
            "loss: 0.860873  [   44/ 3200]\n",
            "loss: 1.165472  [   45/ 3200]\n",
            "loss: 1.271325  [   46/ 3200]\n",
            "loss: 0.942813  [   47/ 3200]\n",
            "loss: 0.991486  [   48/ 3200]\n",
            "loss: 1.041732  [   49/ 3200]\n",
            "loss: 0.857470  [   50/ 3200]\n",
            "loss: 1.104013  [   51/ 3200]\n",
            "loss: 0.982075  [   52/ 3200]\n",
            "loss: 0.986560  [   53/ 3200]\n",
            "loss: 1.121646  [   54/ 3200]\n",
            "loss: 0.959179  [   55/ 3200]\n",
            "loss: 0.905047  [   56/ 3200]\n",
            "loss: 1.319831  [   57/ 3200]\n",
            "loss: 0.773934  [   58/ 3200]\n",
            "loss: 0.716879  [   59/ 3200]\n",
            "loss: 0.853747  [   60/ 3200]\n",
            "loss: 1.001914  [   61/ 3200]\n",
            "loss: 0.822462  [   62/ 3200]\n",
            "loss: 1.231250  [   63/ 3200]\n",
            "loss: 0.785337  [   64/ 3200]\n",
            "loss: 1.067330  [   65/ 3200]\n",
            "loss: 1.171224  [   66/ 3200]\n",
            "loss: 0.967649  [   67/ 3200]\n",
            "loss: 0.961004  [   68/ 3200]\n",
            "loss: 1.064228  [   69/ 3200]\n",
            "loss: 0.826456  [   70/ 3200]\n",
            "loss: 0.829964  [   71/ 3200]\n",
            "loss: 0.964987  [   72/ 3200]\n",
            "loss: 0.838948  [   73/ 3200]\n",
            "loss: 0.932583  [   74/ 3200]\n",
            "loss: 1.074503  [   75/ 3200]\n",
            "loss: 0.953180  [   76/ 3200]\n",
            "loss: 0.873297  [   77/ 3200]\n",
            "loss: 1.023646  [   78/ 3200]\n",
            "loss: 0.834104  [   79/ 3200]\n",
            "loss: 1.051844  [   80/ 3200]\n",
            "loss: 0.719390  [   81/ 3200]\n",
            "loss: 1.135005  [   82/ 3200]\n",
            "loss: 0.909412  [   83/ 3200]\n",
            "loss: 0.841780  [   84/ 3200]\n",
            "loss: 0.906594  [   85/ 3200]\n",
            "loss: 0.938617  [   86/ 3200]\n",
            "loss: 0.968347  [   87/ 3200]\n",
            "loss: 0.889240  [   88/ 3200]\n",
            "loss: 0.917124  [   89/ 3200]\n",
            "loss: 0.997601  [   90/ 3200]\n",
            "loss: 1.007943  [   91/ 3200]\n",
            "loss: 1.124140  [   92/ 3200]\n",
            "loss: 1.021941  [   93/ 3200]\n",
            "loss: 1.059489  [   94/ 3200]\n",
            "loss: 1.207501  [   95/ 3200]\n",
            "loss: 0.946525  [   96/ 3200]\n",
            "loss: 0.948926  [   97/ 3200]\n",
            "loss: 1.179562  [   98/ 3200]\n",
            "loss: 0.880585  [   99/ 3200]\n",
            "loss: 1.084447  [  100/ 3200]\n",
            "loss: 1.240777  [  101/ 3200]\n",
            "loss: 1.116394  [  102/ 3200]\n",
            "loss: 0.926578  [  103/ 3200]\n",
            "loss: 0.879291  [  104/ 3200]\n",
            "loss: 1.029834  [  105/ 3200]\n",
            "loss: 1.095704  [  106/ 3200]\n",
            "loss: 0.861190  [  107/ 3200]\n",
            "loss: 0.886448  [  108/ 3200]\n",
            "loss: 0.886739  [  109/ 3200]\n",
            "loss: 1.120559  [  110/ 3200]\n",
            "loss: 1.122601  [  111/ 3200]\n",
            "loss: 1.000099  [  112/ 3200]\n",
            "loss: 1.084263  [  113/ 3200]\n",
            "loss: 1.373558  [  114/ 3200]\n",
            "loss: 0.839765  [  115/ 3200]\n",
            "loss: 0.994624  [  116/ 3200]\n",
            "loss: 1.108800  [  117/ 3200]\n",
            "loss: 0.997850  [  118/ 3200]\n",
            "loss: 1.423358  [  119/ 3200]\n",
            "loss: 1.186824  [  120/ 3200]\n",
            "loss: 0.897895  [  121/ 3200]\n",
            "loss: 0.726524  [  122/ 3200]\n",
            "loss: 0.982954  [  123/ 3200]\n",
            "loss: 1.077968  [  124/ 3200]\n",
            "loss: 1.118353  [  125/ 3200]\n",
            "loss: 1.003420  [  126/ 3200]\n",
            "loss: 1.037939  [  127/ 3200]\n",
            "loss: 1.066065  [  128/ 3200]\n",
            "loss: 1.201907  [  129/ 3200]\n",
            "loss: 0.844336  [  130/ 3200]\n",
            "loss: 1.203058  [  131/ 3200]\n",
            "loss: 1.136410  [  132/ 3200]\n",
            "loss: 1.034004  [  133/ 3200]\n",
            "loss: 0.831626  [  134/ 3200]\n",
            "loss: 0.934530  [  135/ 3200]\n",
            "loss: 1.405203  [  136/ 3200]\n",
            "loss: 0.990745  [  137/ 3200]\n",
            "loss: 1.077419  [  138/ 3200]\n",
            "loss: 1.001216  [  139/ 3200]\n",
            "loss: 0.775297  [  140/ 3200]\n",
            "loss: 0.970789  [  141/ 3200]\n",
            "loss: 1.074828  [  142/ 3200]\n",
            "loss: 0.912177  [  143/ 3200]\n",
            "loss: 0.897321  [  144/ 3200]\n",
            "loss: 1.030305  [  145/ 3200]\n",
            "loss: 1.105668  [  146/ 3200]\n",
            "loss: 0.968971  [  147/ 3200]\n",
            "loss: 1.035240  [  148/ 3200]\n",
            "loss: 1.021428  [  149/ 3200]\n",
            "loss: 0.877456  [  150/ 3200]\n",
            "loss: 0.816504  [  151/ 3200]\n",
            "loss: 0.967655  [  152/ 3200]\n",
            "loss: 1.031068  [  153/ 3200]\n",
            "loss: 0.774549  [  154/ 3200]\n",
            "loss: 1.160630  [  155/ 3200]\n",
            "loss: 1.068609  [  156/ 3200]\n",
            "loss: 0.684211  [  157/ 3200]\n",
            "loss: 0.998182  [  158/ 3200]\n",
            "loss: 0.950630  [  159/ 3200]\n",
            "loss: 1.022574  [  160/ 3200]\n",
            "loss: 1.076256  [  161/ 3200]\n",
            "loss: 0.932590  [  162/ 3200]\n",
            "loss: 0.884501  [  163/ 3200]\n",
            "loss: 1.092709  [  164/ 3200]\n",
            "loss: 0.911288  [  165/ 3200]\n",
            "loss: 1.135333  [  166/ 3200]\n",
            "loss: 0.767545  [  167/ 3200]\n",
            "loss: 0.831339  [  168/ 3200]\n",
            "loss: 1.028512  [  169/ 3200]\n",
            "loss: 0.853926  [  170/ 3200]\n",
            "loss: 0.887392  [  171/ 3200]\n",
            "loss: 1.058053  [  172/ 3200]\n",
            "loss: 1.043658  [  173/ 3200]\n",
            "loss: 0.946501  [  174/ 3200]\n",
            "loss: 0.743699  [  175/ 3200]\n",
            "loss: 0.998501  [  176/ 3200]\n",
            "loss: 0.833768  [  177/ 3200]\n",
            "loss: 0.949504  [  178/ 3200]\n",
            "loss: 0.962294  [  179/ 3200]\n",
            "loss: 1.371141  [  180/ 3200]\n",
            "loss: 0.779857  [  181/ 3200]\n",
            "loss: 0.771141  [  182/ 3200]\n",
            "loss: 1.159996  [  183/ 3200]\n",
            "loss: 0.823848  [  184/ 3200]\n",
            "loss: 1.310014  [  185/ 3200]\n",
            "loss: 1.033706  [  186/ 3200]\n",
            "loss: 1.033078  [  187/ 3200]\n",
            "loss: 1.020545  [  188/ 3200]\n",
            "loss: 0.919158  [  189/ 3200]\n",
            "loss: 0.699754  [  190/ 3200]\n",
            "loss: 0.843762  [  191/ 3200]\n",
            "loss: 1.049802  [  192/ 3200]\n",
            "loss: 0.987035  [  193/ 3200]\n",
            "loss: 0.719795  [  194/ 3200]\n",
            "loss: 0.800789  [  195/ 3200]\n",
            "loss: 0.875954  [  196/ 3200]\n",
            "loss: 1.182989  [  197/ 3200]\n",
            "loss: 0.904730  [  198/ 3200]\n",
            "loss: 1.121268  [  199/ 3200]\n",
            "Epoch:  28\n",
            "loss: 0.750744  [    0/ 3200]\n",
            "loss: 0.852563  [    1/ 3200]\n",
            "loss: 0.896128  [    2/ 3200]\n",
            "loss: 1.071992  [    3/ 3200]\n",
            "loss: 0.728433  [    4/ 3200]\n",
            "loss: 1.179463  [    5/ 3200]\n",
            "loss: 0.697256  [    6/ 3200]\n",
            "loss: 0.724770  [    7/ 3200]\n",
            "loss: 1.259996  [    8/ 3200]\n",
            "loss: 0.894868  [    9/ 3200]\n",
            "loss: 1.003440  [   10/ 3200]\n",
            "loss: 0.899175  [   11/ 3200]\n",
            "loss: 0.978540  [   12/ 3200]\n",
            "loss: 1.300514  [   13/ 3200]\n",
            "loss: 0.964691  [   14/ 3200]\n",
            "loss: 1.119979  [   15/ 3200]\n",
            "loss: 0.919521  [   16/ 3200]\n",
            "loss: 1.132378  [   17/ 3200]\n",
            "loss: 0.755113  [   18/ 3200]\n",
            "loss: 1.269927  [   19/ 3200]\n",
            "loss: 1.032496  [   20/ 3200]\n",
            "loss: 0.986466  [   21/ 3200]\n",
            "loss: 1.207710  [   22/ 3200]\n",
            "loss: 1.093369  [   23/ 3200]\n",
            "loss: 0.989041  [   24/ 3200]\n",
            "loss: 0.857519  [   25/ 3200]\n",
            "loss: 0.844167  [   26/ 3200]\n",
            "loss: 1.124385  [   27/ 3200]\n",
            "loss: 0.984736  [   28/ 3200]\n",
            "loss: 0.889053  [   29/ 3200]\n",
            "loss: 1.028790  [   30/ 3200]\n",
            "loss: 1.251402  [   31/ 3200]\n",
            "loss: 1.194498  [   32/ 3200]\n",
            "loss: 0.952513  [   33/ 3200]\n",
            "loss: 0.882212  [   34/ 3200]\n",
            "loss: 1.186880  [   35/ 3200]\n",
            "loss: 1.020826  [   36/ 3200]\n",
            "loss: 0.923826  [   37/ 3200]\n",
            "loss: 1.236735  [   38/ 3200]\n",
            "loss: 1.180502  [   39/ 3200]\n",
            "loss: 1.028541  [   40/ 3200]\n",
            "loss: 0.799745  [   41/ 3200]\n",
            "loss: 1.091132  [   42/ 3200]\n",
            "loss: 0.875009  [   43/ 3200]\n",
            "loss: 0.820589  [   44/ 3200]\n",
            "loss: 1.027536  [   45/ 3200]\n",
            "loss: 0.910237  [   46/ 3200]\n",
            "loss: 1.182049  [   47/ 3200]\n",
            "loss: 1.105631  [   48/ 3200]\n",
            "loss: 1.285288  [   49/ 3200]\n",
            "loss: 1.013918  [   50/ 3200]\n",
            "loss: 0.938270  [   51/ 3200]\n",
            "loss: 1.403364  [   52/ 3200]\n",
            "loss: 1.108097  [   53/ 3200]\n",
            "loss: 0.913802  [   54/ 3200]\n",
            "loss: 0.870970  [   55/ 3200]\n",
            "loss: 0.999250  [   56/ 3200]\n",
            "loss: 0.965990  [   57/ 3200]\n",
            "loss: 0.894325  [   58/ 3200]\n",
            "loss: 1.088060  [   59/ 3200]\n",
            "loss: 0.854223  [   60/ 3200]\n",
            "loss: 1.003619  [   61/ 3200]\n",
            "loss: 0.903958  [   62/ 3200]\n",
            "loss: 1.214051  [   63/ 3200]\n",
            "loss: 0.909008  [   64/ 3200]\n",
            "loss: 0.859256  [   65/ 3200]\n",
            "loss: 1.257559  [   66/ 3200]\n",
            "loss: 0.758847  [   67/ 3200]\n",
            "loss: 0.974592  [   68/ 3200]\n",
            "loss: 1.114844  [   69/ 3200]\n",
            "loss: 0.896695  [   70/ 3200]\n",
            "loss: 1.107235  [   71/ 3200]\n",
            "loss: 1.222319  [   72/ 3200]\n",
            "loss: 1.026813  [   73/ 3200]\n",
            "loss: 0.897540  [   74/ 3200]\n",
            "loss: 0.920040  [   75/ 3200]\n",
            "loss: 0.846565  [   76/ 3200]\n",
            "loss: 1.633946  [   77/ 3200]\n",
            "loss: 1.199033  [   78/ 3200]\n",
            "loss: 1.198010  [   79/ 3200]\n",
            "loss: 0.993035  [   80/ 3200]\n",
            "loss: 1.261169  [   81/ 3200]\n",
            "loss: 0.807857  [   82/ 3200]\n",
            "loss: 0.836630  [   83/ 3200]\n",
            "loss: 0.920221  [   84/ 3200]\n",
            "loss: 1.309636  [   85/ 3200]\n",
            "loss: 1.026991  [   86/ 3200]\n",
            "loss: 1.095932  [   87/ 3200]\n",
            "loss: 0.985157  [   88/ 3200]\n",
            "loss: 1.085031  [   89/ 3200]\n",
            "loss: 1.099587  [   90/ 3200]\n",
            "loss: 0.894027  [   91/ 3200]\n",
            "loss: 0.876601  [   92/ 3200]\n",
            "loss: 1.085544  [   93/ 3200]\n",
            "loss: 0.959597  [   94/ 3200]\n",
            "loss: 1.008152  [   95/ 3200]\n",
            "loss: 0.752670  [   96/ 3200]\n",
            "loss: 0.969630  [   97/ 3200]\n",
            "loss: 1.004249  [   98/ 3200]\n",
            "loss: 0.999661  [   99/ 3200]\n",
            "loss: 0.727527  [  100/ 3200]\n",
            "loss: 0.974317  [  101/ 3200]\n",
            "loss: 0.886583  [  102/ 3200]\n",
            "loss: 0.873344  [  103/ 3200]\n",
            "loss: 0.829537  [  104/ 3200]\n",
            "loss: 0.930185  [  105/ 3200]\n",
            "loss: 0.783056  [  106/ 3200]\n",
            "loss: 0.858123  [  107/ 3200]\n",
            "loss: 0.868031  [  108/ 3200]\n",
            "loss: 1.069571  [  109/ 3200]\n",
            "loss: 1.287245  [  110/ 3200]\n",
            "loss: 1.261268  [  111/ 3200]\n",
            "loss: 0.760916  [  112/ 3200]\n",
            "loss: 0.870986  [  113/ 3200]\n",
            "loss: 1.082010  [  114/ 3200]\n",
            "loss: 0.797629  [  115/ 3200]\n",
            "loss: 0.735900  [  116/ 3200]\n",
            "loss: 0.681102  [  117/ 3200]\n",
            "loss: 0.773120  [  118/ 3200]\n",
            "loss: 0.855506  [  119/ 3200]\n",
            "loss: 0.975851  [  120/ 3200]\n",
            "loss: 1.046214  [  121/ 3200]\n",
            "loss: 0.953378  [  122/ 3200]\n",
            "loss: 0.824608  [  123/ 3200]\n",
            "loss: 1.018991  [  124/ 3200]\n",
            "loss: 1.157987  [  125/ 3200]\n",
            "loss: 1.029647  [  126/ 3200]\n",
            "loss: 0.904817  [  127/ 3200]\n",
            "loss: 0.805802  [  128/ 3200]\n",
            "loss: 0.931100  [  129/ 3200]\n",
            "loss: 0.931727  [  130/ 3200]\n",
            "loss: 1.021020  [  131/ 3200]\n",
            "loss: 1.027957  [  132/ 3200]\n",
            "loss: 1.186105  [  133/ 3200]\n",
            "loss: 1.315878  [  134/ 3200]\n",
            "loss: 1.225824  [  135/ 3200]\n",
            "loss: 0.913878  [  136/ 3200]\n",
            "loss: 0.899502  [  137/ 3200]\n",
            "loss: 0.919701  [  138/ 3200]\n",
            "loss: 1.104687  [  139/ 3200]\n",
            "loss: 0.839961  [  140/ 3200]\n",
            "loss: 0.845706  [  141/ 3200]\n",
            "loss: 0.867909  [  142/ 3200]\n",
            "loss: 1.301581  [  143/ 3200]\n",
            "loss: 0.904467  [  144/ 3200]\n",
            "loss: 0.703804  [  145/ 3200]\n",
            "loss: 0.880051  [  146/ 3200]\n",
            "loss: 1.075840  [  147/ 3200]\n",
            "loss: 1.130265  [  148/ 3200]\n",
            "loss: 0.930655  [  149/ 3200]\n",
            "loss: 0.738581  [  150/ 3200]\n",
            "loss: 0.936520  [  151/ 3200]\n",
            "loss: 0.939591  [  152/ 3200]\n",
            "loss: 1.164215  [  153/ 3200]\n",
            "loss: 0.828183  [  154/ 3200]\n",
            "loss: 1.085705  [  155/ 3200]\n",
            "loss: 0.922981  [  156/ 3200]\n",
            "loss: 0.847815  [  157/ 3200]\n",
            "loss: 0.844376  [  158/ 3200]\n",
            "loss: 0.840010  [  159/ 3200]\n",
            "loss: 1.107751  [  160/ 3200]\n",
            "loss: 1.018710  [  161/ 3200]\n",
            "loss: 0.981745  [  162/ 3200]\n",
            "loss: 0.962604  [  163/ 3200]\n",
            "loss: 0.931372  [  164/ 3200]\n",
            "loss: 0.894221  [  165/ 3200]\n",
            "loss: 1.108554  [  166/ 3200]\n",
            "loss: 1.172714  [  167/ 3200]\n",
            "loss: 1.011707  [  168/ 3200]\n",
            "loss: 0.938562  [  169/ 3200]\n",
            "loss: 1.138330  [  170/ 3200]\n",
            "loss: 1.173777  [  171/ 3200]\n",
            "loss: 1.152050  [  172/ 3200]\n",
            "loss: 0.855057  [  173/ 3200]\n",
            "loss: 0.792895  [  174/ 3200]\n",
            "loss: 1.020836  [  175/ 3200]\n",
            "loss: 0.902685  [  176/ 3200]\n",
            "loss: 0.756404  [  177/ 3200]\n",
            "loss: 1.122271  [  178/ 3200]\n",
            "loss: 1.048005  [  179/ 3200]\n",
            "loss: 0.909529  [  180/ 3200]\n",
            "loss: 0.941793  [  181/ 3200]\n",
            "loss: 0.883924  [  182/ 3200]\n",
            "loss: 0.953054  [  183/ 3200]\n",
            "loss: 1.091814  [  184/ 3200]\n",
            "loss: 1.248700  [  185/ 3200]\n",
            "loss: 0.910714  [  186/ 3200]\n",
            "loss: 0.760745  [  187/ 3200]\n",
            "loss: 1.080344  [  188/ 3200]\n",
            "loss: 0.894158  [  189/ 3200]\n",
            "loss: 0.984899  [  190/ 3200]\n",
            "loss: 0.975356  [  191/ 3200]\n",
            "loss: 0.871613  [  192/ 3200]\n",
            "loss: 0.967877  [  193/ 3200]\n",
            "loss: 0.985922  [  194/ 3200]\n",
            "loss: 0.872549  [  195/ 3200]\n",
            "loss: 1.062177  [  196/ 3200]\n",
            "loss: 1.016258  [  197/ 3200]\n",
            "loss: 0.964918  [  198/ 3200]\n",
            "loss: 1.020532  [  199/ 3200]\n",
            "Epoch:  29\n",
            "loss: 0.784727  [    0/ 3200]\n",
            "loss: 0.855927  [    1/ 3200]\n",
            "loss: 1.020656  [    2/ 3200]\n",
            "loss: 1.171197  [    3/ 3200]\n",
            "loss: 1.052282  [    4/ 3200]\n",
            "loss: 1.418543  [    5/ 3200]\n",
            "loss: 0.990186  [    6/ 3200]\n",
            "loss: 1.119173  [    7/ 3200]\n",
            "loss: 1.258693  [    8/ 3200]\n",
            "loss: 0.919295  [    9/ 3200]\n",
            "loss: 1.302707  [   10/ 3200]\n",
            "loss: 0.953521  [   11/ 3200]\n",
            "loss: 1.079928  [   12/ 3200]\n",
            "loss: 0.776810  [   13/ 3200]\n",
            "loss: 0.731450  [   14/ 3200]\n",
            "loss: 1.077558  [   15/ 3200]\n",
            "loss: 0.761615  [   16/ 3200]\n",
            "loss: 0.812115  [   17/ 3200]\n",
            "loss: 0.699561  [   18/ 3200]\n",
            "loss: 0.876030  [   19/ 3200]\n",
            "loss: 1.074607  [   20/ 3200]\n",
            "loss: 0.932575  [   21/ 3200]\n",
            "loss: 0.844316  [   22/ 3200]\n",
            "loss: 0.942365  [   23/ 3200]\n",
            "loss: 0.831663  [   24/ 3200]\n",
            "loss: 0.886040  [   25/ 3200]\n",
            "loss: 0.785043  [   26/ 3200]\n",
            "loss: 0.846359  [   27/ 3200]\n",
            "loss: 0.965992  [   28/ 3200]\n",
            "loss: 0.956696  [   29/ 3200]\n",
            "loss: 0.714718  [   30/ 3200]\n",
            "loss: 1.008100  [   31/ 3200]\n",
            "loss: 0.991696  [   32/ 3200]\n",
            "loss: 1.612265  [   33/ 3200]\n",
            "loss: 0.936454  [   34/ 3200]\n",
            "loss: 1.177880  [   35/ 3200]\n",
            "loss: 1.206938  [   36/ 3200]\n",
            "loss: 0.994946  [   37/ 3200]\n",
            "loss: 0.934936  [   38/ 3200]\n",
            "loss: 0.889107  [   39/ 3200]\n",
            "loss: 1.272290  [   40/ 3200]\n",
            "loss: 0.959850  [   41/ 3200]\n",
            "loss: 0.978216  [   42/ 3200]\n",
            "loss: 0.883434  [   43/ 3200]\n",
            "loss: 1.057157  [   44/ 3200]\n",
            "loss: 0.919043  [   45/ 3200]\n",
            "loss: 1.099426  [   46/ 3200]\n",
            "loss: 0.829808  [   47/ 3200]\n",
            "loss: 1.304314  [   48/ 3200]\n",
            "loss: 0.791752  [   49/ 3200]\n",
            "loss: 1.176340  [   50/ 3200]\n",
            "loss: 0.963417  [   51/ 3200]\n",
            "loss: 0.952965  [   52/ 3200]\n",
            "loss: 1.075202  [   53/ 3200]\n",
            "loss: 1.052728  [   54/ 3200]\n",
            "loss: 0.956260  [   55/ 3200]\n",
            "loss: 0.878332  [   56/ 3200]\n",
            "loss: 0.741035  [   57/ 3200]\n",
            "loss: 1.003029  [   58/ 3200]\n",
            "loss: 0.939624  [   59/ 3200]\n",
            "loss: 0.983661  [   60/ 3200]\n",
            "loss: 1.052518  [   61/ 3200]\n",
            "loss: 1.249366  [   62/ 3200]\n",
            "loss: 1.063949  [   63/ 3200]\n",
            "loss: 1.189080  [   64/ 3200]\n",
            "loss: 0.880695  [   65/ 3200]\n",
            "loss: 1.005030  [   66/ 3200]\n",
            "loss: 0.891262  [   67/ 3200]\n",
            "loss: 1.091700  [   68/ 3200]\n",
            "loss: 1.272264  [   69/ 3200]\n",
            "loss: 0.816517  [   70/ 3200]\n",
            "loss: 0.889063  [   71/ 3200]\n",
            "loss: 0.993857  [   72/ 3200]\n",
            "loss: 0.780234  [   73/ 3200]\n",
            "loss: 0.903107  [   74/ 3200]\n",
            "loss: 1.061373  [   75/ 3200]\n",
            "loss: 0.763503  [   76/ 3200]\n",
            "loss: 0.920916  [   77/ 3200]\n",
            "loss: 0.990602  [   78/ 3200]\n",
            "loss: 1.031517  [   79/ 3200]\n",
            "loss: 1.243082  [   80/ 3200]\n",
            "loss: 1.073207  [   81/ 3200]\n",
            "loss: 0.996368  [   82/ 3200]\n",
            "loss: 0.765690  [   83/ 3200]\n",
            "loss: 0.957370  [   84/ 3200]\n",
            "loss: 0.905508  [   85/ 3200]\n",
            "loss: 0.794645  [   86/ 3200]\n",
            "loss: 0.897288  [   87/ 3200]\n",
            "loss: 0.806310  [   88/ 3200]\n",
            "loss: 1.030612  [   89/ 3200]\n",
            "loss: 0.622815  [   90/ 3200]\n",
            "loss: 0.912971  [   91/ 3200]\n",
            "loss: 0.801463  [   92/ 3200]\n",
            "loss: 0.988758  [   93/ 3200]\n",
            "loss: 1.132899  [   94/ 3200]\n",
            "loss: 0.949307  [   95/ 3200]\n",
            "loss: 1.162845  [   96/ 3200]\n",
            "loss: 1.124549  [   97/ 3200]\n",
            "loss: 1.076150  [   98/ 3200]\n",
            "loss: 1.204193  [   99/ 3200]\n",
            "loss: 0.858199  [  100/ 3200]\n",
            "loss: 0.960804  [  101/ 3200]\n",
            "loss: 0.845405  [  102/ 3200]\n",
            "loss: 1.012756  [  103/ 3200]\n",
            "loss: 0.933028  [  104/ 3200]\n",
            "loss: 0.978744  [  105/ 3200]\n",
            "loss: 0.833026  [  106/ 3200]\n",
            "loss: 0.981836  [  107/ 3200]\n",
            "loss: 1.009235  [  108/ 3200]\n",
            "loss: 1.125503  [  109/ 3200]\n",
            "loss: 1.022076  [  110/ 3200]\n",
            "loss: 0.900809  [  111/ 3200]\n",
            "loss: 1.014792  [  112/ 3200]\n",
            "loss: 0.793964  [  113/ 3200]\n",
            "loss: 0.859510  [  114/ 3200]\n",
            "loss: 1.029044  [  115/ 3200]\n",
            "loss: 0.981194  [  116/ 3200]\n",
            "loss: 0.833919  [  117/ 3200]\n",
            "loss: 0.850764  [  118/ 3200]\n",
            "loss: 0.717451  [  119/ 3200]\n",
            "loss: 1.105263  [  120/ 3200]\n",
            "loss: 0.883508  [  121/ 3200]\n",
            "loss: 1.072455  [  122/ 3200]\n",
            "loss: 1.269442  [  123/ 3200]\n",
            "loss: 0.645593  [  124/ 3200]\n",
            "loss: 0.921093  [  125/ 3200]\n",
            "loss: 0.953032  [  126/ 3200]\n",
            "loss: 1.071821  [  127/ 3200]\n",
            "loss: 0.988658  [  128/ 3200]\n",
            "loss: 0.811684  [  129/ 3200]\n",
            "loss: 1.083028  [  130/ 3200]\n",
            "loss: 0.882923  [  131/ 3200]\n",
            "loss: 1.102927  [  132/ 3200]\n",
            "loss: 1.085733  [  133/ 3200]\n",
            "loss: 0.849414  [  134/ 3200]\n",
            "loss: 1.076437  [  135/ 3200]\n",
            "loss: 0.898910  [  136/ 3200]\n",
            "loss: 1.017643  [  137/ 3200]\n",
            "loss: 0.959555  [  138/ 3200]\n",
            "loss: 0.970970  [  139/ 3200]\n",
            "loss: 0.696972  [  140/ 3200]\n",
            "loss: 1.189123  [  141/ 3200]\n",
            "loss: 0.740715  [  142/ 3200]\n",
            "loss: 1.120304  [  143/ 3200]\n",
            "loss: 0.993764  [  144/ 3200]\n",
            "loss: 1.080558  [  145/ 3200]\n",
            "loss: 1.171502  [  146/ 3200]\n",
            "loss: 0.708852  [  147/ 3200]\n",
            "loss: 0.886588  [  148/ 3200]\n",
            "loss: 1.090140  [  149/ 3200]\n",
            "loss: 1.062623  [  150/ 3200]\n",
            "loss: 1.200960  [  151/ 3200]\n",
            "loss: 1.016383  [  152/ 3200]\n",
            "loss: 0.858451  [  153/ 3200]\n",
            "loss: 0.743021  [  154/ 3200]\n",
            "loss: 1.068526  [  155/ 3200]\n",
            "loss: 0.514328  [  156/ 3200]\n",
            "loss: 0.790860  [  157/ 3200]\n",
            "loss: 1.236641  [  158/ 3200]\n",
            "loss: 1.037740  [  159/ 3200]\n",
            "loss: 0.924258  [  160/ 3200]\n",
            "loss: 1.034032  [  161/ 3200]\n",
            "loss: 1.091716  [  162/ 3200]\n",
            "loss: 0.852904  [  163/ 3200]\n",
            "loss: 1.061281  [  164/ 3200]\n",
            "loss: 0.996512  [  165/ 3200]\n",
            "loss: 1.082431  [  166/ 3200]\n",
            "loss: 0.999282  [  167/ 3200]\n",
            "loss: 1.294336  [  168/ 3200]\n",
            "loss: 1.121285  [  169/ 3200]\n",
            "loss: 1.001578  [  170/ 3200]\n",
            "loss: 0.959681  [  171/ 3200]\n",
            "loss: 1.102322  [  172/ 3200]\n",
            "loss: 0.886847  [  173/ 3200]\n",
            "loss: 1.094429  [  174/ 3200]\n",
            "loss: 0.875187  [  175/ 3200]\n",
            "loss: 1.062500  [  176/ 3200]\n",
            "loss: 0.971740  [  177/ 3200]\n",
            "loss: 1.019644  [  178/ 3200]\n",
            "loss: 1.093412  [  179/ 3200]\n",
            "loss: 0.719142  [  180/ 3200]\n",
            "loss: 0.836889  [  181/ 3200]\n",
            "loss: 1.086178  [  182/ 3200]\n",
            "loss: 1.159899  [  183/ 3200]\n",
            "loss: 0.709574  [  184/ 3200]\n",
            "loss: 0.881578  [  185/ 3200]\n",
            "loss: 0.803368  [  186/ 3200]\n",
            "loss: 1.519432  [  187/ 3200]\n",
            "loss: 1.189318  [  188/ 3200]\n",
            "loss: 0.975422  [  189/ 3200]\n",
            "loss: 1.022622  [  190/ 3200]\n",
            "loss: 0.941671  [  191/ 3200]\n",
            "loss: 0.834728  [  192/ 3200]\n",
            "loss: 0.961732  [  193/ 3200]\n",
            "loss: 1.048338  [  194/ 3200]\n",
            "loss: 1.214330  [  195/ 3200]\n",
            "loss: 1.062956  [  196/ 3200]\n",
            "loss: 0.970219  [  197/ 3200]\n",
            "loss: 0.866758  [  198/ 3200]\n",
            "loss: 0.806227  [  199/ 3200]\n",
            "Epoch:  30\n",
            "loss: 0.639764  [    0/ 3200]\n",
            "loss: 0.818528  [    1/ 3200]\n",
            "loss: 1.060925  [    2/ 3200]\n",
            "loss: 0.980362  [    3/ 3200]\n",
            "loss: 0.805264  [    4/ 3200]\n",
            "loss: 0.688129  [    5/ 3200]\n",
            "loss: 1.531071  [    6/ 3200]\n",
            "loss: 0.989032  [    7/ 3200]\n",
            "loss: 0.657164  [    8/ 3200]\n",
            "loss: 0.967525  [    9/ 3200]\n",
            "loss: 0.797766  [   10/ 3200]\n",
            "loss: 1.127686  [   11/ 3200]\n",
            "loss: 0.791743  [   12/ 3200]\n",
            "loss: 0.850314  [   13/ 3200]\n",
            "loss: 1.246771  [   14/ 3200]\n",
            "loss: 0.920303  [   15/ 3200]\n",
            "loss: 0.964076  [   16/ 3200]\n",
            "loss: 0.860955  [   17/ 3200]\n",
            "loss: 0.799372  [   18/ 3200]\n",
            "loss: 1.022633  [   19/ 3200]\n",
            "loss: 0.824873  [   20/ 3200]\n",
            "loss: 0.848093  [   21/ 3200]\n",
            "loss: 0.998687  [   22/ 3200]\n",
            "loss: 0.795063  [   23/ 3200]\n",
            "loss: 0.821799  [   24/ 3200]\n",
            "loss: 1.046316  [   25/ 3200]\n",
            "loss: 1.108593  [   26/ 3200]\n",
            "loss: 1.094849  [   27/ 3200]\n",
            "loss: 0.809073  [   28/ 3200]\n",
            "loss: 0.660207  [   29/ 3200]\n",
            "loss: 0.877571  [   30/ 3200]\n",
            "loss: 1.187584  [   31/ 3200]\n",
            "loss: 1.095608  [   32/ 3200]\n",
            "loss: 1.102428  [   33/ 3200]\n",
            "loss: 1.012386  [   34/ 3200]\n",
            "loss: 1.188893  [   35/ 3200]\n",
            "loss: 1.433732  [   36/ 3200]\n",
            "loss: 0.781909  [   37/ 3200]\n",
            "loss: 1.037112  [   38/ 3200]\n",
            "loss: 1.043252  [   39/ 3200]\n",
            "loss: 1.096639  [   40/ 3200]\n",
            "loss: 1.099158  [   41/ 3200]\n",
            "loss: 0.941660  [   42/ 3200]\n",
            "loss: 1.087151  [   43/ 3200]\n",
            "loss: 0.884968  [   44/ 3200]\n",
            "loss: 1.026805  [   45/ 3200]\n",
            "loss: 0.817495  [   46/ 3200]\n",
            "loss: 1.052948  [   47/ 3200]\n",
            "loss: 0.831506  [   48/ 3200]\n",
            "loss: 0.924593  [   49/ 3200]\n",
            "loss: 0.788940  [   50/ 3200]\n",
            "loss: 0.676014  [   51/ 3200]\n",
            "loss: 0.882295  [   52/ 3200]\n",
            "loss: 1.195804  [   53/ 3200]\n",
            "loss: 0.800503  [   54/ 3200]\n",
            "loss: 0.901104  [   55/ 3200]\n",
            "loss: 1.022850  [   56/ 3200]\n",
            "loss: 0.918287  [   57/ 3200]\n",
            "loss: 1.457833  [   58/ 3200]\n",
            "loss: 0.909597  [   59/ 3200]\n",
            "loss: 0.940966  [   60/ 3200]\n",
            "loss: 0.779906  [   61/ 3200]\n",
            "loss: 1.295071  [   62/ 3200]\n",
            "loss: 0.933796  [   63/ 3200]\n",
            "loss: 0.867772  [   64/ 3200]\n",
            "loss: 1.018344  [   65/ 3200]\n",
            "loss: 0.832640  [   66/ 3200]\n",
            "loss: 1.255882  [   67/ 3200]\n",
            "loss: 1.105112  [   68/ 3200]\n",
            "loss: 0.934126  [   69/ 3200]\n",
            "loss: 0.809494  [   70/ 3200]\n",
            "loss: 1.131709  [   71/ 3200]\n",
            "loss: 0.987550  [   72/ 3200]\n",
            "loss: 1.099346  [   73/ 3200]\n",
            "loss: 1.157766  [   74/ 3200]\n",
            "loss: 1.150180  [   75/ 3200]\n",
            "loss: 1.077586  [   76/ 3200]\n",
            "loss: 1.025105  [   77/ 3200]\n",
            "loss: 1.212999  [   78/ 3200]\n",
            "loss: 1.140705  [   79/ 3200]\n",
            "loss: 0.815406  [   80/ 3200]\n",
            "loss: 0.973537  [   81/ 3200]\n",
            "loss: 1.341664  [   82/ 3200]\n",
            "loss: 1.058214  [   83/ 3200]\n",
            "loss: 1.018801  [   84/ 3200]\n",
            "loss: 1.066793  [   85/ 3200]\n",
            "loss: 0.838312  [   86/ 3200]\n",
            "loss: 1.125199  [   87/ 3200]\n",
            "loss: 0.623210  [   88/ 3200]\n",
            "loss: 1.020754  [   89/ 3200]\n",
            "loss: 0.930257  [   90/ 3200]\n",
            "loss: 0.879104  [   91/ 3200]\n",
            "loss: 0.960187  [   92/ 3200]\n",
            "loss: 0.796856  [   93/ 3200]\n",
            "loss: 1.249846  [   94/ 3200]\n",
            "loss: 0.966287  [   95/ 3200]\n",
            "loss: 1.199762  [   96/ 3200]\n",
            "loss: 1.307383  [   97/ 3200]\n",
            "loss: 1.018754  [   98/ 3200]\n",
            "loss: 1.042209  [   99/ 3200]\n",
            "loss: 1.060049  [  100/ 3200]\n",
            "loss: 0.787904  [  101/ 3200]\n",
            "loss: 0.695875  [  102/ 3200]\n",
            "loss: 1.063979  [  103/ 3200]\n",
            "loss: 1.007515  [  104/ 3200]\n",
            "loss: 0.847529  [  105/ 3200]\n",
            "loss: 0.988319  [  106/ 3200]\n",
            "loss: 1.421884  [  107/ 3200]\n",
            "loss: 0.910115  [  108/ 3200]\n",
            "loss: 1.300563  [  109/ 3200]\n",
            "loss: 1.003184  [  110/ 3200]\n",
            "loss: 0.990790  [  111/ 3200]\n",
            "loss: 0.873774  [  112/ 3200]\n",
            "loss: 0.955547  [  113/ 3200]\n",
            "loss: 1.145242  [  114/ 3200]\n",
            "loss: 0.672935  [  115/ 3200]\n",
            "loss: 1.122111  [  116/ 3200]\n",
            "loss: 1.049064  [  117/ 3200]\n",
            "loss: 0.923772  [  118/ 3200]\n",
            "loss: 1.278092  [  119/ 3200]\n",
            "loss: 1.160731  [  120/ 3200]\n",
            "loss: 1.013621  [  121/ 3200]\n",
            "loss: 1.299314  [  122/ 3200]\n",
            "loss: 1.136428  [  123/ 3200]\n",
            "loss: 0.904716  [  124/ 3200]\n",
            "loss: 0.800981  [  125/ 3200]\n",
            "loss: 1.293737  [  126/ 3200]\n",
            "loss: 0.833743  [  127/ 3200]\n",
            "loss: 0.914401  [  128/ 3200]\n",
            "loss: 1.127192  [  129/ 3200]\n",
            "loss: 0.939127  [  130/ 3200]\n",
            "loss: 1.100985  [  131/ 3200]\n",
            "loss: 0.959416  [  132/ 3200]\n",
            "loss: 0.797029  [  133/ 3200]\n",
            "loss: 0.934776  [  134/ 3200]\n",
            "loss: 1.141393  [  135/ 3200]\n",
            "loss: 0.890597  [  136/ 3200]\n",
            "loss: 1.106311  [  137/ 3200]\n",
            "loss: 1.041295  [  138/ 3200]\n",
            "loss: 0.841924  [  139/ 3200]\n",
            "loss: 0.920712  [  140/ 3200]\n",
            "loss: 0.870722  [  141/ 3200]\n",
            "loss: 0.671404  [  142/ 3200]\n",
            "loss: 0.770688  [  143/ 3200]\n",
            "loss: 0.810967  [  144/ 3200]\n",
            "loss: 0.960956  [  145/ 3200]\n",
            "loss: 0.785715  [  146/ 3200]\n",
            "loss: 1.005309  [  147/ 3200]\n",
            "loss: 1.055071  [  148/ 3200]\n",
            "loss: 0.864244  [  149/ 3200]\n",
            "loss: 1.129740  [  150/ 3200]\n",
            "loss: 1.037965  [  151/ 3200]\n",
            "loss: 0.913412  [  152/ 3200]\n",
            "loss: 0.823313  [  153/ 3200]\n",
            "loss: 1.081522  [  154/ 3200]\n",
            "loss: 1.150635  [  155/ 3200]\n",
            "loss: 0.924993  [  156/ 3200]\n",
            "loss: 0.891289  [  157/ 3200]\n",
            "loss: 0.842359  [  158/ 3200]\n",
            "loss: 1.039775  [  159/ 3200]\n",
            "loss: 0.854074  [  160/ 3200]\n",
            "loss: 0.987630  [  161/ 3200]\n",
            "loss: 0.904379  [  162/ 3200]\n",
            "loss: 0.810616  [  163/ 3200]\n",
            "loss: 1.413040  [  164/ 3200]\n",
            "loss: 0.980166  [  165/ 3200]\n",
            "loss: 0.822496  [  166/ 3200]\n",
            "loss: 0.715001  [  167/ 3200]\n",
            "loss: 0.933510  [  168/ 3200]\n",
            "loss: 1.128957  [  169/ 3200]\n",
            "loss: 0.861461  [  170/ 3200]\n",
            "loss: 1.061789  [  171/ 3200]\n",
            "loss: 1.005251  [  172/ 3200]\n",
            "loss: 0.993513  [  173/ 3200]\n",
            "loss: 0.812946  [  174/ 3200]\n",
            "loss: 0.814050  [  175/ 3200]\n",
            "loss: 0.659755  [  176/ 3200]\n",
            "loss: 0.904159  [  177/ 3200]\n",
            "loss: 0.748643  [  178/ 3200]\n",
            "loss: 0.904692  [  179/ 3200]\n",
            "loss: 0.956437  [  180/ 3200]\n",
            "loss: 0.817548  [  181/ 3200]\n",
            "loss: 1.282632  [  182/ 3200]\n",
            "loss: 1.058477  [  183/ 3200]\n",
            "loss: 1.105843  [  184/ 3200]\n",
            "loss: 0.715710  [  185/ 3200]\n",
            "loss: 0.888034  [  186/ 3200]\n",
            "loss: 1.023432  [  187/ 3200]\n",
            "loss: 0.860653  [  188/ 3200]\n",
            "loss: 0.682500  [  189/ 3200]\n",
            "loss: 0.970792  [  190/ 3200]\n",
            "loss: 1.140311  [  191/ 3200]\n",
            "loss: 1.173886  [  192/ 3200]\n",
            "loss: 1.021639  [  193/ 3200]\n",
            "loss: 1.027301  [  194/ 3200]\n",
            "loss: 0.884750  [  195/ 3200]\n",
            "loss: 0.998636  [  196/ 3200]\n",
            "loss: 1.074957  [  197/ 3200]\n",
            "loss: 0.983519  [  198/ 3200]\n",
            "loss: 0.685578  [  199/ 3200]\n",
            "Training finished.\n",
            "Average Loss: 0.9927868652482366\n",
            "F1 Score (Macro): 0.5753785882300191\n",
            "Accuracy: 0.6097383720930233\n",
            "Confusion Matrix:\n",
            "[[ 48  33 165  78]\n",
            " [ 25 241  24   7]\n",
            " [ 19  15 311  11]\n",
            " [ 46  33  81 239]]\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "# Define the optimizer, learning rate, and loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 30\n",
        "\n",
        "# Train the model\n",
        "train_network(model, optimizer, train_dataloader, loss_function, num_epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "avg_loss, f1_score_macro, accuracy, confusion_mat = evaluate_model(model, test_dataloader, loss_function)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Average Loss: {avg_loss}\")\n",
        "print(f\"F1 Score (Macro): {f1_score_macro}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "print(device)\n",
        "\n",
        "# Return the trained model\n",
        "trained_model = model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-7t8lF3Z4lj8"
      },
      "source": [
        "## Train Network with GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "J-tvFSLB4lj9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device= cpu\n",
            "Epoch:  1\n",
            "loss: 1.449543  [    0/ 3200]\n",
            "loss: 1.410747  [    1/ 3200]\n",
            "loss: 1.386439  [    2/ 3200]\n",
            "loss: 1.409988  [    3/ 3200]\n",
            "loss: 1.467227  [    4/ 3200]\n",
            "loss: 1.432923  [    5/ 3200]\n",
            "loss: 1.436219  [    6/ 3200]\n",
            "loss: 1.357206  [    7/ 3200]\n",
            "loss: 1.398784  [    8/ 3200]\n",
            "loss: 1.426616  [    9/ 3200]\n",
            "loss: 1.414331  [   10/ 3200]\n",
            "loss: 1.428527  [   11/ 3200]\n",
            "loss: 1.355962  [   12/ 3200]\n",
            "loss: 1.433728  [   13/ 3200]\n",
            "loss: 1.420559  [   14/ 3200]\n",
            "loss: 1.492343  [   15/ 3200]\n",
            "loss: 1.427404  [   16/ 3200]\n",
            "loss: 1.334879  [   17/ 3200]\n",
            "loss: 1.436475  [   18/ 3200]\n",
            "loss: 1.439841  [   19/ 3200]\n",
            "loss: 1.303013  [   20/ 3200]\n",
            "loss: 1.410639  [   21/ 3200]\n",
            "loss: 1.401489  [   22/ 3200]\n",
            "loss: 1.416447  [   23/ 3200]\n",
            "loss: 1.412828  [   24/ 3200]\n",
            "loss: 1.374321  [   25/ 3200]\n",
            "loss: 1.395023  [   26/ 3200]\n",
            "loss: 1.354107  [   27/ 3200]\n",
            "loss: 1.353079  [   28/ 3200]\n",
            "loss: 1.438228  [   29/ 3200]\n",
            "loss: 1.419839  [   30/ 3200]\n",
            "loss: 1.377177  [   31/ 3200]\n",
            "loss: 1.382166  [   32/ 3200]\n",
            "loss: 1.389699  [   33/ 3200]\n",
            "loss: 1.394580  [   34/ 3200]\n",
            "loss: 1.372466  [   35/ 3200]\n",
            "loss: 1.379371  [   36/ 3200]\n",
            "loss: 1.437264  [   37/ 3200]\n",
            "loss: 1.358911  [   38/ 3200]\n",
            "loss: 1.409307  [   39/ 3200]\n",
            "loss: 1.373775  [   40/ 3200]\n",
            "loss: 1.389362  [   41/ 3200]\n",
            "loss: 1.390785  [   42/ 3200]\n",
            "loss: 1.414666  [   43/ 3200]\n",
            "loss: 1.373697  [   44/ 3200]\n",
            "loss: 1.390706  [   45/ 3200]\n",
            "loss: 1.382290  [   46/ 3200]\n",
            "loss: 1.406401  [   47/ 3200]\n",
            "loss: 1.394388  [   48/ 3200]\n",
            "loss: 1.391680  [   49/ 3200]\n",
            "loss: 1.422870  [   50/ 3200]\n",
            "loss: 1.378894  [   51/ 3200]\n",
            "loss: 1.372618  [   52/ 3200]\n",
            "loss: 1.347518  [   53/ 3200]\n",
            "loss: 1.412850  [   54/ 3200]\n",
            "loss: 1.396617  [   55/ 3200]\n",
            "loss: 1.384113  [   56/ 3200]\n",
            "loss: 1.415102  [   57/ 3200]\n",
            "loss: 1.365794  [   58/ 3200]\n",
            "loss: 1.389182  [   59/ 3200]\n",
            "loss: 1.431828  [   60/ 3200]\n",
            "loss: 1.424910  [   61/ 3200]\n",
            "loss: 1.419112  [   62/ 3200]\n",
            "loss: 1.383632  [   63/ 3200]\n",
            "loss: 1.395431  [   64/ 3200]\n",
            "loss: 1.414739  [   65/ 3200]\n",
            "loss: 1.396479  [   66/ 3200]\n",
            "loss: 1.384507  [   67/ 3200]\n",
            "loss: 1.403320  [   68/ 3200]\n",
            "loss: 1.390281  [   69/ 3200]\n",
            "loss: 1.393882  [   70/ 3200]\n",
            "loss: 1.388421  [   71/ 3200]\n",
            "loss: 1.413234  [   72/ 3200]\n",
            "loss: 1.391264  [   73/ 3200]\n",
            "loss: 1.389476  [   74/ 3200]\n",
            "loss: 1.385869  [   75/ 3200]\n",
            "loss: 1.362136  [   76/ 3200]\n",
            "loss: 1.381170  [   77/ 3200]\n",
            "loss: 1.395746  [   78/ 3200]\n",
            "loss: 1.431165  [   79/ 3200]\n",
            "loss: 1.378726  [   80/ 3200]\n",
            "loss: 1.391324  [   81/ 3200]\n",
            "loss: 1.383528  [   82/ 3200]\n",
            "loss: 1.411015  [   83/ 3200]\n",
            "loss: 1.389934  [   84/ 3200]\n",
            "loss: 1.385401  [   85/ 3200]\n",
            "loss: 1.367050  [   86/ 3200]\n",
            "loss: 1.393892  [   87/ 3200]\n",
            "loss: 1.427281  [   88/ 3200]\n",
            "loss: 1.401990  [   89/ 3200]\n",
            "loss: 1.388459  [   90/ 3200]\n",
            "loss: 1.401643  [   91/ 3200]\n",
            "loss: 1.395115  [   92/ 3200]\n",
            "loss: 1.375974  [   93/ 3200]\n",
            "loss: 1.390440  [   94/ 3200]\n",
            "loss: 1.376883  [   95/ 3200]\n",
            "loss: 1.333262  [   96/ 3200]\n",
            "loss: 1.359181  [   97/ 3200]\n",
            "loss: 1.424045  [   98/ 3200]\n",
            "loss: 1.378748  [   99/ 3200]\n",
            "loss: 1.369903  [  100/ 3200]\n",
            "loss: 1.428099  [  101/ 3200]\n",
            "loss: 1.391285  [  102/ 3200]\n",
            "loss: 1.408294  [  103/ 3200]\n",
            "loss: 1.359528  [  104/ 3200]\n",
            "loss: 1.391720  [  105/ 3200]\n",
            "loss: 1.335371  [  106/ 3200]\n",
            "loss: 1.391435  [  107/ 3200]\n",
            "loss: 1.359472  [  108/ 3200]\n",
            "loss: 1.379072  [  109/ 3200]\n",
            "loss: 1.388915  [  110/ 3200]\n",
            "loss: 1.411422  [  111/ 3200]\n",
            "loss: 1.378600  [  112/ 3200]\n",
            "loss: 1.449785  [  113/ 3200]\n",
            "loss: 1.358315  [  114/ 3200]\n",
            "loss: 1.368732  [  115/ 3200]\n",
            "loss: 1.352546  [  116/ 3200]\n",
            "loss: 1.354191  [  117/ 3200]\n",
            "loss: 1.431678  [  118/ 3200]\n",
            "loss: 1.401775  [  119/ 3200]\n",
            "loss: 1.364357  [  120/ 3200]\n",
            "loss: 1.400394  [  121/ 3200]\n",
            "loss: 1.363109  [  122/ 3200]\n",
            "loss: 1.355650  [  123/ 3200]\n",
            "loss: 1.372800  [  124/ 3200]\n",
            "loss: 1.431172  [  125/ 3200]\n",
            "loss: 1.403699  [  126/ 3200]\n",
            "loss: 1.425386  [  127/ 3200]\n",
            "loss: 1.400208  [  128/ 3200]\n",
            "loss: 1.351594  [  129/ 3200]\n",
            "loss: 1.375718  [  130/ 3200]\n",
            "loss: 1.414182  [  131/ 3200]\n",
            "loss: 1.404531  [  132/ 3200]\n",
            "loss: 1.372313  [  133/ 3200]\n",
            "loss: 1.403874  [  134/ 3200]\n",
            "loss: 1.387002  [  135/ 3200]\n",
            "loss: 1.389147  [  136/ 3200]\n",
            "loss: 1.377892  [  137/ 3200]\n",
            "loss: 1.376832  [  138/ 3200]\n",
            "loss: 1.387609  [  139/ 3200]\n",
            "loss: 1.374880  [  140/ 3200]\n",
            "loss: 1.423091  [  141/ 3200]\n",
            "loss: 1.376953  [  142/ 3200]\n",
            "loss: 1.393147  [  143/ 3200]\n",
            "loss: 1.363729  [  144/ 3200]\n",
            "loss: 1.430738  [  145/ 3200]\n",
            "loss: 1.397746  [  146/ 3200]\n",
            "loss: 1.395425  [  147/ 3200]\n",
            "loss: 1.390368  [  148/ 3200]\n",
            "loss: 1.377413  [  149/ 3200]\n",
            "loss: 1.385541  [  150/ 3200]\n",
            "loss: 1.406706  [  151/ 3200]\n",
            "loss: 1.367444  [  152/ 3200]\n",
            "loss: 1.363132  [  153/ 3200]\n",
            "loss: 1.379165  [  154/ 3200]\n",
            "loss: 1.405256  [  155/ 3200]\n",
            "loss: 1.395142  [  156/ 3200]\n",
            "loss: 1.404486  [  157/ 3200]\n",
            "loss: 1.404109  [  158/ 3200]\n",
            "loss: 1.396316  [  159/ 3200]\n",
            "loss: 1.383160  [  160/ 3200]\n",
            "loss: 1.374310  [  161/ 3200]\n",
            "loss: 1.396789  [  162/ 3200]\n",
            "loss: 1.394961  [  163/ 3200]\n",
            "loss: 1.394836  [  164/ 3200]\n",
            "loss: 1.388840  [  165/ 3200]\n",
            "loss: 1.398834  [  166/ 3200]\n",
            "loss: 1.368224  [  167/ 3200]\n",
            "loss: 1.337659  [  168/ 3200]\n",
            "loss: 1.389144  [  169/ 3200]\n",
            "loss: 1.414688  [  170/ 3200]\n",
            "loss: 1.365273  [  171/ 3200]\n",
            "loss: 1.389738  [  172/ 3200]\n",
            "loss: 1.366739  [  173/ 3200]\n",
            "loss: 1.395931  [  174/ 3200]\n",
            "loss: 1.406627  [  175/ 3200]\n",
            "loss: 1.377900  [  176/ 3200]\n",
            "loss: 1.380169  [  177/ 3200]\n",
            "loss: 1.431799  [  178/ 3200]\n",
            "loss: 1.367436  [  179/ 3200]\n",
            "loss: 1.397391  [  180/ 3200]\n",
            "loss: 1.367782  [  181/ 3200]\n",
            "loss: 1.395656  [  182/ 3200]\n",
            "loss: 1.408652  [  183/ 3200]\n",
            "loss: 1.376413  [  184/ 3200]\n",
            "loss: 1.371540  [  185/ 3200]\n",
            "loss: 1.345644  [  186/ 3200]\n",
            "loss: 1.385499  [  187/ 3200]\n",
            "loss: 1.347861  [  188/ 3200]\n",
            "loss: 1.382104  [  189/ 3200]\n",
            "loss: 1.369504  [  190/ 3200]\n",
            "loss: 1.403814  [  191/ 3200]\n",
            "loss: 1.369130  [  192/ 3200]\n",
            "loss: 1.356429  [  193/ 3200]\n",
            "loss: 1.373022  [  194/ 3200]\n",
            "loss: 1.357242  [  195/ 3200]\n",
            "loss: 1.430771  [  196/ 3200]\n",
            "loss: 1.382008  [  197/ 3200]\n",
            "loss: 1.363129  [  198/ 3200]\n",
            "loss: 1.398793  [  199/ 3200]\n",
            "Epoch:  2\n",
            "loss: 1.409332  [    0/ 3200]\n",
            "loss: 1.425341  [    1/ 3200]\n",
            "loss: 1.388503  [    2/ 3200]\n",
            "loss: 1.378293  [    3/ 3200]\n",
            "loss: 1.365466  [    4/ 3200]\n",
            "loss: 1.368744  [    5/ 3200]\n",
            "loss: 1.387782  [    6/ 3200]\n",
            "loss: 1.378089  [    7/ 3200]\n",
            "loss: 1.390477  [    8/ 3200]\n",
            "loss: 1.375763  [    9/ 3200]\n",
            "loss: 1.371507  [   10/ 3200]\n",
            "loss: 1.357027  [   11/ 3200]\n",
            "loss: 1.376554  [   12/ 3200]\n",
            "loss: 1.376490  [   13/ 3200]\n",
            "loss: 1.385858  [   14/ 3200]\n",
            "loss: 1.369580  [   15/ 3200]\n",
            "loss: 1.395679  [   16/ 3200]\n",
            "loss: 1.366858  [   17/ 3200]\n",
            "loss: 1.369518  [   18/ 3200]\n",
            "loss: 1.389833  [   19/ 3200]\n",
            "loss: 1.423490  [   20/ 3200]\n",
            "loss: 1.337854  [   21/ 3200]\n",
            "loss: 1.386179  [   22/ 3200]\n",
            "loss: 1.406489  [   23/ 3200]\n",
            "loss: 1.345034  [   24/ 3200]\n",
            "loss: 1.378535  [   25/ 3200]\n",
            "loss: 1.402774  [   26/ 3200]\n",
            "loss: 1.401410  [   27/ 3200]\n",
            "loss: 1.373285  [   28/ 3200]\n",
            "loss: 1.392514  [   29/ 3200]\n",
            "loss: 1.399390  [   30/ 3200]\n",
            "loss: 1.387148  [   31/ 3200]\n",
            "loss: 1.394168  [   32/ 3200]\n",
            "loss: 1.372221  [   33/ 3200]\n",
            "loss: 1.373768  [   34/ 3200]\n",
            "loss: 1.383385  [   35/ 3200]\n",
            "loss: 1.369025  [   36/ 3200]\n",
            "loss: 1.374712  [   37/ 3200]\n",
            "loss: 1.372635  [   38/ 3200]\n",
            "loss: 1.382540  [   39/ 3200]\n",
            "loss: 1.381378  [   40/ 3200]\n",
            "loss: 1.387805  [   41/ 3200]\n",
            "loss: 1.392366  [   42/ 3200]\n",
            "loss: 1.390572  [   43/ 3200]\n",
            "loss: 1.380494  [   44/ 3200]\n",
            "loss: 1.396060  [   45/ 3200]\n",
            "loss: 1.379033  [   46/ 3200]\n",
            "loss: 1.371752  [   47/ 3200]\n",
            "loss: 1.391457  [   48/ 3200]\n",
            "loss: 1.403871  [   49/ 3200]\n",
            "loss: 1.396380  [   50/ 3200]\n",
            "loss: 1.391122  [   51/ 3200]\n",
            "loss: 1.366086  [   52/ 3200]\n",
            "loss: 1.361530  [   53/ 3200]\n",
            "loss: 1.364936  [   54/ 3200]\n",
            "loss: 1.363210  [   55/ 3200]\n",
            "loss: 1.352295  [   56/ 3200]\n",
            "loss: 1.397661  [   57/ 3200]\n",
            "loss: 1.380247  [   58/ 3200]\n",
            "loss: 1.382962  [   59/ 3200]\n",
            "loss: 1.369518  [   60/ 3200]\n",
            "loss: 1.413652  [   61/ 3200]\n",
            "loss: 1.356274  [   62/ 3200]\n",
            "loss: 1.371430  [   63/ 3200]\n",
            "loss: 1.361201  [   64/ 3200]\n",
            "loss: 1.359591  [   65/ 3200]\n",
            "loss: 1.365959  [   66/ 3200]\n",
            "loss: 1.361471  [   67/ 3200]\n",
            "loss: 1.467313  [   68/ 3200]\n",
            "loss: 1.351637  [   69/ 3200]\n",
            "loss: 1.363619  [   70/ 3200]\n",
            "loss: 1.365225  [   71/ 3200]\n",
            "loss: 1.358464  [   72/ 3200]\n",
            "loss: 1.365259  [   73/ 3200]\n",
            "loss: 1.352656  [   74/ 3200]\n",
            "loss: 1.388044  [   75/ 3200]\n",
            "loss: 1.372421  [   76/ 3200]\n",
            "loss: 1.368934  [   77/ 3200]\n",
            "loss: 1.354707  [   78/ 3200]\n",
            "loss: 1.399270  [   79/ 3200]\n",
            "loss: 1.427254  [   80/ 3200]\n",
            "loss: 1.402558  [   81/ 3200]\n",
            "loss: 1.384532  [   82/ 3200]\n",
            "loss: 1.386822  [   83/ 3200]\n",
            "loss: 1.353393  [   84/ 3200]\n",
            "loss: 1.357999  [   85/ 3200]\n",
            "loss: 1.402448  [   86/ 3200]\n",
            "loss: 1.373040  [   87/ 3200]\n",
            "loss: 1.400330  [   88/ 3200]\n",
            "loss: 1.371762  [   89/ 3200]\n",
            "loss: 1.366296  [   90/ 3200]\n",
            "loss: 1.400115  [   91/ 3200]\n",
            "loss: 1.402977  [   92/ 3200]\n",
            "loss: 1.411610  [   93/ 3200]\n",
            "loss: 1.378985  [   94/ 3200]\n",
            "loss: 1.353445  [   95/ 3200]\n",
            "loss: 1.373849  [   96/ 3200]\n",
            "loss: 1.408382  [   97/ 3200]\n",
            "loss: 1.366775  [   98/ 3200]\n",
            "loss: 1.394448  [   99/ 3200]\n",
            "loss: 1.377465  [  100/ 3200]\n",
            "loss: 1.379489  [  101/ 3200]\n",
            "loss: 1.374340  [  102/ 3200]\n",
            "loss: 1.377215  [  103/ 3200]\n",
            "loss: 1.400107  [  104/ 3200]\n",
            "loss: 1.382861  [  105/ 3200]\n",
            "loss: 1.382531  [  106/ 3200]\n",
            "loss: 1.380294  [  107/ 3200]\n",
            "loss: 1.381835  [  108/ 3200]\n",
            "loss: 1.390129  [  109/ 3200]\n",
            "loss: 1.357179  [  110/ 3200]\n",
            "loss: 1.372234  [  111/ 3200]\n",
            "loss: 1.357832  [  112/ 3200]\n",
            "loss: 1.430629  [  113/ 3200]\n",
            "loss: 1.387362  [  114/ 3200]\n",
            "loss: 1.367757  [  115/ 3200]\n",
            "loss: 1.363534  [  116/ 3200]\n",
            "loss: 1.376316  [  117/ 3200]\n",
            "loss: 1.350601  [  118/ 3200]\n",
            "loss: 1.405499  [  119/ 3200]\n",
            "loss: 1.354206  [  120/ 3200]\n",
            "loss: 1.406698  [  121/ 3200]\n",
            "loss: 1.381598  [  122/ 3200]\n",
            "loss: 1.397454  [  123/ 3200]\n",
            "loss: 1.390007  [  124/ 3200]\n",
            "loss: 1.385204  [  125/ 3200]\n",
            "loss: 1.375032  [  126/ 3200]\n",
            "loss: 1.333108  [  127/ 3200]\n",
            "loss: 1.404589  [  128/ 3200]\n",
            "loss: 1.373856  [  129/ 3200]\n",
            "loss: 1.362075  [  130/ 3200]\n",
            "loss: 1.319417  [  131/ 3200]\n",
            "loss: 1.372151  [  132/ 3200]\n",
            "loss: 1.392073  [  133/ 3200]\n",
            "loss: 1.365030  [  134/ 3200]\n",
            "loss: 1.365135  [  135/ 3200]\n",
            "loss: 1.367918  [  136/ 3200]\n",
            "loss: 1.368673  [  137/ 3200]\n",
            "loss: 1.347489  [  138/ 3200]\n",
            "loss: 1.391490  [  139/ 3200]\n",
            "loss: 1.324471  [  140/ 3200]\n",
            "loss: 1.347923  [  141/ 3200]\n",
            "loss: 1.372597  [  142/ 3200]\n",
            "loss: 1.352348  [  143/ 3200]\n",
            "loss: 1.311892  [  144/ 3200]\n",
            "loss: 1.390637  [  145/ 3200]\n",
            "loss: 1.437488  [  146/ 3200]\n",
            "loss: 1.420128  [  147/ 3200]\n",
            "loss: 1.405164  [  148/ 3200]\n",
            "loss: 1.355283  [  149/ 3200]\n",
            "loss: 1.383980  [  150/ 3200]\n",
            "loss: 1.386878  [  151/ 3200]\n",
            "loss: 1.346480  [  152/ 3200]\n",
            "loss: 1.420779  [  153/ 3200]\n",
            "loss: 1.370142  [  154/ 3200]\n",
            "loss: 1.375502  [  155/ 3200]\n",
            "loss: 1.372743  [  156/ 3200]\n",
            "loss: 1.364459  [  157/ 3200]\n",
            "loss: 1.332575  [  158/ 3200]\n",
            "loss: 1.395038  [  159/ 3200]\n",
            "loss: 1.391038  [  160/ 3200]\n",
            "loss: 1.398000  [  161/ 3200]\n",
            "loss: 1.384822  [  162/ 3200]\n",
            "loss: 1.392534  [  163/ 3200]\n",
            "loss: 1.374084  [  164/ 3200]\n",
            "loss: 1.374885  [  165/ 3200]\n",
            "loss: 1.351509  [  166/ 3200]\n",
            "loss: 1.356753  [  167/ 3200]\n",
            "loss: 1.375036  [  168/ 3200]\n",
            "loss: 1.372097  [  169/ 3200]\n",
            "loss: 1.348813  [  170/ 3200]\n",
            "loss: 1.356341  [  171/ 3200]\n",
            "loss: 1.396280  [  172/ 3200]\n",
            "loss: 1.384851  [  173/ 3200]\n",
            "loss: 1.399060  [  174/ 3200]\n",
            "loss: 1.387368  [  175/ 3200]\n",
            "loss: 1.391118  [  176/ 3200]\n",
            "loss: 1.368568  [  177/ 3200]\n",
            "loss: 1.382031  [  178/ 3200]\n",
            "loss: 1.380367  [  179/ 3200]\n",
            "loss: 1.382040  [  180/ 3200]\n",
            "loss: 1.377684  [  181/ 3200]\n",
            "loss: 1.377389  [  182/ 3200]\n",
            "loss: 1.359046  [  183/ 3200]\n",
            "loss: 1.388066  [  184/ 3200]\n",
            "loss: 1.379348  [  185/ 3200]\n",
            "loss: 1.381220  [  186/ 3200]\n",
            "loss: 1.362970  [  187/ 3200]\n",
            "loss: 1.379027  [  188/ 3200]\n",
            "loss: 1.393939  [  189/ 3200]\n",
            "loss: 1.388489  [  190/ 3200]\n",
            "loss: 1.379876  [  191/ 3200]\n",
            "loss: 1.387234  [  192/ 3200]\n",
            "loss: 1.353854  [  193/ 3200]\n",
            "loss: 1.337314  [  194/ 3200]\n",
            "loss: 1.358693  [  195/ 3200]\n",
            "loss: 1.328628  [  196/ 3200]\n",
            "loss: 1.321645  [  197/ 3200]\n",
            "loss: 1.376613  [  198/ 3200]\n",
            "loss: 1.361038  [  199/ 3200]\n",
            "Epoch:  3\n",
            "loss: 1.443382  [    0/ 3200]\n",
            "loss: 1.341273  [    1/ 3200]\n",
            "loss: 1.337476  [    2/ 3200]\n",
            "loss: 1.358255  [    3/ 3200]\n",
            "loss: 1.401600  [    4/ 3200]\n",
            "loss: 1.390381  [    5/ 3200]\n",
            "loss: 1.373964  [    6/ 3200]\n",
            "loss: 1.350157  [    7/ 3200]\n",
            "loss: 1.337877  [    8/ 3200]\n",
            "loss: 1.342807  [    9/ 3200]\n",
            "loss: 1.350236  [   10/ 3200]\n",
            "loss: 1.400517  [   11/ 3200]\n",
            "loss: 1.393866  [   12/ 3200]\n",
            "loss: 1.395032  [   13/ 3200]\n",
            "loss: 1.336238  [   14/ 3200]\n",
            "loss: 1.382985  [   15/ 3200]\n",
            "loss: 1.319347  [   16/ 3200]\n",
            "loss: 1.346953  [   17/ 3200]\n",
            "loss: 1.376344  [   18/ 3200]\n",
            "loss: 1.380535  [   19/ 3200]\n",
            "loss: 1.384764  [   20/ 3200]\n",
            "loss: 1.381677  [   21/ 3200]\n",
            "loss: 1.390297  [   22/ 3200]\n",
            "loss: 1.377849  [   23/ 3200]\n",
            "loss: 1.361490  [   24/ 3200]\n",
            "loss: 1.370301  [   25/ 3200]\n",
            "loss: 1.373650  [   26/ 3200]\n",
            "loss: 1.374155  [   27/ 3200]\n",
            "loss: 1.363173  [   28/ 3200]\n",
            "loss: 1.382324  [   29/ 3200]\n",
            "loss: 1.380567  [   30/ 3200]\n",
            "loss: 1.384387  [   31/ 3200]\n",
            "loss: 1.370234  [   32/ 3200]\n",
            "loss: 1.397582  [   33/ 3200]\n",
            "loss: 1.379854  [   34/ 3200]\n",
            "loss: 1.354698  [   35/ 3200]\n",
            "loss: 1.385084  [   36/ 3200]\n",
            "loss: 1.359390  [   37/ 3200]\n",
            "loss: 1.359671  [   38/ 3200]\n",
            "loss: 1.354027  [   39/ 3200]\n",
            "loss: 1.352835  [   40/ 3200]\n",
            "loss: 1.370392  [   41/ 3200]\n",
            "loss: 1.387003  [   42/ 3200]\n",
            "loss: 1.324458  [   43/ 3200]\n",
            "loss: 1.401236  [   44/ 3200]\n",
            "loss: 1.386809  [   45/ 3200]\n",
            "loss: 1.367911  [   46/ 3200]\n",
            "loss: 1.366845  [   47/ 3200]\n",
            "loss: 1.362041  [   48/ 3200]\n",
            "loss: 1.340350  [   49/ 3200]\n",
            "loss: 1.348989  [   50/ 3200]\n",
            "loss: 1.384249  [   51/ 3200]\n",
            "loss: 1.376700  [   52/ 3200]\n",
            "loss: 1.355468  [   53/ 3200]\n",
            "loss: 1.361346  [   54/ 3200]\n",
            "loss: 1.344248  [   55/ 3200]\n",
            "loss: 1.331167  [   56/ 3200]\n",
            "loss: 1.382232  [   57/ 3200]\n",
            "loss: 1.438388  [   58/ 3200]\n",
            "loss: 1.385303  [   59/ 3200]\n",
            "loss: 1.394181  [   60/ 3200]\n",
            "loss: 1.387130  [   61/ 3200]\n",
            "loss: 1.365243  [   62/ 3200]\n",
            "loss: 1.383713  [   63/ 3200]\n",
            "loss: 1.365726  [   64/ 3200]\n",
            "loss: 1.377726  [   65/ 3200]\n",
            "loss: 1.370890  [   66/ 3200]\n",
            "loss: 1.380320  [   67/ 3200]\n",
            "loss: 1.366986  [   68/ 3200]\n",
            "loss: 1.378051  [   69/ 3200]\n",
            "loss: 1.380409  [   70/ 3200]\n",
            "loss: 1.368074  [   71/ 3200]\n",
            "loss: 1.370004  [   72/ 3200]\n",
            "loss: 1.375097  [   73/ 3200]\n",
            "loss: 1.356510  [   74/ 3200]\n",
            "loss: 1.352060  [   75/ 3200]\n",
            "loss: 1.358261  [   76/ 3200]\n",
            "loss: 1.361233  [   77/ 3200]\n",
            "loss: 1.384057  [   78/ 3200]\n",
            "loss: 1.367479  [   79/ 3200]\n",
            "loss: 1.358851  [   80/ 3200]\n",
            "loss: 1.355234  [   81/ 3200]\n",
            "loss: 1.396314  [   82/ 3200]\n",
            "loss: 1.350757  [   83/ 3200]\n",
            "loss: 1.359007  [   84/ 3200]\n",
            "loss: 1.376775  [   85/ 3200]\n",
            "loss: 1.339942  [   86/ 3200]\n",
            "loss: 1.364141  [   87/ 3200]\n",
            "loss: 1.390942  [   88/ 3200]\n",
            "loss: 1.398841  [   89/ 3200]\n",
            "loss: 1.368497  [   90/ 3200]\n",
            "loss: 1.330531  [   91/ 3200]\n",
            "loss: 1.351343  [   92/ 3200]\n",
            "loss: 1.368188  [   93/ 3200]\n",
            "loss: 1.395748  [   94/ 3200]\n",
            "loss: 1.382414  [   95/ 3200]\n",
            "loss: 1.354711  [   96/ 3200]\n",
            "loss: 1.352421  [   97/ 3200]\n",
            "loss: 1.430066  [   98/ 3200]\n",
            "loss: 1.341361  [   99/ 3200]\n",
            "loss: 1.352711  [  100/ 3200]\n",
            "loss: 1.362581  [  101/ 3200]\n",
            "loss: 1.372912  [  102/ 3200]\n",
            "loss: 1.376769  [  103/ 3200]\n",
            "loss: 1.360066  [  104/ 3200]\n",
            "loss: 1.383885  [  105/ 3200]\n",
            "loss: 1.355989  [  106/ 3200]\n",
            "loss: 1.376714  [  107/ 3200]\n",
            "loss: 1.369795  [  108/ 3200]\n",
            "loss: 1.366470  [  109/ 3200]\n",
            "loss: 1.345987  [  110/ 3200]\n",
            "loss: 1.395184  [  111/ 3200]\n",
            "loss: 1.375908  [  112/ 3200]\n",
            "loss: 1.378763  [  113/ 3200]\n",
            "loss: 1.348174  [  114/ 3200]\n",
            "loss: 1.402860  [  115/ 3200]\n",
            "loss: 1.373706  [  116/ 3200]\n",
            "loss: 1.374387  [  117/ 3200]\n",
            "loss: 1.374071  [  118/ 3200]\n",
            "loss: 1.347241  [  119/ 3200]\n",
            "loss: 1.367152  [  120/ 3200]\n",
            "loss: 1.360273  [  121/ 3200]\n",
            "loss: 1.350513  [  122/ 3200]\n",
            "loss: 1.371834  [  123/ 3200]\n",
            "loss: 1.397545  [  124/ 3200]\n",
            "loss: 1.342061  [  125/ 3200]\n",
            "loss: 1.379265  [  126/ 3200]\n",
            "loss: 1.339853  [  127/ 3200]\n",
            "loss: 1.355716  [  128/ 3200]\n",
            "loss: 1.362351  [  129/ 3200]\n",
            "loss: 1.359742  [  130/ 3200]\n",
            "loss: 1.326490  [  131/ 3200]\n",
            "loss: 1.389300  [  132/ 3200]\n",
            "loss: 1.375114  [  133/ 3200]\n",
            "loss: 1.350465  [  134/ 3200]\n",
            "loss: 1.376765  [  135/ 3200]\n",
            "loss: 1.373229  [  136/ 3200]\n",
            "loss: 1.386269  [  137/ 3200]\n",
            "loss: 1.377118  [  138/ 3200]\n",
            "loss: 1.373489  [  139/ 3200]\n",
            "loss: 1.372717  [  140/ 3200]\n",
            "loss: 1.397510  [  141/ 3200]\n",
            "loss: 1.379853  [  142/ 3200]\n",
            "loss: 1.351942  [  143/ 3200]\n",
            "loss: 1.357511  [  144/ 3200]\n",
            "loss: 1.384337  [  145/ 3200]\n",
            "loss: 1.365183  [  146/ 3200]\n",
            "loss: 1.372203  [  147/ 3200]\n",
            "loss: 1.359895  [  148/ 3200]\n",
            "loss: 1.379396  [  149/ 3200]\n",
            "loss: 1.370509  [  150/ 3200]\n",
            "loss: 1.368806  [  151/ 3200]\n",
            "loss: 1.361460  [  152/ 3200]\n",
            "loss: 1.361795  [  153/ 3200]\n",
            "loss: 1.354492  [  154/ 3200]\n",
            "loss: 1.358928  [  155/ 3200]\n",
            "loss: 1.350142  [  156/ 3200]\n",
            "loss: 1.307150  [  157/ 3200]\n",
            "loss: 1.351570  [  158/ 3200]\n",
            "loss: 1.336314  [  159/ 3200]\n",
            "loss: 1.384081  [  160/ 3200]\n",
            "loss: 1.307813  [  161/ 3200]\n",
            "loss: 1.300887  [  162/ 3200]\n",
            "loss: 1.347973  [  163/ 3200]\n",
            "loss: 1.360501  [  164/ 3200]\n",
            "loss: 1.334867  [  165/ 3200]\n",
            "loss: 1.288343  [  166/ 3200]\n",
            "loss: 1.348844  [  167/ 3200]\n",
            "loss: 1.385545  [  168/ 3200]\n",
            "loss: 1.427074  [  169/ 3200]\n",
            "loss: 1.368179  [  170/ 3200]\n",
            "loss: 1.318404  [  171/ 3200]\n",
            "loss: 1.312527  [  172/ 3200]\n",
            "loss: 1.282386  [  173/ 3200]\n",
            "loss: 1.337252  [  174/ 3200]\n",
            "loss: 1.386476  [  175/ 3200]\n",
            "loss: 1.425428  [  176/ 3200]\n",
            "loss: 1.398065  [  177/ 3200]\n",
            "loss: 1.336352  [  178/ 3200]\n",
            "loss: 1.407850  [  179/ 3200]\n",
            "loss: 1.344432  [  180/ 3200]\n",
            "loss: 1.339061  [  181/ 3200]\n",
            "loss: 1.354602  [  182/ 3200]\n",
            "loss: 1.349654  [  183/ 3200]\n",
            "loss: 1.375538  [  184/ 3200]\n",
            "loss: 1.345282  [  185/ 3200]\n",
            "loss: 1.398818  [  186/ 3200]\n",
            "loss: 1.349726  [  187/ 3200]\n",
            "loss: 1.425996  [  188/ 3200]\n",
            "loss: 1.356284  [  189/ 3200]\n",
            "loss: 1.357686  [  190/ 3200]\n",
            "loss: 1.363545  [  191/ 3200]\n",
            "loss: 1.345630  [  192/ 3200]\n",
            "loss: 1.388546  [  193/ 3200]\n",
            "loss: 1.394770  [  194/ 3200]\n",
            "loss: 1.408020  [  195/ 3200]\n",
            "loss: 1.350919  [  196/ 3200]\n",
            "loss: 1.369590  [  197/ 3200]\n",
            "loss: 1.360338  [  198/ 3200]\n",
            "loss: 1.383173  [  199/ 3200]\n",
            "Epoch:  4\n",
            "loss: 1.373786  [    0/ 3200]\n",
            "loss: 1.364084  [    1/ 3200]\n",
            "loss: 1.352828  [    2/ 3200]\n",
            "loss: 1.348515  [    3/ 3200]\n",
            "loss: 1.320534  [    4/ 3200]\n",
            "loss: 1.355217  [    5/ 3200]\n",
            "loss: 1.357128  [    6/ 3200]\n",
            "loss: 1.341811  [    7/ 3200]\n",
            "loss: 1.368000  [    8/ 3200]\n",
            "loss: 1.387953  [    9/ 3200]\n",
            "loss: 1.330353  [   10/ 3200]\n",
            "loss: 1.360184  [   11/ 3200]\n",
            "loss: 1.353706  [   12/ 3200]\n",
            "loss: 1.316610  [   13/ 3200]\n",
            "loss: 1.361363  [   14/ 3200]\n",
            "loss: 1.355722  [   15/ 3200]\n",
            "loss: 1.315164  [   16/ 3200]\n",
            "loss: 1.383641  [   17/ 3200]\n",
            "loss: 1.417745  [   18/ 3200]\n",
            "loss: 1.372213  [   19/ 3200]\n",
            "loss: 1.352916  [   20/ 3200]\n",
            "loss: 1.341840  [   21/ 3200]\n",
            "loss: 1.367148  [   22/ 3200]\n",
            "loss: 1.423834  [   23/ 3200]\n",
            "loss: 1.380215  [   24/ 3200]\n",
            "loss: 1.337844  [   25/ 3200]\n",
            "loss: 1.359148  [   26/ 3200]\n",
            "loss: 1.399137  [   27/ 3200]\n",
            "loss: 1.339654  [   28/ 3200]\n",
            "loss: 1.363474  [   29/ 3200]\n",
            "loss: 1.365163  [   30/ 3200]\n",
            "loss: 1.357781  [   31/ 3200]\n",
            "loss: 1.367043  [   32/ 3200]\n",
            "loss: 1.346621  [   33/ 3200]\n",
            "loss: 1.405987  [   34/ 3200]\n",
            "loss: 1.359037  [   35/ 3200]\n",
            "loss: 1.367594  [   36/ 3200]\n",
            "loss: 1.384438  [   37/ 3200]\n",
            "loss: 1.363325  [   38/ 3200]\n",
            "loss: 1.359319  [   39/ 3200]\n",
            "loss: 1.367018  [   40/ 3200]\n",
            "loss: 1.363302  [   41/ 3200]\n",
            "loss: 1.351137  [   42/ 3200]\n",
            "loss: 1.364469  [   43/ 3200]\n",
            "loss: 1.361974  [   44/ 3200]\n",
            "loss: 1.360990  [   45/ 3200]\n",
            "loss: 1.364212  [   46/ 3200]\n",
            "loss: 1.347001  [   47/ 3200]\n",
            "loss: 1.327416  [   48/ 3200]\n",
            "loss: 1.328741  [   49/ 3200]\n",
            "loss: 1.362681  [   50/ 3200]\n",
            "loss: 1.353961  [   51/ 3200]\n",
            "loss: 1.338373  [   52/ 3200]\n",
            "loss: 1.348974  [   53/ 3200]\n",
            "loss: 1.344333  [   54/ 3200]\n",
            "loss: 1.380309  [   55/ 3200]\n",
            "loss: 1.312068  [   56/ 3200]\n",
            "loss: 1.382532  [   57/ 3200]\n",
            "loss: 1.385928  [   58/ 3200]\n",
            "loss: 1.349639  [   59/ 3200]\n",
            "loss: 1.377833  [   60/ 3200]\n",
            "loss: 1.337223  [   61/ 3200]\n",
            "loss: 1.365275  [   62/ 3200]\n",
            "loss: 1.358777  [   63/ 3200]\n",
            "loss: 1.302040  [   64/ 3200]\n",
            "loss: 1.410993  [   65/ 3200]\n",
            "loss: 1.397141  [   66/ 3200]\n",
            "loss: 1.359812  [   67/ 3200]\n",
            "loss: 1.358433  [   68/ 3200]\n",
            "loss: 1.366486  [   69/ 3200]\n",
            "loss: 1.347916  [   70/ 3200]\n",
            "loss: 1.344445  [   71/ 3200]\n",
            "loss: 1.339137  [   72/ 3200]\n",
            "loss: 1.373783  [   73/ 3200]\n",
            "loss: 1.431118  [   74/ 3200]\n",
            "loss: 1.368320  [   75/ 3200]\n",
            "loss: 1.381589  [   76/ 3200]\n",
            "loss: 1.353438  [   77/ 3200]\n",
            "loss: 1.346599  [   78/ 3200]\n",
            "loss: 1.369538  [   79/ 3200]\n",
            "loss: 1.339731  [   80/ 3200]\n",
            "loss: 1.351905  [   81/ 3200]\n",
            "loss: 1.371969  [   82/ 3200]\n",
            "loss: 1.371721  [   83/ 3200]\n",
            "loss: 1.347688  [   84/ 3200]\n",
            "loss: 1.416128  [   85/ 3200]\n",
            "loss: 1.359988  [   86/ 3200]\n",
            "loss: 1.372793  [   87/ 3200]\n",
            "loss: 1.361194  [   88/ 3200]\n",
            "loss: 1.365190  [   89/ 3200]\n",
            "loss: 1.369581  [   90/ 3200]\n",
            "loss: 1.344569  [   91/ 3200]\n",
            "loss: 1.358125  [   92/ 3200]\n",
            "loss: 1.368129  [   93/ 3200]\n",
            "loss: 1.374841  [   94/ 3200]\n",
            "loss: 1.370842  [   95/ 3200]\n",
            "loss: 1.367382  [   96/ 3200]\n",
            "loss: 1.350261  [   97/ 3200]\n",
            "loss: 1.371254  [   98/ 3200]\n",
            "loss: 1.387192  [   99/ 3200]\n",
            "loss: 1.352325  [  100/ 3200]\n",
            "loss: 1.346609  [  101/ 3200]\n",
            "loss: 1.352829  [  102/ 3200]\n",
            "loss: 1.365643  [  103/ 3200]\n",
            "loss: 1.361799  [  104/ 3200]\n",
            "loss: 1.345974  [  105/ 3200]\n",
            "loss: 1.343130  [  106/ 3200]\n",
            "loss: 1.339530  [  107/ 3200]\n",
            "loss: 1.358465  [  108/ 3200]\n",
            "loss: 1.362787  [  109/ 3200]\n",
            "loss: 1.376852  [  110/ 3200]\n",
            "loss: 1.348983  [  111/ 3200]\n",
            "loss: 1.354245  [  112/ 3200]\n",
            "loss: 1.348439  [  113/ 3200]\n",
            "loss: 1.364187  [  114/ 3200]\n",
            "loss: 1.342064  [  115/ 3200]\n",
            "loss: 1.346547  [  116/ 3200]\n",
            "loss: 1.292943  [  117/ 3200]\n",
            "loss: 1.323641  [  118/ 3200]\n",
            "loss: 1.406142  [  119/ 3200]\n",
            "loss: 1.396128  [  120/ 3200]\n",
            "loss: 1.346432  [  121/ 3200]\n",
            "loss: 1.371687  [  122/ 3200]\n",
            "loss: 1.352507  [  123/ 3200]\n",
            "loss: 1.357874  [  124/ 3200]\n",
            "loss: 1.418612  [  125/ 3200]\n",
            "loss: 1.330680  [  126/ 3200]\n",
            "loss: 1.342885  [  127/ 3200]\n",
            "loss: 1.381521  [  128/ 3200]\n",
            "loss: 1.359800  [  129/ 3200]\n",
            "loss: 1.363706  [  130/ 3200]\n",
            "loss: 1.376136  [  131/ 3200]\n",
            "loss: 1.346684  [  132/ 3200]\n",
            "loss: 1.381716  [  133/ 3200]\n",
            "loss: 1.383419  [  134/ 3200]\n",
            "loss: 1.345848  [  135/ 3200]\n",
            "loss: 1.346953  [  136/ 3200]\n",
            "loss: 1.334347  [  137/ 3200]\n",
            "loss: 1.320020  [  138/ 3200]\n",
            "loss: 1.348957  [  139/ 3200]\n",
            "loss: 1.340332  [  140/ 3200]\n",
            "loss: 1.347865  [  141/ 3200]\n",
            "loss: 1.321941  [  142/ 3200]\n",
            "loss: 1.344931  [  143/ 3200]\n",
            "loss: 1.338447  [  144/ 3200]\n",
            "loss: 1.341959  [  145/ 3200]\n",
            "loss: 1.310569  [  146/ 3200]\n",
            "loss: 1.445085  [  147/ 3200]\n",
            "loss: 1.304915  [  148/ 3200]\n",
            "loss: 1.358585  [  149/ 3200]\n",
            "loss: 1.360492  [  150/ 3200]\n",
            "loss: 1.393576  [  151/ 3200]\n",
            "loss: 1.342273  [  152/ 3200]\n",
            "loss: 1.376042  [  153/ 3200]\n",
            "loss: 1.345540  [  154/ 3200]\n",
            "loss: 1.363236  [  155/ 3200]\n",
            "loss: 1.341312  [  156/ 3200]\n",
            "loss: 1.388939  [  157/ 3200]\n",
            "loss: 1.375003  [  158/ 3200]\n",
            "loss: 1.376325  [  159/ 3200]\n",
            "loss: 1.374725  [  160/ 3200]\n",
            "loss: 1.390384  [  161/ 3200]\n",
            "loss: 1.349804  [  162/ 3200]\n",
            "loss: 1.370988  [  163/ 3200]\n",
            "loss: 1.343604  [  164/ 3200]\n",
            "loss: 1.351292  [  165/ 3200]\n",
            "loss: 1.381366  [  166/ 3200]\n",
            "loss: 1.351772  [  167/ 3200]\n",
            "loss: 1.360435  [  168/ 3200]\n",
            "loss: 1.354076  [  169/ 3200]\n",
            "loss: 1.354788  [  170/ 3200]\n",
            "loss: 1.379049  [  171/ 3200]\n",
            "loss: 1.369775  [  172/ 3200]\n",
            "loss: 1.339108  [  173/ 3200]\n",
            "loss: 1.331906  [  174/ 3200]\n",
            "loss: 1.352557  [  175/ 3200]\n",
            "loss: 1.381243  [  176/ 3200]\n",
            "loss: 1.353100  [  177/ 3200]\n",
            "loss: 1.376191  [  178/ 3200]\n",
            "loss: 1.364195  [  179/ 3200]\n",
            "loss: 1.350826  [  180/ 3200]\n",
            "loss: 1.369743  [  181/ 3200]\n",
            "loss: 1.340090  [  182/ 3200]\n",
            "loss: 1.355149  [  183/ 3200]\n",
            "loss: 1.355922  [  184/ 3200]\n",
            "loss: 1.312904  [  185/ 3200]\n",
            "loss: 1.342655  [  186/ 3200]\n",
            "loss: 1.438359  [  187/ 3200]\n",
            "loss: 1.363738  [  188/ 3200]\n",
            "loss: 1.369228  [  189/ 3200]\n",
            "loss: 1.365442  [  190/ 3200]\n",
            "loss: 1.340127  [  191/ 3200]\n",
            "loss: 1.364814  [  192/ 3200]\n",
            "loss: 1.352245  [  193/ 3200]\n",
            "loss: 1.354250  [  194/ 3200]\n",
            "loss: 1.360632  [  195/ 3200]\n",
            "loss: 1.372449  [  196/ 3200]\n",
            "loss: 1.358027  [  197/ 3200]\n",
            "loss: 1.346837  [  198/ 3200]\n",
            "loss: 1.342819  [  199/ 3200]\n",
            "Epoch:  5\n",
            "loss: 1.355653  [    0/ 3200]\n",
            "loss: 1.352089  [    1/ 3200]\n",
            "loss: 1.343060  [    2/ 3200]\n",
            "loss: 1.325876  [    3/ 3200]\n",
            "loss: 1.379949  [    4/ 3200]\n",
            "loss: 1.339223  [    5/ 3200]\n",
            "loss: 1.354302  [    6/ 3200]\n",
            "loss: 1.375545  [    7/ 3200]\n",
            "loss: 1.367616  [    8/ 3200]\n",
            "loss: 1.321446  [    9/ 3200]\n",
            "loss: 1.388332  [   10/ 3200]\n",
            "loss: 1.363804  [   11/ 3200]\n",
            "loss: 1.355660  [   12/ 3200]\n",
            "loss: 1.348564  [   13/ 3200]\n",
            "loss: 1.392478  [   14/ 3200]\n",
            "loss: 1.322708  [   15/ 3200]\n",
            "loss: 1.359883  [   16/ 3200]\n",
            "loss: 1.348273  [   17/ 3200]\n",
            "loss: 1.349798  [   18/ 3200]\n",
            "loss: 1.343929  [   19/ 3200]\n",
            "loss: 1.320338  [   20/ 3200]\n",
            "loss: 1.362933  [   21/ 3200]\n",
            "loss: 1.372961  [   22/ 3200]\n",
            "loss: 1.353848  [   23/ 3200]\n",
            "loss: 1.336417  [   24/ 3200]\n",
            "loss: 1.361823  [   25/ 3200]\n",
            "loss: 1.324650  [   26/ 3200]\n",
            "loss: 1.381313  [   27/ 3200]\n",
            "loss: 1.370360  [   28/ 3200]\n",
            "loss: 1.339205  [   29/ 3200]\n",
            "loss: 1.347662  [   30/ 3200]\n",
            "loss: 1.365526  [   31/ 3200]\n",
            "loss: 1.377863  [   32/ 3200]\n",
            "loss: 1.329709  [   33/ 3200]\n",
            "loss: 1.384550  [   34/ 3200]\n",
            "loss: 1.394031  [   35/ 3200]\n",
            "loss: 1.349326  [   36/ 3200]\n",
            "loss: 1.347394  [   37/ 3200]\n",
            "loss: 1.342272  [   38/ 3200]\n",
            "loss: 1.361509  [   39/ 3200]\n",
            "loss: 1.342995  [   40/ 3200]\n",
            "loss: 1.366975  [   41/ 3200]\n",
            "loss: 1.334004  [   42/ 3200]\n",
            "loss: 1.348105  [   43/ 3200]\n",
            "loss: 1.334124  [   44/ 3200]\n",
            "loss: 1.370052  [   45/ 3200]\n",
            "loss: 1.358593  [   46/ 3200]\n",
            "loss: 1.346977  [   47/ 3200]\n",
            "loss: 1.371691  [   48/ 3200]\n",
            "loss: 1.358834  [   49/ 3200]\n",
            "loss: 1.360731  [   50/ 3200]\n",
            "loss: 1.378346  [   51/ 3200]\n",
            "loss: 1.344759  [   52/ 3200]\n",
            "loss: 1.348093  [   53/ 3200]\n",
            "loss: 1.399908  [   54/ 3200]\n",
            "loss: 1.366664  [   55/ 3200]\n",
            "loss: 1.348310  [   56/ 3200]\n",
            "loss: 1.341689  [   57/ 3200]\n",
            "loss: 1.361328  [   58/ 3200]\n",
            "loss: 1.322827  [   59/ 3200]\n",
            "loss: 1.375397  [   60/ 3200]\n",
            "loss: 1.329683  [   61/ 3200]\n",
            "loss: 1.368930  [   62/ 3200]\n",
            "loss: 1.343038  [   63/ 3200]\n",
            "loss: 1.340142  [   64/ 3200]\n",
            "loss: 1.362210  [   65/ 3200]\n",
            "loss: 1.341531  [   66/ 3200]\n",
            "loss: 1.330705  [   67/ 3200]\n",
            "loss: 1.355264  [   68/ 3200]\n",
            "loss: 1.368541  [   69/ 3200]\n",
            "loss: 1.335576  [   70/ 3200]\n",
            "loss: 1.344752  [   71/ 3200]\n",
            "loss: 1.363896  [   72/ 3200]\n",
            "loss: 1.347096  [   73/ 3200]\n",
            "loss: 1.355318  [   74/ 3200]\n",
            "loss: 1.337571  [   75/ 3200]\n",
            "loss: 1.399776  [   76/ 3200]\n",
            "loss: 1.346776  [   77/ 3200]\n",
            "loss: 1.368193  [   78/ 3200]\n",
            "loss: 1.354408  [   79/ 3200]\n",
            "loss: 1.353406  [   80/ 3200]\n",
            "loss: 1.353636  [   81/ 3200]\n",
            "loss: 1.319013  [   82/ 3200]\n",
            "loss: 1.358499  [   83/ 3200]\n",
            "loss: 1.376026  [   84/ 3200]\n",
            "loss: 1.338905  [   85/ 3200]\n",
            "loss: 1.333117  [   86/ 3200]\n",
            "loss: 1.352733  [   87/ 3200]\n",
            "loss: 1.328166  [   88/ 3200]\n",
            "loss: 1.368909  [   89/ 3200]\n",
            "loss: 1.341607  [   90/ 3200]\n",
            "loss: 1.349709  [   91/ 3200]\n",
            "loss: 1.337476  [   92/ 3200]\n",
            "loss: 1.324036  [   93/ 3200]\n",
            "loss: 1.404026  [   94/ 3200]\n",
            "loss: 1.345842  [   95/ 3200]\n",
            "loss: 1.319527  [   96/ 3200]\n",
            "loss: 1.342626  [   97/ 3200]\n",
            "loss: 1.322210  [   98/ 3200]\n",
            "loss: 1.319055  [   99/ 3200]\n",
            "loss: 1.350920  [  100/ 3200]\n",
            "loss: 1.326840  [  101/ 3200]\n",
            "loss: 1.316638  [  102/ 3200]\n",
            "loss: 1.410231  [  103/ 3200]\n",
            "loss: 1.349534  [  104/ 3200]\n",
            "loss: 1.339658  [  105/ 3200]\n",
            "loss: 1.338890  [  106/ 3200]\n",
            "loss: 1.376580  [  107/ 3200]\n",
            "loss: 1.348453  [  108/ 3200]\n",
            "loss: 1.345927  [  109/ 3200]\n",
            "loss: 1.369194  [  110/ 3200]\n",
            "loss: 1.352641  [  111/ 3200]\n",
            "loss: 1.379605  [  112/ 3200]\n",
            "loss: 1.352682  [  113/ 3200]\n",
            "loss: 1.324792  [  114/ 3200]\n",
            "loss: 1.350725  [  115/ 3200]\n",
            "loss: 1.339569  [  116/ 3200]\n",
            "loss: 1.313605  [  117/ 3200]\n",
            "loss: 1.401608  [  118/ 3200]\n",
            "loss: 1.340025  [  119/ 3200]\n",
            "loss: 1.340599  [  120/ 3200]\n",
            "loss: 1.339389  [  121/ 3200]\n",
            "loss: 1.357232  [  122/ 3200]\n",
            "loss: 1.350430  [  123/ 3200]\n",
            "loss: 1.368182  [  124/ 3200]\n",
            "loss: 1.333589  [  125/ 3200]\n",
            "loss: 1.356626  [  126/ 3200]\n",
            "loss: 1.313838  [  127/ 3200]\n",
            "loss: 1.362876  [  128/ 3200]\n",
            "loss: 1.361505  [  129/ 3200]\n",
            "loss: 1.349753  [  130/ 3200]\n",
            "loss: 1.335193  [  131/ 3200]\n",
            "loss: 1.359541  [  132/ 3200]\n",
            "loss: 1.288727  [  133/ 3200]\n",
            "loss: 1.313317  [  134/ 3200]\n",
            "loss: 1.363370  [  135/ 3200]\n",
            "loss: 1.374691  [  136/ 3200]\n",
            "loss: 1.360257  [  137/ 3200]\n",
            "loss: 1.394099  [  138/ 3200]\n",
            "loss: 1.347659  [  139/ 3200]\n",
            "loss: 1.351390  [  140/ 3200]\n",
            "loss: 1.341931  [  141/ 3200]\n",
            "loss: 1.347116  [  142/ 3200]\n",
            "loss: 1.341093  [  143/ 3200]\n",
            "loss: 1.355577  [  144/ 3200]\n",
            "loss: 1.343419  [  145/ 3200]\n",
            "loss: 1.350365  [  146/ 3200]\n",
            "loss: 1.273319  [  147/ 3200]\n",
            "loss: 1.337654  [  148/ 3200]\n",
            "loss: 1.358216  [  149/ 3200]\n",
            "loss: 1.310177  [  150/ 3200]\n",
            "loss: 1.389830  [  151/ 3200]\n",
            "loss: 1.347247  [  152/ 3200]\n",
            "loss: 1.413941  [  153/ 3200]\n",
            "loss: 1.332545  [  154/ 3200]\n",
            "loss: 1.363532  [  155/ 3200]\n",
            "loss: 1.368532  [  156/ 3200]\n",
            "loss: 1.362589  [  157/ 3200]\n",
            "loss: 1.353156  [  158/ 3200]\n",
            "loss: 1.366183  [  159/ 3200]\n",
            "loss: 1.341091  [  160/ 3200]\n",
            "loss: 1.369025  [  161/ 3200]\n",
            "loss: 1.350028  [  162/ 3200]\n",
            "loss: 1.387771  [  163/ 3200]\n",
            "loss: 1.357805  [  164/ 3200]\n",
            "loss: 1.343760  [  165/ 3200]\n",
            "loss: 1.342845  [  166/ 3200]\n",
            "loss: 1.354759  [  167/ 3200]\n",
            "loss: 1.357190  [  168/ 3200]\n",
            "loss: 1.318523  [  169/ 3200]\n",
            "loss: 1.355876  [  170/ 3200]\n",
            "loss: 1.344162  [  171/ 3200]\n",
            "loss: 1.353922  [  172/ 3200]\n",
            "loss: 1.374162  [  173/ 3200]\n",
            "loss: 1.358391  [  174/ 3200]\n",
            "loss: 1.353716  [  175/ 3200]\n",
            "loss: 1.359597  [  176/ 3200]\n",
            "loss: 1.347876  [  177/ 3200]\n",
            "loss: 1.356279  [  178/ 3200]\n",
            "loss: 1.357159  [  179/ 3200]\n",
            "loss: 1.367701  [  180/ 3200]\n",
            "loss: 1.332450  [  181/ 3200]\n",
            "loss: 1.348624  [  182/ 3200]\n",
            "loss: 1.333783  [  183/ 3200]\n",
            "loss: 1.314281  [  184/ 3200]\n",
            "loss: 1.324584  [  185/ 3200]\n",
            "loss: 1.330116  [  186/ 3200]\n",
            "loss: 1.344082  [  187/ 3200]\n",
            "loss: 1.365318  [  188/ 3200]\n",
            "loss: 1.350782  [  189/ 3200]\n",
            "loss: 1.314947  [  190/ 3200]\n",
            "loss: 1.275918  [  191/ 3200]\n",
            "loss: 1.287071  [  192/ 3200]\n",
            "loss: 1.422164  [  193/ 3200]\n",
            "loss: 1.388675  [  194/ 3200]\n",
            "loss: 1.324187  [  195/ 3200]\n",
            "loss: 1.353185  [  196/ 3200]\n",
            "loss: 1.344997  [  197/ 3200]\n",
            "loss: 1.329794  [  198/ 3200]\n",
            "loss: 1.357583  [  199/ 3200]\n",
            "Epoch:  6\n",
            "loss: 1.287512  [    0/ 3200]\n",
            "loss: 1.299606  [    1/ 3200]\n",
            "loss: 1.351285  [    2/ 3200]\n",
            "loss: 1.325807  [    3/ 3200]\n",
            "loss: 1.340693  [    4/ 3200]\n",
            "loss: 1.295454  [    5/ 3200]\n",
            "loss: 1.415898  [    6/ 3200]\n",
            "loss: 1.323549  [    7/ 3200]\n",
            "loss: 1.350196  [    8/ 3200]\n",
            "loss: 1.419260  [    9/ 3200]\n",
            "loss: 1.343918  [   10/ 3200]\n",
            "loss: 1.338582  [   11/ 3200]\n",
            "loss: 1.353314  [   12/ 3200]\n",
            "loss: 1.389730  [   13/ 3200]\n",
            "loss: 1.344218  [   14/ 3200]\n",
            "loss: 1.355433  [   15/ 3200]\n",
            "loss: 1.377645  [   16/ 3200]\n",
            "loss: 1.346970  [   17/ 3200]\n",
            "loss: 1.362644  [   18/ 3200]\n",
            "loss: 1.330137  [   19/ 3200]\n",
            "loss: 1.363809  [   20/ 3200]\n",
            "loss: 1.315559  [   21/ 3200]\n",
            "loss: 1.336202  [   22/ 3200]\n",
            "loss: 1.350600  [   23/ 3200]\n",
            "loss: 1.391630  [   24/ 3200]\n",
            "loss: 1.356755  [   25/ 3200]\n",
            "loss: 1.340842  [   26/ 3200]\n",
            "loss: 1.331497  [   27/ 3200]\n",
            "loss: 1.347421  [   28/ 3200]\n",
            "loss: 1.331976  [   29/ 3200]\n",
            "loss: 1.346793  [   30/ 3200]\n",
            "loss: 1.341205  [   31/ 3200]\n",
            "loss: 1.343514  [   32/ 3200]\n",
            "loss: 1.354046  [   33/ 3200]\n",
            "loss: 1.323661  [   34/ 3200]\n",
            "loss: 1.346035  [   35/ 3200]\n",
            "loss: 1.376004  [   36/ 3200]\n",
            "loss: 1.372136  [   37/ 3200]\n",
            "loss: 1.336699  [   38/ 3200]\n",
            "loss: 1.348406  [   39/ 3200]\n",
            "loss: 1.338567  [   40/ 3200]\n",
            "loss: 1.339570  [   41/ 3200]\n",
            "loss: 1.335036  [   42/ 3200]\n",
            "loss: 1.371304  [   43/ 3200]\n",
            "loss: 1.340278  [   44/ 3200]\n",
            "loss: 1.345640  [   45/ 3200]\n",
            "loss: 1.380771  [   46/ 3200]\n",
            "loss: 1.316020  [   47/ 3200]\n",
            "loss: 1.314148  [   48/ 3200]\n",
            "loss: 1.364061  [   49/ 3200]\n",
            "loss: 1.334689  [   50/ 3200]\n",
            "loss: 1.325426  [   51/ 3200]\n",
            "loss: 1.302806  [   52/ 3200]\n",
            "loss: 1.328722  [   53/ 3200]\n",
            "loss: 1.343831  [   54/ 3200]\n",
            "loss: 1.347468  [   55/ 3200]\n",
            "loss: 1.350011  [   56/ 3200]\n",
            "loss: 1.374595  [   57/ 3200]\n",
            "loss: 1.360259  [   58/ 3200]\n",
            "loss: 1.312736  [   59/ 3200]\n",
            "loss: 1.359944  [   60/ 3200]\n",
            "loss: 1.357072  [   61/ 3200]\n",
            "loss: 1.305039  [   62/ 3200]\n",
            "loss: 1.347537  [   63/ 3200]\n",
            "loss: 1.410904  [   64/ 3200]\n",
            "loss: 1.324828  [   65/ 3200]\n",
            "loss: 1.354748  [   66/ 3200]\n",
            "loss: 1.368125  [   67/ 3200]\n",
            "loss: 1.319157  [   68/ 3200]\n",
            "loss: 1.369307  [   69/ 3200]\n",
            "loss: 1.324756  [   70/ 3200]\n",
            "loss: 1.382515  [   71/ 3200]\n",
            "loss: 1.352824  [   72/ 3200]\n",
            "loss: 1.339394  [   73/ 3200]\n",
            "loss: 1.370161  [   74/ 3200]\n",
            "loss: 1.330650  [   75/ 3200]\n",
            "loss: 1.311118  [   76/ 3200]\n",
            "loss: 1.339550  [   77/ 3200]\n",
            "loss: 1.340597  [   78/ 3200]\n",
            "loss: 1.310686  [   79/ 3200]\n",
            "loss: 1.385657  [   80/ 3200]\n",
            "loss: 1.378448  [   81/ 3200]\n",
            "loss: 1.343982  [   82/ 3200]\n",
            "loss: 1.324186  [   83/ 3200]\n",
            "loss: 1.346228  [   84/ 3200]\n",
            "loss: 1.357532  [   85/ 3200]\n",
            "loss: 1.346893  [   86/ 3200]\n",
            "loss: 1.364955  [   87/ 3200]\n",
            "loss: 1.315255  [   88/ 3200]\n",
            "loss: 1.336161  [   89/ 3200]\n",
            "loss: 1.343069  [   90/ 3200]\n",
            "loss: 1.336683  [   91/ 3200]\n",
            "loss: 1.358140  [   92/ 3200]\n",
            "loss: 1.364878  [   93/ 3200]\n",
            "loss: 1.294990  [   94/ 3200]\n",
            "loss: 1.357994  [   95/ 3200]\n",
            "loss: 1.343649  [   96/ 3200]\n",
            "loss: 1.347645  [   97/ 3200]\n",
            "loss: 1.356368  [   98/ 3200]\n",
            "loss: 1.366760  [   99/ 3200]\n",
            "loss: 1.390348  [  100/ 3200]\n",
            "loss: 1.321606  [  101/ 3200]\n",
            "loss: 1.341260  [  102/ 3200]\n",
            "loss: 1.332646  [  103/ 3200]\n",
            "loss: 1.347918  [  104/ 3200]\n",
            "loss: 1.332009  [  105/ 3200]\n",
            "loss: 1.342173  [  106/ 3200]\n",
            "loss: 1.288207  [  107/ 3200]\n",
            "loss: 1.325819  [  108/ 3200]\n",
            "loss: 1.305584  [  109/ 3200]\n",
            "loss: 1.316905  [  110/ 3200]\n",
            "loss: 1.344879  [  111/ 3200]\n",
            "loss: 1.333097  [  112/ 3200]\n",
            "loss: 1.376295  [  113/ 3200]\n",
            "loss: 1.341728  [  114/ 3200]\n",
            "loss: 1.318203  [  115/ 3200]\n",
            "loss: 1.394874  [  116/ 3200]\n",
            "loss: 1.364152  [  117/ 3200]\n",
            "loss: 1.329079  [  118/ 3200]\n",
            "loss: 1.293783  [  119/ 3200]\n",
            "loss: 1.326701  [  120/ 3200]\n",
            "loss: 1.330112  [  121/ 3200]\n",
            "loss: 1.355489  [  122/ 3200]\n",
            "loss: 1.347689  [  123/ 3200]\n",
            "loss: 1.303079  [  124/ 3200]\n",
            "loss: 1.331322  [  125/ 3200]\n",
            "loss: 1.373392  [  126/ 3200]\n",
            "loss: 1.323192  [  127/ 3200]\n",
            "loss: 1.345723  [  128/ 3200]\n",
            "loss: 1.356014  [  129/ 3200]\n",
            "loss: 1.340758  [  130/ 3200]\n",
            "loss: 1.331977  [  131/ 3200]\n",
            "loss: 1.319117  [  132/ 3200]\n",
            "loss: 1.279068  [  133/ 3200]\n",
            "loss: 1.404618  [  134/ 3200]\n",
            "loss: 1.277237  [  135/ 3200]\n",
            "loss: 1.310770  [  136/ 3200]\n",
            "loss: 1.346520  [  137/ 3200]\n",
            "loss: 1.364759  [  138/ 3200]\n",
            "loss: 1.291777  [  139/ 3200]\n",
            "loss: 1.344904  [  140/ 3200]\n",
            "loss: 1.332807  [  141/ 3200]\n",
            "loss: 1.334752  [  142/ 3200]\n",
            "loss: 1.289276  [  143/ 3200]\n",
            "loss: 1.340219  [  144/ 3200]\n",
            "loss: 1.348770  [  145/ 3200]\n",
            "loss: 1.355263  [  146/ 3200]\n",
            "loss: 1.347857  [  147/ 3200]\n",
            "loss: 1.327474  [  148/ 3200]\n",
            "loss: 1.315988  [  149/ 3200]\n",
            "loss: 1.295563  [  150/ 3200]\n",
            "loss: 1.321875  [  151/ 3200]\n",
            "loss: 1.355636  [  152/ 3200]\n",
            "loss: 1.379165  [  153/ 3200]\n",
            "loss: 1.307277  [  154/ 3200]\n",
            "loss: 1.338605  [  155/ 3200]\n",
            "loss: 1.342850  [  156/ 3200]\n",
            "loss: 1.365710  [  157/ 3200]\n",
            "loss: 1.299573  [  158/ 3200]\n",
            "loss: 1.348399  [  159/ 3200]\n",
            "loss: 1.360507  [  160/ 3200]\n",
            "loss: 1.371056  [  161/ 3200]\n",
            "loss: 1.368726  [  162/ 3200]\n",
            "loss: 1.345336  [  163/ 3200]\n",
            "loss: 1.370795  [  164/ 3200]\n",
            "loss: 1.342596  [  165/ 3200]\n",
            "loss: 1.348648  [  166/ 3200]\n",
            "loss: 1.347613  [  167/ 3200]\n",
            "loss: 1.318622  [  168/ 3200]\n",
            "loss: 1.336502  [  169/ 3200]\n",
            "loss: 1.331392  [  170/ 3200]\n",
            "loss: 1.313228  [  171/ 3200]\n",
            "loss: 1.328395  [  172/ 3200]\n",
            "loss: 1.325868  [  173/ 3200]\n",
            "loss: 1.307292  [  174/ 3200]\n",
            "loss: 1.360833  [  175/ 3200]\n",
            "loss: 1.354564  [  176/ 3200]\n",
            "loss: 1.336585  [  177/ 3200]\n",
            "loss: 1.328146  [  178/ 3200]\n",
            "loss: 1.333705  [  179/ 3200]\n",
            "loss: 1.342175  [  180/ 3200]\n",
            "loss: 1.340657  [  181/ 3200]\n",
            "loss: 1.334617  [  182/ 3200]\n",
            "loss: 1.368068  [  183/ 3200]\n",
            "loss: 1.340645  [  184/ 3200]\n",
            "loss: 1.344536  [  185/ 3200]\n",
            "loss: 1.338428  [  186/ 3200]\n",
            "loss: 1.340146  [  187/ 3200]\n",
            "loss: 1.344526  [  188/ 3200]\n",
            "loss: 1.343623  [  189/ 3200]\n",
            "loss: 1.327116  [  190/ 3200]\n",
            "loss: 1.330296  [  191/ 3200]\n",
            "loss: 1.354308  [  192/ 3200]\n",
            "loss: 1.339675  [  193/ 3200]\n",
            "loss: 1.345473  [  194/ 3200]\n",
            "loss: 1.363314  [  195/ 3200]\n",
            "loss: 1.327909  [  196/ 3200]\n",
            "loss: 1.337664  [  197/ 3200]\n",
            "loss: 1.354391  [  198/ 3200]\n",
            "loss: 1.342921  [  199/ 3200]\n",
            "Epoch:  7\n",
            "loss: 1.319819  [    0/ 3200]\n",
            "loss: 1.326012  [    1/ 3200]\n",
            "loss: 1.324824  [    2/ 3200]\n",
            "loss: 1.349481  [    3/ 3200]\n",
            "loss: 1.316820  [    4/ 3200]\n",
            "loss: 1.410053  [    5/ 3200]\n",
            "loss: 1.343565  [    6/ 3200]\n",
            "loss: 1.322163  [    7/ 3200]\n",
            "loss: 1.347742  [    8/ 3200]\n",
            "loss: 1.351610  [    9/ 3200]\n",
            "loss: 1.317480  [   10/ 3200]\n",
            "loss: 1.365484  [   11/ 3200]\n",
            "loss: 1.313245  [   12/ 3200]\n",
            "loss: 1.349510  [   13/ 3200]\n",
            "loss: 1.324375  [   14/ 3200]\n",
            "loss: 1.306753  [   15/ 3200]\n",
            "loss: 1.308625  [   16/ 3200]\n",
            "loss: 1.370301  [   17/ 3200]\n",
            "loss: 1.318244  [   18/ 3200]\n",
            "loss: 1.314445  [   19/ 3200]\n",
            "loss: 1.371152  [   20/ 3200]\n",
            "loss: 1.337060  [   21/ 3200]\n",
            "loss: 1.370911  [   22/ 3200]\n",
            "loss: 1.312846  [   23/ 3200]\n",
            "loss: 1.369539  [   24/ 3200]\n",
            "loss: 1.364490  [   25/ 3200]\n",
            "loss: 1.329301  [   26/ 3200]\n",
            "loss: 1.347721  [   27/ 3200]\n",
            "loss: 1.347862  [   28/ 3200]\n",
            "loss: 1.332169  [   29/ 3200]\n",
            "loss: 1.322369  [   30/ 3200]\n",
            "loss: 1.347564  [   31/ 3200]\n",
            "loss: 1.351992  [   32/ 3200]\n",
            "loss: 1.333631  [   33/ 3200]\n",
            "loss: 1.338877  [   34/ 3200]\n",
            "loss: 1.321468  [   35/ 3200]\n",
            "loss: 1.310311  [   36/ 3200]\n",
            "loss: 1.310697  [   37/ 3200]\n",
            "loss: 1.289081  [   38/ 3200]\n",
            "loss: 1.413256  [   39/ 3200]\n",
            "loss: 1.327756  [   40/ 3200]\n",
            "loss: 1.282048  [   41/ 3200]\n",
            "loss: 1.395924  [   42/ 3200]\n",
            "loss: 1.335163  [   43/ 3200]\n",
            "loss: 1.318622  [   44/ 3200]\n",
            "loss: 1.310234  [   45/ 3200]\n",
            "loss: 1.324698  [   46/ 3200]\n",
            "loss: 1.375962  [   47/ 3200]\n",
            "loss: 1.315228  [   48/ 3200]\n",
            "loss: 1.280274  [   49/ 3200]\n",
            "loss: 1.372974  [   50/ 3200]\n",
            "loss: 1.394776  [   51/ 3200]\n",
            "loss: 1.312834  [   52/ 3200]\n",
            "loss: 1.321882  [   53/ 3200]\n",
            "loss: 1.364659  [   54/ 3200]\n",
            "loss: 1.327119  [   55/ 3200]\n",
            "loss: 1.343365  [   56/ 3200]\n",
            "loss: 1.316556  [   57/ 3200]\n",
            "loss: 1.342999  [   58/ 3200]\n",
            "loss: 1.304582  [   59/ 3200]\n",
            "loss: 1.376400  [   60/ 3200]\n",
            "loss: 1.326293  [   61/ 3200]\n",
            "loss: 1.327779  [   62/ 3200]\n",
            "loss: 1.332044  [   63/ 3200]\n",
            "loss: 1.342767  [   64/ 3200]\n",
            "loss: 1.326528  [   65/ 3200]\n",
            "loss: 1.351510  [   66/ 3200]\n",
            "loss: 1.322206  [   67/ 3200]\n",
            "loss: 1.352937  [   68/ 3200]\n",
            "loss: 1.309906  [   69/ 3200]\n",
            "loss: 1.331120  [   70/ 3200]\n",
            "loss: 1.320645  [   71/ 3200]\n",
            "loss: 1.292956  [   72/ 3200]\n",
            "loss: 1.369207  [   73/ 3200]\n",
            "loss: 1.328833  [   74/ 3200]\n",
            "loss: 1.368865  [   75/ 3200]\n",
            "loss: 1.228896  [   76/ 3200]\n",
            "loss: 1.300700  [   77/ 3200]\n",
            "loss: 1.259943  [   78/ 3200]\n",
            "loss: 1.398564  [   79/ 3200]\n",
            "loss: 1.359962  [   80/ 3200]\n",
            "loss: 1.324847  [   81/ 3200]\n",
            "loss: 1.350443  [   82/ 3200]\n",
            "loss: 1.308009  [   83/ 3200]\n",
            "loss: 1.362480  [   84/ 3200]\n",
            "loss: 1.311299  [   85/ 3200]\n",
            "loss: 1.352437  [   86/ 3200]\n",
            "loss: 1.375011  [   87/ 3200]\n",
            "loss: 1.297260  [   88/ 3200]\n",
            "loss: 1.373709  [   89/ 3200]\n",
            "loss: 1.338732  [   90/ 3200]\n",
            "loss: 1.335853  [   91/ 3200]\n",
            "loss: 1.346059  [   92/ 3200]\n",
            "loss: 1.286688  [   93/ 3200]\n",
            "loss: 1.366285  [   94/ 3200]\n",
            "loss: 1.297697  [   95/ 3200]\n",
            "loss: 1.311229  [   96/ 3200]\n",
            "loss: 1.307333  [   97/ 3200]\n",
            "loss: 1.402857  [   98/ 3200]\n",
            "loss: 1.336012  [   99/ 3200]\n",
            "loss: 1.316274  [  100/ 3200]\n",
            "loss: 1.322008  [  101/ 3200]\n",
            "loss: 1.345708  [  102/ 3200]\n",
            "loss: 1.386786  [  103/ 3200]\n",
            "loss: 1.331015  [  104/ 3200]\n",
            "loss: 1.296998  [  105/ 3200]\n",
            "loss: 1.319848  [  106/ 3200]\n",
            "loss: 1.321456  [  107/ 3200]\n",
            "loss: 1.362586  [  108/ 3200]\n",
            "loss: 1.330225  [  109/ 3200]\n",
            "loss: 1.333058  [  110/ 3200]\n",
            "loss: 1.347162  [  111/ 3200]\n",
            "loss: 1.339559  [  112/ 3200]\n",
            "loss: 1.292619  [  113/ 3200]\n",
            "loss: 1.292179  [  114/ 3200]\n",
            "loss: 1.313983  [  115/ 3200]\n",
            "loss: 1.323235  [  116/ 3200]\n",
            "loss: 1.340333  [  117/ 3200]\n",
            "loss: 1.335706  [  118/ 3200]\n",
            "loss: 1.360693  [  119/ 3200]\n",
            "loss: 1.304935  [  120/ 3200]\n",
            "loss: 1.327681  [  121/ 3200]\n",
            "loss: 1.321948  [  122/ 3200]\n",
            "loss: 1.315152  [  123/ 3200]\n",
            "loss: 1.351311  [  124/ 3200]\n",
            "loss: 1.336813  [  125/ 3200]\n",
            "loss: 1.335972  [  126/ 3200]\n",
            "loss: 1.348679  [  127/ 3200]\n",
            "loss: 1.356095  [  128/ 3200]\n",
            "loss: 1.343118  [  129/ 3200]\n",
            "loss: 1.324720  [  130/ 3200]\n",
            "loss: 1.345160  [  131/ 3200]\n",
            "loss: 1.349571  [  132/ 3200]\n",
            "loss: 1.333716  [  133/ 3200]\n",
            "loss: 1.311914  [  134/ 3200]\n",
            "loss: 1.278111  [  135/ 3200]\n",
            "loss: 1.347135  [  136/ 3200]\n",
            "loss: 1.348888  [  137/ 3200]\n",
            "loss: 1.303264  [  138/ 3200]\n",
            "loss: 1.357244  [  139/ 3200]\n",
            "loss: 1.332133  [  140/ 3200]\n",
            "loss: 1.273514  [  141/ 3200]\n",
            "loss: 1.356985  [  142/ 3200]\n",
            "loss: 1.333608  [  143/ 3200]\n",
            "loss: 1.346965  [  144/ 3200]\n",
            "loss: 1.299723  [  145/ 3200]\n",
            "loss: 1.343070  [  146/ 3200]\n",
            "loss: 1.318567  [  147/ 3200]\n",
            "loss: 1.337471  [  148/ 3200]\n",
            "loss: 1.327698  [  149/ 3200]\n",
            "loss: 1.289805  [  150/ 3200]\n",
            "loss: 1.322230  [  151/ 3200]\n",
            "loss: 1.306237  [  152/ 3200]\n",
            "loss: 1.319161  [  153/ 3200]\n",
            "loss: 1.349712  [  154/ 3200]\n",
            "loss: 1.370870  [  155/ 3200]\n",
            "loss: 1.307173  [  156/ 3200]\n",
            "loss: 1.278275  [  157/ 3200]\n",
            "loss: 1.275447  [  158/ 3200]\n",
            "loss: 1.287020  [  159/ 3200]\n",
            "loss: 1.306756  [  160/ 3200]\n",
            "loss: 1.360462  [  161/ 3200]\n",
            "loss: 1.330742  [  162/ 3200]\n",
            "loss: 1.271132  [  163/ 3200]\n",
            "loss: 1.313491  [  164/ 3200]\n",
            "loss: 1.348192  [  165/ 3200]\n",
            "loss: 1.310083  [  166/ 3200]\n",
            "loss: 1.306113  [  167/ 3200]\n",
            "loss: 1.341867  [  168/ 3200]\n",
            "loss: 1.345198  [  169/ 3200]\n",
            "loss: 1.351807  [  170/ 3200]\n",
            "loss: 1.314668  [  171/ 3200]\n",
            "loss: 1.251113  [  172/ 3200]\n",
            "loss: 1.349576  [  173/ 3200]\n",
            "loss: 1.288534  [  174/ 3200]\n",
            "loss: 1.293041  [  175/ 3200]\n",
            "loss: 1.348268  [  176/ 3200]\n",
            "loss: 1.340813  [  177/ 3200]\n",
            "loss: 1.282205  [  178/ 3200]\n",
            "loss: 1.272824  [  179/ 3200]\n",
            "loss: 1.291559  [  180/ 3200]\n",
            "loss: 1.363003  [  181/ 3200]\n",
            "loss: 1.304326  [  182/ 3200]\n",
            "loss: 1.309366  [  183/ 3200]\n",
            "loss: 1.308252  [  184/ 3200]\n",
            "loss: 1.351071  [  185/ 3200]\n",
            "loss: 1.292547  [  186/ 3200]\n",
            "loss: 1.338908  [  187/ 3200]\n",
            "loss: 1.346817  [  188/ 3200]\n",
            "loss: 1.329258  [  189/ 3200]\n",
            "loss: 1.364386  [  190/ 3200]\n",
            "loss: 1.330288  [  191/ 3200]\n",
            "loss: 1.327644  [  192/ 3200]\n",
            "loss: 1.343013  [  193/ 3200]\n",
            "loss: 1.330596  [  194/ 3200]\n",
            "loss: 1.328922  [  195/ 3200]\n",
            "loss: 1.342147  [  196/ 3200]\n",
            "loss: 1.355553  [  197/ 3200]\n",
            "loss: 1.318478  [  198/ 3200]\n",
            "loss: 1.340902  [  199/ 3200]\n",
            "Epoch:  8\n",
            "loss: 1.339042  [    0/ 3200]\n",
            "loss: 1.323978  [    1/ 3200]\n",
            "loss: 1.321800  [    2/ 3200]\n",
            "loss: 1.311447  [    3/ 3200]\n",
            "loss: 1.329062  [    4/ 3200]\n",
            "loss: 1.301527  [    5/ 3200]\n",
            "loss: 1.399007  [    6/ 3200]\n",
            "loss: 1.363567  [    7/ 3200]\n",
            "loss: 1.327453  [    8/ 3200]\n",
            "loss: 1.346341  [    9/ 3200]\n",
            "loss: 1.318966  [   10/ 3200]\n",
            "loss: 1.319882  [   11/ 3200]\n",
            "loss: 1.302717  [   12/ 3200]\n",
            "loss: 1.336105  [   13/ 3200]\n",
            "loss: 1.338420  [   14/ 3200]\n",
            "loss: 1.302667  [   15/ 3200]\n",
            "loss: 1.299372  [   16/ 3200]\n",
            "loss: 1.342928  [   17/ 3200]\n",
            "loss: 1.330020  [   18/ 3200]\n",
            "loss: 1.318182  [   19/ 3200]\n",
            "loss: 1.332832  [   20/ 3200]\n",
            "loss: 1.286322  [   21/ 3200]\n",
            "loss: 1.312365  [   22/ 3200]\n",
            "loss: 1.302267  [   23/ 3200]\n",
            "loss: 1.328164  [   24/ 3200]\n",
            "loss: 1.329820  [   25/ 3200]\n",
            "loss: 1.330720  [   26/ 3200]\n",
            "loss: 1.302356  [   27/ 3200]\n",
            "loss: 1.314248  [   28/ 3200]\n",
            "loss: 1.305783  [   29/ 3200]\n",
            "loss: 1.359615  [   30/ 3200]\n",
            "loss: 1.297875  [   31/ 3200]\n",
            "loss: 1.310047  [   32/ 3200]\n",
            "loss: 1.363555  [   33/ 3200]\n",
            "loss: 1.357816  [   34/ 3200]\n",
            "loss: 1.303059  [   35/ 3200]\n",
            "loss: 1.316995  [   36/ 3200]\n",
            "loss: 1.330755  [   37/ 3200]\n",
            "loss: 1.327246  [   38/ 3200]\n",
            "loss: 1.339834  [   39/ 3200]\n",
            "loss: 1.276459  [   40/ 3200]\n",
            "loss: 1.329712  [   41/ 3200]\n",
            "loss: 1.267284  [   42/ 3200]\n",
            "loss: 1.265325  [   43/ 3200]\n",
            "loss: 1.367889  [   44/ 3200]\n",
            "loss: 1.326659  [   45/ 3200]\n",
            "loss: 1.359716  [   46/ 3200]\n",
            "loss: 1.322998  [   47/ 3200]\n",
            "loss: 1.323020  [   48/ 3200]\n",
            "loss: 1.335169  [   49/ 3200]\n",
            "loss: 1.328140  [   50/ 3200]\n",
            "loss: 1.294364  [   51/ 3200]\n",
            "loss: 1.332816  [   52/ 3200]\n",
            "loss: 1.288108  [   53/ 3200]\n",
            "loss: 1.315949  [   54/ 3200]\n",
            "loss: 1.263610  [   55/ 3200]\n",
            "loss: 1.259893  [   56/ 3200]\n",
            "loss: 1.390623  [   57/ 3200]\n",
            "loss: 1.374101  [   58/ 3200]\n",
            "loss: 1.285901  [   59/ 3200]\n",
            "loss: 1.384262  [   60/ 3200]\n",
            "loss: 1.377589  [   61/ 3200]\n",
            "loss: 1.275290  [   62/ 3200]\n",
            "loss: 1.299418  [   63/ 3200]\n",
            "loss: 1.307994  [   64/ 3200]\n",
            "loss: 1.357618  [   65/ 3200]\n",
            "loss: 1.277757  [   66/ 3200]\n",
            "loss: 1.264445  [   67/ 3200]\n",
            "loss: 1.256497  [   68/ 3200]\n",
            "loss: 1.246419  [   69/ 3200]\n",
            "loss: 1.400381  [   70/ 3200]\n",
            "loss: 1.294642  [   71/ 3200]\n",
            "loss: 1.334995  [   72/ 3200]\n",
            "loss: 1.317262  [   73/ 3200]\n",
            "loss: 1.355627  [   74/ 3200]\n",
            "loss: 1.344477  [   75/ 3200]\n",
            "loss: 1.300149  [   76/ 3200]\n",
            "loss: 1.315544  [   77/ 3200]\n",
            "loss: 1.302778  [   78/ 3200]\n",
            "loss: 1.371768  [   79/ 3200]\n",
            "loss: 1.306078  [   80/ 3200]\n",
            "loss: 1.274252  [   81/ 3200]\n",
            "loss: 1.341343  [   82/ 3200]\n",
            "loss: 1.326747  [   83/ 3200]\n",
            "loss: 1.288961  [   84/ 3200]\n",
            "loss: 1.330140  [   85/ 3200]\n",
            "loss: 1.344017  [   86/ 3200]\n",
            "loss: 1.315874  [   87/ 3200]\n",
            "loss: 1.353774  [   88/ 3200]\n",
            "loss: 1.314074  [   89/ 3200]\n",
            "loss: 1.347333  [   90/ 3200]\n",
            "loss: 1.302886  [   91/ 3200]\n",
            "loss: 1.320786  [   92/ 3200]\n",
            "loss: 1.301881  [   93/ 3200]\n",
            "loss: 1.330523  [   94/ 3200]\n",
            "loss: 1.351084  [   95/ 3200]\n",
            "loss: 1.319129  [   96/ 3200]\n",
            "loss: 1.320485  [   97/ 3200]\n",
            "loss: 1.294872  [   98/ 3200]\n",
            "loss: 1.289095  [   99/ 3200]\n",
            "loss: 1.335870  [  100/ 3200]\n",
            "loss: 1.334654  [  101/ 3200]\n",
            "loss: 1.288271  [  102/ 3200]\n",
            "loss: 1.295259  [  103/ 3200]\n",
            "loss: 1.285401  [  104/ 3200]\n",
            "loss: 1.308188  [  105/ 3200]\n",
            "loss: 1.322425  [  106/ 3200]\n",
            "loss: 1.321267  [  107/ 3200]\n",
            "loss: 1.327620  [  108/ 3200]\n",
            "loss: 1.311411  [  109/ 3200]\n",
            "loss: 1.336001  [  110/ 3200]\n",
            "loss: 1.289629  [  111/ 3200]\n",
            "loss: 1.328733  [  112/ 3200]\n",
            "loss: 1.396015  [  113/ 3200]\n",
            "loss: 1.292942  [  114/ 3200]\n",
            "loss: 1.320776  [  115/ 3200]\n",
            "loss: 1.334295  [  116/ 3200]\n",
            "loss: 1.317013  [  117/ 3200]\n",
            "loss: 1.307205  [  118/ 3200]\n",
            "loss: 1.313799  [  119/ 3200]\n",
            "loss: 1.301979  [  120/ 3200]\n",
            "loss: 1.302650  [  121/ 3200]\n",
            "loss: 1.261483  [  122/ 3200]\n",
            "loss: 1.327376  [  123/ 3200]\n",
            "loss: 1.353797  [  124/ 3200]\n",
            "loss: 1.305215  [  125/ 3200]\n",
            "loss: 1.284887  [  126/ 3200]\n",
            "loss: 1.331666  [  127/ 3200]\n",
            "loss: 1.335395  [  128/ 3200]\n",
            "loss: 1.277528  [  129/ 3200]\n",
            "loss: 1.299912  [  130/ 3200]\n",
            "loss: 1.341614  [  131/ 3200]\n",
            "loss: 1.352591  [  132/ 3200]\n",
            "loss: 1.350742  [  133/ 3200]\n",
            "loss: 1.351062  [  134/ 3200]\n",
            "loss: 1.345469  [  135/ 3200]\n",
            "loss: 1.300712  [  136/ 3200]\n",
            "loss: 1.334716  [  137/ 3200]\n",
            "loss: 1.359613  [  138/ 3200]\n",
            "loss: 1.313035  [  139/ 3200]\n",
            "loss: 1.318388  [  140/ 3200]\n",
            "loss: 1.304170  [  141/ 3200]\n",
            "loss: 1.322929  [  142/ 3200]\n",
            "loss: 1.292099  [  143/ 3200]\n",
            "loss: 1.269970  [  144/ 3200]\n",
            "loss: 1.297145  [  145/ 3200]\n",
            "loss: 1.290425  [  146/ 3200]\n",
            "loss: 1.343105  [  147/ 3200]\n",
            "loss: 1.306884  [  148/ 3200]\n",
            "loss: 1.378746  [  149/ 3200]\n",
            "loss: 1.277622  [  150/ 3200]\n",
            "loss: 1.316576  [  151/ 3200]\n",
            "loss: 1.305029  [  152/ 3200]\n",
            "loss: 1.316894  [  153/ 3200]\n",
            "loss: 1.281844  [  154/ 3200]\n",
            "loss: 1.269778  [  155/ 3200]\n",
            "loss: 1.294389  [  156/ 3200]\n",
            "loss: 1.260059  [  157/ 3200]\n",
            "loss: 1.325523  [  158/ 3200]\n",
            "loss: 1.290187  [  159/ 3200]\n",
            "loss: 1.310588  [  160/ 3200]\n",
            "loss: 1.342251  [  161/ 3200]\n",
            "loss: 1.410689  [  162/ 3200]\n",
            "loss: 1.312882  [  163/ 3200]\n",
            "loss: 1.235655  [  164/ 3200]\n",
            "loss: 1.304856  [  165/ 3200]\n",
            "loss: 1.338818  [  166/ 3200]\n",
            "loss: 1.335181  [  167/ 3200]\n",
            "loss: 1.291201  [  168/ 3200]\n",
            "loss: 1.288995  [  169/ 3200]\n",
            "loss: 1.307244  [  170/ 3200]\n",
            "loss: 1.287230  [  171/ 3200]\n",
            "loss: 1.338224  [  172/ 3200]\n",
            "loss: 1.343815  [  173/ 3200]\n",
            "loss: 1.342042  [  174/ 3200]\n",
            "loss: 1.279175  [  175/ 3200]\n",
            "loss: 1.300052  [  176/ 3200]\n",
            "loss: 1.295814  [  177/ 3200]\n",
            "loss: 1.345346  [  178/ 3200]\n",
            "loss: 1.284390  [  179/ 3200]\n",
            "loss: 1.338442  [  180/ 3200]\n",
            "loss: 1.336217  [  181/ 3200]\n",
            "loss: 1.314419  [  182/ 3200]\n",
            "loss: 1.317094  [  183/ 3200]\n",
            "loss: 1.252740  [  184/ 3200]\n",
            "loss: 1.311877  [  185/ 3200]\n",
            "loss: 1.263867  [  186/ 3200]\n",
            "loss: 1.399235  [  187/ 3200]\n",
            "loss: 1.287924  [  188/ 3200]\n",
            "loss: 1.289249  [  189/ 3200]\n",
            "loss: 1.313031  [  190/ 3200]\n",
            "loss: 1.308256  [  191/ 3200]\n",
            "loss: 1.337381  [  192/ 3200]\n",
            "loss: 1.346110  [  193/ 3200]\n",
            "loss: 1.305697  [  194/ 3200]\n",
            "loss: 1.303815  [  195/ 3200]\n",
            "loss: 1.330453  [  196/ 3200]\n",
            "loss: 1.267028  [  197/ 3200]\n",
            "loss: 1.303961  [  198/ 3200]\n",
            "loss: 1.279157  [  199/ 3200]\n",
            "Epoch:  9\n",
            "loss: 1.281325  [    0/ 3200]\n",
            "loss: 1.257542  [    1/ 3200]\n",
            "loss: 1.232267  [    2/ 3200]\n",
            "loss: 1.345526  [    3/ 3200]\n",
            "loss: 1.334511  [    4/ 3200]\n",
            "loss: 1.336871  [    5/ 3200]\n",
            "loss: 1.283020  [    6/ 3200]\n",
            "loss: 1.443832  [    7/ 3200]\n",
            "loss: 1.307394  [    8/ 3200]\n",
            "loss: 1.246932  [    9/ 3200]\n",
            "loss: 1.318708  [   10/ 3200]\n",
            "loss: 1.301594  [   11/ 3200]\n",
            "loss: 1.333393  [   12/ 3200]\n",
            "loss: 1.321127  [   13/ 3200]\n",
            "loss: 1.328971  [   14/ 3200]\n",
            "loss: 1.326505  [   15/ 3200]\n",
            "loss: 1.335041  [   16/ 3200]\n",
            "loss: 1.360979  [   17/ 3200]\n",
            "loss: 1.331871  [   18/ 3200]\n",
            "loss: 1.296290  [   19/ 3200]\n",
            "loss: 1.306592  [   20/ 3200]\n",
            "loss: 1.276754  [   21/ 3200]\n",
            "loss: 1.320402  [   22/ 3200]\n",
            "loss: 1.302534  [   23/ 3200]\n",
            "loss: 1.282841  [   24/ 3200]\n",
            "loss: 1.292301  [   25/ 3200]\n",
            "loss: 1.303604  [   26/ 3200]\n",
            "loss: 1.340628  [   27/ 3200]\n",
            "loss: 1.303414  [   28/ 3200]\n",
            "loss: 1.296676  [   29/ 3200]\n",
            "loss: 1.299185  [   30/ 3200]\n",
            "loss: 1.327575  [   31/ 3200]\n",
            "loss: 1.330736  [   32/ 3200]\n",
            "loss: 1.277456  [   33/ 3200]\n",
            "loss: 1.339061  [   34/ 3200]\n",
            "loss: 1.242338  [   35/ 3200]\n",
            "loss: 1.280537  [   36/ 3200]\n",
            "loss: 1.315005  [   37/ 3200]\n",
            "loss: 1.310491  [   38/ 3200]\n",
            "loss: 1.309270  [   39/ 3200]\n",
            "loss: 1.284019  [   40/ 3200]\n",
            "loss: 1.311774  [   41/ 3200]\n",
            "loss: 1.347969  [   42/ 3200]\n",
            "loss: 1.360915  [   43/ 3200]\n",
            "loss: 1.273247  [   44/ 3200]\n",
            "loss: 1.268103  [   45/ 3200]\n",
            "loss: 1.332397  [   46/ 3200]\n",
            "loss: 1.288993  [   47/ 3200]\n",
            "loss: 1.332327  [   48/ 3200]\n",
            "loss: 1.293797  [   49/ 3200]\n",
            "loss: 1.329428  [   50/ 3200]\n",
            "loss: 1.253400  [   51/ 3200]\n",
            "loss: 1.235155  [   52/ 3200]\n",
            "loss: 1.365749  [   53/ 3200]\n",
            "loss: 1.251774  [   54/ 3200]\n",
            "loss: 1.242422  [   55/ 3200]\n",
            "loss: 1.323118  [   56/ 3200]\n",
            "loss: 1.280699  [   57/ 3200]\n",
            "loss: 1.297688  [   58/ 3200]\n",
            "loss: 1.331605  [   59/ 3200]\n",
            "loss: 1.308184  [   60/ 3200]\n",
            "loss: 1.309670  [   61/ 3200]\n",
            "loss: 1.299379  [   62/ 3200]\n",
            "loss: 1.286300  [   63/ 3200]\n",
            "loss: 1.335472  [   64/ 3200]\n",
            "loss: 1.313340  [   65/ 3200]\n",
            "loss: 1.356313  [   66/ 3200]\n",
            "loss: 1.399081  [   67/ 3200]\n",
            "loss: 1.267552  [   68/ 3200]\n",
            "loss: 1.308694  [   69/ 3200]\n",
            "loss: 1.285199  [   70/ 3200]\n",
            "loss: 1.284831  [   71/ 3200]\n",
            "loss: 1.270261  [   72/ 3200]\n",
            "loss: 1.337179  [   73/ 3200]\n",
            "loss: 1.319778  [   74/ 3200]\n",
            "loss: 1.287528  [   75/ 3200]\n",
            "loss: 1.304818  [   76/ 3200]\n",
            "loss: 1.313835  [   77/ 3200]\n",
            "loss: 1.310619  [   78/ 3200]\n",
            "loss: 1.316471  [   79/ 3200]\n",
            "loss: 1.297627  [   80/ 3200]\n",
            "loss: 1.304022  [   81/ 3200]\n",
            "loss: 1.337597  [   82/ 3200]\n",
            "loss: 1.283589  [   83/ 3200]\n",
            "loss: 1.312945  [   84/ 3200]\n",
            "loss: 1.305932  [   85/ 3200]\n",
            "loss: 1.368885  [   86/ 3200]\n",
            "loss: 1.285423  [   87/ 3200]\n",
            "loss: 1.299339  [   88/ 3200]\n",
            "loss: 1.322127  [   89/ 3200]\n",
            "loss: 1.293835  [   90/ 3200]\n",
            "loss: 1.323341  [   91/ 3200]\n",
            "loss: 1.305569  [   92/ 3200]\n",
            "loss: 1.269146  [   93/ 3200]\n",
            "loss: 1.253444  [   94/ 3200]\n",
            "loss: 1.308362  [   95/ 3200]\n",
            "loss: 1.325864  [   96/ 3200]\n",
            "loss: 1.288445  [   97/ 3200]\n",
            "loss: 1.266188  [   98/ 3200]\n",
            "loss: 1.303228  [   99/ 3200]\n",
            "loss: 1.337820  [  100/ 3200]\n",
            "loss: 1.304618  [  101/ 3200]\n",
            "loss: 1.294872  [  102/ 3200]\n",
            "loss: 1.282099  [  103/ 3200]\n",
            "loss: 1.289784  [  104/ 3200]\n",
            "loss: 1.279775  [  105/ 3200]\n",
            "loss: 1.314856  [  106/ 3200]\n",
            "loss: 1.303372  [  107/ 3200]\n",
            "loss: 1.303035  [  108/ 3200]\n",
            "loss: 1.265111  [  109/ 3200]\n",
            "loss: 1.273702  [  110/ 3200]\n",
            "loss: 1.324268  [  111/ 3200]\n",
            "loss: 1.368530  [  112/ 3200]\n",
            "loss: 1.311390  [  113/ 3200]\n",
            "loss: 1.284701  [  114/ 3200]\n",
            "loss: 1.307742  [  115/ 3200]\n",
            "loss: 1.324982  [  116/ 3200]\n",
            "loss: 1.285732  [  117/ 3200]\n",
            "loss: 1.338363  [  118/ 3200]\n",
            "loss: 1.315822  [  119/ 3200]\n",
            "loss: 1.305534  [  120/ 3200]\n",
            "loss: 1.326625  [  121/ 3200]\n",
            "loss: 1.276130  [  122/ 3200]\n",
            "loss: 1.280020  [  123/ 3200]\n",
            "loss: 1.332417  [  124/ 3200]\n",
            "loss: 1.316258  [  125/ 3200]\n",
            "loss: 1.281605  [  126/ 3200]\n",
            "loss: 1.332798  [  127/ 3200]\n",
            "loss: 1.286042  [  128/ 3200]\n",
            "loss: 1.318625  [  129/ 3200]\n",
            "loss: 1.254499  [  130/ 3200]\n",
            "loss: 1.292129  [  131/ 3200]\n",
            "loss: 1.258532  [  132/ 3200]\n",
            "loss: 1.297555  [  133/ 3200]\n",
            "loss: 1.309645  [  134/ 3200]\n",
            "loss: 1.311515  [  135/ 3200]\n",
            "loss: 1.292870  [  136/ 3200]\n",
            "loss: 1.318617  [  137/ 3200]\n",
            "loss: 1.257969  [  138/ 3200]\n",
            "loss: 1.324219  [  139/ 3200]\n",
            "loss: 1.297328  [  140/ 3200]\n",
            "loss: 1.308039  [  141/ 3200]\n",
            "loss: 1.285084  [  142/ 3200]\n",
            "loss: 1.313539  [  143/ 3200]\n",
            "loss: 1.248159  [  144/ 3200]\n",
            "loss: 1.210191  [  145/ 3200]\n",
            "loss: 1.265550  [  146/ 3200]\n",
            "loss: 1.326624  [  147/ 3200]\n",
            "loss: 1.438099  [  148/ 3200]\n",
            "loss: 1.317307  [  149/ 3200]\n",
            "loss: 1.258624  [  150/ 3200]\n",
            "loss: 1.332252  [  151/ 3200]\n",
            "loss: 1.283555  [  152/ 3200]\n",
            "loss: 1.309969  [  153/ 3200]\n",
            "loss: 1.300829  [  154/ 3200]\n",
            "loss: 1.260705  [  155/ 3200]\n",
            "loss: 1.249100  [  156/ 3200]\n",
            "loss: 1.252265  [  157/ 3200]\n",
            "loss: 1.215705  [  158/ 3200]\n",
            "loss: 1.299682  [  159/ 3200]\n",
            "loss: 1.314842  [  160/ 3200]\n",
            "loss: 1.339606  [  161/ 3200]\n",
            "loss: 1.387888  [  162/ 3200]\n",
            "loss: 1.295642  [  163/ 3200]\n",
            "loss: 1.273231  [  164/ 3200]\n",
            "loss: 1.274931  [  165/ 3200]\n",
            "loss: 1.392642  [  166/ 3200]\n",
            "loss: 1.371168  [  167/ 3200]\n",
            "loss: 1.376107  [  168/ 3200]\n",
            "loss: 1.306311  [  169/ 3200]\n",
            "loss: 1.276894  [  170/ 3200]\n",
            "loss: 1.289833  [  171/ 3200]\n",
            "loss: 1.291026  [  172/ 3200]\n",
            "loss: 1.307855  [  173/ 3200]\n",
            "loss: 1.285003  [  174/ 3200]\n",
            "loss: 1.296153  [  175/ 3200]\n",
            "loss: 1.337758  [  176/ 3200]\n",
            "loss: 1.293458  [  177/ 3200]\n",
            "loss: 1.307212  [  178/ 3200]\n",
            "loss: 1.295527  [  179/ 3200]\n",
            "loss: 1.255282  [  180/ 3200]\n",
            "loss: 1.297902  [  181/ 3200]\n",
            "loss: 1.288266  [  182/ 3200]\n",
            "loss: 1.334715  [  183/ 3200]\n",
            "loss: 1.297626  [  184/ 3200]\n",
            "loss: 1.322891  [  185/ 3200]\n",
            "loss: 1.173597  [  186/ 3200]\n",
            "loss: 1.361208  [  187/ 3200]\n",
            "loss: 1.299875  [  188/ 3200]\n",
            "loss: 1.330047  [  189/ 3200]\n",
            "loss: 1.192018  [  190/ 3200]\n",
            "loss: 1.365265  [  191/ 3200]\n",
            "loss: 1.297112  [  192/ 3200]\n",
            "loss: 1.301017  [  193/ 3200]\n",
            "loss: 1.299749  [  194/ 3200]\n",
            "loss: 1.266542  [  195/ 3200]\n",
            "loss: 1.271025  [  196/ 3200]\n",
            "loss: 1.289052  [  197/ 3200]\n",
            "loss: 1.330033  [  198/ 3200]\n",
            "loss: 1.294773  [  199/ 3200]\n",
            "Epoch:  10\n",
            "loss: 1.255312  [    0/ 3200]\n",
            "loss: 1.279649  [    1/ 3200]\n",
            "loss: 1.289931  [    2/ 3200]\n",
            "loss: 1.286716  [    3/ 3200]\n",
            "loss: 1.291893  [    4/ 3200]\n",
            "loss: 1.330399  [    5/ 3200]\n",
            "loss: 1.367237  [    6/ 3200]\n",
            "loss: 1.342866  [    7/ 3200]\n",
            "loss: 1.325195  [    8/ 3200]\n",
            "loss: 1.249185  [    9/ 3200]\n",
            "loss: 1.302657  [   10/ 3200]\n",
            "loss: 1.320632  [   11/ 3200]\n",
            "loss: 1.286011  [   12/ 3200]\n",
            "loss: 1.223554  [   13/ 3200]\n",
            "loss: 1.287771  [   14/ 3200]\n",
            "loss: 1.261470  [   15/ 3200]\n",
            "loss: 1.361479  [   16/ 3200]\n",
            "loss: 1.314765  [   17/ 3200]\n",
            "loss: 1.301261  [   18/ 3200]\n",
            "loss: 1.331802  [   19/ 3200]\n",
            "loss: 1.251459  [   20/ 3200]\n",
            "loss: 1.308233  [   21/ 3200]\n",
            "loss: 1.258209  [   22/ 3200]\n",
            "loss: 1.344108  [   23/ 3200]\n",
            "loss: 1.271663  [   24/ 3200]\n",
            "loss: 1.268933  [   25/ 3200]\n",
            "loss: 1.309271  [   26/ 3200]\n",
            "loss: 1.242568  [   27/ 3200]\n",
            "loss: 1.294653  [   28/ 3200]\n",
            "loss: 1.430532  [   29/ 3200]\n",
            "loss: 1.295013  [   30/ 3200]\n",
            "loss: 1.332016  [   31/ 3200]\n",
            "loss: 1.330891  [   32/ 3200]\n",
            "loss: 1.271312  [   33/ 3200]\n",
            "loss: 1.322943  [   34/ 3200]\n",
            "loss: 1.265795  [   35/ 3200]\n",
            "loss: 1.326612  [   36/ 3200]\n",
            "loss: 1.292481  [   37/ 3200]\n",
            "loss: 1.314171  [   38/ 3200]\n",
            "loss: 1.289807  [   39/ 3200]\n",
            "loss: 1.199885  [   40/ 3200]\n",
            "loss: 1.265336  [   41/ 3200]\n",
            "loss: 1.363569  [   42/ 3200]\n",
            "loss: 1.301812  [   43/ 3200]\n",
            "loss: 1.299049  [   44/ 3200]\n",
            "loss: 1.235609  [   45/ 3200]\n",
            "loss: 1.243875  [   46/ 3200]\n",
            "loss: 1.316833  [   47/ 3200]\n",
            "loss: 1.277342  [   48/ 3200]\n",
            "loss: 1.362870  [   49/ 3200]\n",
            "loss: 1.290758  [   50/ 3200]\n",
            "loss: 1.217448  [   51/ 3200]\n",
            "loss: 1.277852  [   52/ 3200]\n",
            "loss: 1.284388  [   53/ 3200]\n",
            "loss: 1.333337  [   54/ 3200]\n",
            "loss: 1.406427  [   55/ 3200]\n",
            "loss: 1.240505  [   56/ 3200]\n",
            "loss: 1.288204  [   57/ 3200]\n",
            "loss: 1.300948  [   58/ 3200]\n",
            "loss: 1.326697  [   59/ 3200]\n",
            "loss: 1.282778  [   60/ 3200]\n",
            "loss: 1.297821  [   61/ 3200]\n",
            "loss: 1.264983  [   62/ 3200]\n",
            "loss: 1.226184  [   63/ 3200]\n",
            "loss: 1.274224  [   64/ 3200]\n",
            "loss: 1.330585  [   65/ 3200]\n",
            "loss: 1.289487  [   66/ 3200]\n",
            "loss: 1.282879  [   67/ 3200]\n",
            "loss: 1.273950  [   68/ 3200]\n",
            "loss: 1.295609  [   69/ 3200]\n",
            "loss: 1.236191  [   70/ 3200]\n",
            "loss: 1.209740  [   71/ 3200]\n",
            "loss: 1.278113  [   72/ 3200]\n",
            "loss: 1.297275  [   73/ 3200]\n",
            "loss: 1.345371  [   74/ 3200]\n",
            "loss: 1.309222  [   75/ 3200]\n",
            "loss: 1.278529  [   76/ 3200]\n",
            "loss: 1.222202  [   77/ 3200]\n",
            "loss: 1.310756  [   78/ 3200]\n",
            "loss: 1.271987  [   79/ 3200]\n",
            "loss: 1.299424  [   80/ 3200]\n",
            "loss: 1.290346  [   81/ 3200]\n",
            "loss: 1.294526  [   82/ 3200]\n",
            "loss: 1.268494  [   83/ 3200]\n",
            "loss: 1.307706  [   84/ 3200]\n",
            "loss: 1.294546  [   85/ 3200]\n",
            "loss: 1.218717  [   86/ 3200]\n",
            "loss: 1.199643  [   87/ 3200]\n",
            "loss: 1.322038  [   88/ 3200]\n",
            "loss: 1.313147  [   89/ 3200]\n",
            "loss: 1.228743  [   90/ 3200]\n",
            "loss: 1.274127  [   91/ 3200]\n",
            "loss: 1.255826  [   92/ 3200]\n",
            "loss: 1.354165  [   93/ 3200]\n",
            "loss: 1.378542  [   94/ 3200]\n",
            "loss: 1.308856  [   95/ 3200]\n",
            "loss: 1.261716  [   96/ 3200]\n",
            "loss: 1.266398  [   97/ 3200]\n",
            "loss: 1.304448  [   98/ 3200]\n",
            "loss: 1.275465  [   99/ 3200]\n",
            "loss: 1.273029  [  100/ 3200]\n",
            "loss: 1.347775  [  101/ 3200]\n",
            "loss: 1.319555  [  102/ 3200]\n",
            "loss: 1.291775  [  103/ 3200]\n",
            "loss: 1.273688  [  104/ 3200]\n",
            "loss: 1.315259  [  105/ 3200]\n",
            "loss: 1.248921  [  106/ 3200]\n",
            "loss: 1.254911  [  107/ 3200]\n",
            "loss: 1.261606  [  108/ 3200]\n",
            "loss: 1.390403  [  109/ 3200]\n",
            "loss: 1.322879  [  110/ 3200]\n",
            "loss: 1.307567  [  111/ 3200]\n",
            "loss: 1.326783  [  112/ 3200]\n",
            "loss: 1.235415  [  113/ 3200]\n",
            "loss: 1.295497  [  114/ 3200]\n",
            "loss: 1.260518  [  115/ 3200]\n",
            "loss: 1.268856  [  116/ 3200]\n",
            "loss: 1.282748  [  117/ 3200]\n",
            "loss: 1.263351  [  118/ 3200]\n",
            "loss: 1.254912  [  119/ 3200]\n",
            "loss: 1.273767  [  120/ 3200]\n",
            "loss: 1.278852  [  121/ 3200]\n",
            "loss: 1.321212  [  122/ 3200]\n",
            "loss: 1.231207  [  123/ 3200]\n",
            "loss: 1.332150  [  124/ 3200]\n",
            "loss: 1.268619  [  125/ 3200]\n",
            "loss: 1.284579  [  126/ 3200]\n",
            "loss: 1.312416  [  127/ 3200]\n",
            "loss: 1.290563  [  128/ 3200]\n",
            "loss: 1.262861  [  129/ 3200]\n",
            "loss: 1.282651  [  130/ 3200]\n",
            "loss: 1.260703  [  131/ 3200]\n",
            "loss: 1.301212  [  132/ 3200]\n",
            "loss: 1.274767  [  133/ 3200]\n",
            "loss: 1.261894  [  134/ 3200]\n",
            "loss: 1.351520  [  135/ 3200]\n",
            "loss: 1.290565  [  136/ 3200]\n",
            "loss: 1.360896  [  137/ 3200]\n",
            "loss: 1.301569  [  138/ 3200]\n",
            "loss: 1.264351  [  139/ 3200]\n",
            "loss: 1.305348  [  140/ 3200]\n",
            "loss: 1.218582  [  141/ 3200]\n",
            "loss: 1.335137  [  142/ 3200]\n",
            "loss: 1.268098  [  143/ 3200]\n",
            "loss: 1.261926  [  144/ 3200]\n",
            "loss: 1.326370  [  145/ 3200]\n",
            "loss: 1.250027  [  146/ 3200]\n",
            "loss: 1.275772  [  147/ 3200]\n",
            "loss: 1.267642  [  148/ 3200]\n",
            "loss: 1.275797  [  149/ 3200]\n",
            "loss: 1.279333  [  150/ 3200]\n",
            "loss: 1.344263  [  151/ 3200]\n",
            "loss: 1.296962  [  152/ 3200]\n",
            "loss: 1.249972  [  153/ 3200]\n",
            "loss: 1.225123  [  154/ 3200]\n",
            "loss: 1.295518  [  155/ 3200]\n",
            "loss: 1.263101  [  156/ 3200]\n",
            "loss: 1.240338  [  157/ 3200]\n",
            "loss: 1.231807  [  158/ 3200]\n",
            "loss: 1.367071  [  159/ 3200]\n",
            "loss: 1.306015  [  160/ 3200]\n",
            "loss: 1.234330  [  161/ 3200]\n",
            "loss: 1.230918  [  162/ 3200]\n",
            "loss: 1.277225  [  163/ 3200]\n",
            "loss: 1.235119  [  164/ 3200]\n",
            "loss: 1.291881  [  165/ 3200]\n",
            "loss: 1.351818  [  166/ 3200]\n",
            "loss: 1.257627  [  167/ 3200]\n",
            "loss: 1.324776  [  168/ 3200]\n",
            "loss: 1.204248  [  169/ 3200]\n",
            "loss: 1.314645  [  170/ 3200]\n",
            "loss: 1.278469  [  171/ 3200]\n",
            "loss: 1.245358  [  172/ 3200]\n",
            "loss: 1.403725  [  173/ 3200]\n",
            "loss: 1.305115  [  174/ 3200]\n",
            "loss: 1.278053  [  175/ 3200]\n",
            "loss: 1.264018  [  176/ 3200]\n",
            "loss: 1.306659  [  177/ 3200]\n",
            "loss: 1.334522  [  178/ 3200]\n",
            "loss: 1.297616  [  179/ 3200]\n",
            "loss: 1.289279  [  180/ 3200]\n",
            "loss: 1.201262  [  181/ 3200]\n",
            "loss: 1.306404  [  182/ 3200]\n",
            "loss: 1.228491  [  183/ 3200]\n",
            "loss: 1.261123  [  184/ 3200]\n",
            "loss: 1.324833  [  185/ 3200]\n",
            "loss: 1.322562  [  186/ 3200]\n",
            "loss: 1.269961  [  187/ 3200]\n",
            "loss: 1.299357  [  188/ 3200]\n",
            "loss: 1.257748  [  189/ 3200]\n",
            "loss: 1.256600  [  190/ 3200]\n",
            "loss: 1.339215  [  191/ 3200]\n",
            "loss: 1.266099  [  192/ 3200]\n",
            "loss: 1.266283  [  193/ 3200]\n",
            "loss: 1.278586  [  194/ 3200]\n",
            "loss: 1.279195  [  195/ 3200]\n",
            "loss: 1.195473  [  196/ 3200]\n",
            "loss: 1.210603  [  197/ 3200]\n",
            "loss: 1.368639  [  198/ 3200]\n",
            "loss: 1.299305  [  199/ 3200]\n",
            "Epoch:  11\n",
            "loss: 1.235631  [    0/ 3200]\n",
            "loss: 1.235643  [    1/ 3200]\n",
            "loss: 1.250201  [    2/ 3200]\n",
            "loss: 1.277963  [    3/ 3200]\n",
            "loss: 1.343622  [    4/ 3200]\n",
            "loss: 1.228840  [    5/ 3200]\n",
            "loss: 1.245254  [    6/ 3200]\n",
            "loss: 1.256169  [    7/ 3200]\n",
            "loss: 1.243183  [    8/ 3200]\n",
            "loss: 1.171764  [    9/ 3200]\n",
            "loss: 1.200530  [   10/ 3200]\n",
            "loss: 1.349186  [   11/ 3200]\n",
            "loss: 1.348433  [   12/ 3200]\n",
            "loss: 1.209637  [   13/ 3200]\n",
            "loss: 1.262365  [   14/ 3200]\n",
            "loss: 1.250610  [   15/ 3200]\n",
            "loss: 1.270958  [   16/ 3200]\n",
            "loss: 1.342995  [   17/ 3200]\n",
            "loss: 1.265655  [   18/ 3200]\n",
            "loss: 1.230135  [   19/ 3200]\n",
            "loss: 1.381200  [   20/ 3200]\n",
            "loss: 1.274913  [   21/ 3200]\n",
            "loss: 1.221632  [   22/ 3200]\n",
            "loss: 1.243698  [   23/ 3200]\n",
            "loss: 1.288640  [   24/ 3200]\n",
            "loss: 1.288380  [   25/ 3200]\n",
            "loss: 1.192243  [   26/ 3200]\n",
            "loss: 1.327981  [   27/ 3200]\n",
            "loss: 1.249391  [   28/ 3200]\n",
            "loss: 1.262366  [   29/ 3200]\n",
            "loss: 1.268449  [   30/ 3200]\n",
            "loss: 1.291006  [   31/ 3200]\n",
            "loss: 1.309057  [   32/ 3200]\n",
            "loss: 1.197575  [   33/ 3200]\n",
            "loss: 1.280110  [   34/ 3200]\n",
            "loss: 1.221267  [   35/ 3200]\n",
            "loss: 1.290671  [   36/ 3200]\n",
            "loss: 1.275382  [   37/ 3200]\n",
            "loss: 1.345961  [   38/ 3200]\n",
            "loss: 1.213253  [   39/ 3200]\n",
            "loss: 1.267412  [   40/ 3200]\n",
            "loss: 1.221508  [   41/ 3200]\n",
            "loss: 1.177191  [   42/ 3200]\n",
            "loss: 1.310956  [   43/ 3200]\n",
            "loss: 1.265273  [   44/ 3200]\n",
            "loss: 1.232595  [   45/ 3200]\n",
            "loss: 1.298365  [   46/ 3200]\n",
            "loss: 1.297318  [   47/ 3200]\n",
            "loss: 1.300405  [   48/ 3200]\n",
            "loss: 1.268386  [   49/ 3200]\n",
            "loss: 1.294409  [   50/ 3200]\n",
            "loss: 1.326913  [   51/ 3200]\n",
            "loss: 1.304886  [   52/ 3200]\n",
            "loss: 1.273761  [   53/ 3200]\n",
            "loss: 1.309407  [   54/ 3200]\n",
            "loss: 1.240569  [   55/ 3200]\n",
            "loss: 1.306884  [   56/ 3200]\n",
            "loss: 1.271007  [   57/ 3200]\n",
            "loss: 1.269442  [   58/ 3200]\n",
            "loss: 1.265774  [   59/ 3200]\n",
            "loss: 1.291465  [   60/ 3200]\n",
            "loss: 1.262046  [   61/ 3200]\n",
            "loss: 1.255118  [   62/ 3200]\n",
            "loss: 1.281277  [   63/ 3200]\n",
            "loss: 1.237450  [   64/ 3200]\n",
            "loss: 1.288830  [   65/ 3200]\n",
            "loss: 1.226988  [   66/ 3200]\n",
            "loss: 1.303350  [   67/ 3200]\n",
            "loss: 1.327106  [   68/ 3200]\n",
            "loss: 1.292433  [   69/ 3200]\n",
            "loss: 1.322617  [   70/ 3200]\n",
            "loss: 1.240695  [   71/ 3200]\n",
            "loss: 1.313507  [   72/ 3200]\n",
            "loss: 1.290294  [   73/ 3200]\n",
            "loss: 1.275984  [   74/ 3200]\n",
            "loss: 1.222484  [   75/ 3200]\n",
            "loss: 1.183014  [   76/ 3200]\n",
            "loss: 1.248315  [   77/ 3200]\n",
            "loss: 1.343327  [   78/ 3200]\n",
            "loss: 1.357461  [   79/ 3200]\n",
            "loss: 1.254432  [   80/ 3200]\n",
            "loss: 1.332840  [   81/ 3200]\n",
            "loss: 1.289958  [   82/ 3200]\n",
            "loss: 1.344709  [   83/ 3200]\n",
            "loss: 1.228603  [   84/ 3200]\n",
            "loss: 1.314335  [   85/ 3200]\n",
            "loss: 1.180085  [   86/ 3200]\n",
            "loss: 1.274718  [   87/ 3200]\n",
            "loss: 1.199269  [   88/ 3200]\n",
            "loss: 1.290921  [   89/ 3200]\n",
            "loss: 1.273404  [   90/ 3200]\n",
            "loss: 1.350793  [   91/ 3200]\n",
            "loss: 1.226230  [   92/ 3200]\n",
            "loss: 1.310171  [   93/ 3200]\n",
            "loss: 1.291246  [   94/ 3200]\n",
            "loss: 1.247675  [   95/ 3200]\n",
            "loss: 1.352801  [   96/ 3200]\n",
            "loss: 1.273571  [   97/ 3200]\n",
            "loss: 1.291047  [   98/ 3200]\n",
            "loss: 1.332345  [   99/ 3200]\n",
            "loss: 1.256727  [  100/ 3200]\n",
            "loss: 1.246320  [  101/ 3200]\n",
            "loss: 1.249461  [  102/ 3200]\n",
            "loss: 1.255475  [  103/ 3200]\n",
            "loss: 1.308126  [  104/ 3200]\n",
            "loss: 1.218567  [  105/ 3200]\n",
            "loss: 1.290454  [  106/ 3200]\n",
            "loss: 1.317038  [  107/ 3200]\n",
            "loss: 1.210896  [  108/ 3200]\n",
            "loss: 1.292322  [  109/ 3200]\n",
            "loss: 1.256139  [  110/ 3200]\n",
            "loss: 1.270209  [  111/ 3200]\n",
            "loss: 1.282575  [  112/ 3200]\n",
            "loss: 1.248383  [  113/ 3200]\n",
            "loss: 1.281796  [  114/ 3200]\n",
            "loss: 1.233764  [  115/ 3200]\n",
            "loss: 1.277947  [  116/ 3200]\n",
            "loss: 1.261878  [  117/ 3200]\n",
            "loss: 1.235009  [  118/ 3200]\n",
            "loss: 1.260176  [  119/ 3200]\n",
            "loss: 1.305232  [  120/ 3200]\n",
            "loss: 1.320649  [  121/ 3200]\n",
            "loss: 1.320426  [  122/ 3200]\n",
            "loss: 1.292120  [  123/ 3200]\n",
            "loss: 1.258098  [  124/ 3200]\n",
            "loss: 1.269776  [  125/ 3200]\n",
            "loss: 1.244250  [  126/ 3200]\n",
            "loss: 1.279279  [  127/ 3200]\n",
            "loss: 1.266149  [  128/ 3200]\n",
            "loss: 1.225860  [  129/ 3200]\n",
            "loss: 1.274380  [  130/ 3200]\n",
            "loss: 1.236767  [  131/ 3200]\n",
            "loss: 1.294477  [  132/ 3200]\n",
            "loss: 1.272700  [  133/ 3200]\n",
            "loss: 1.327855  [  134/ 3200]\n",
            "loss: 1.260465  [  135/ 3200]\n",
            "loss: 1.204117  [  136/ 3200]\n",
            "loss: 1.151964  [  137/ 3200]\n",
            "loss: 1.347958  [  138/ 3200]\n",
            "loss: 1.184957  [  139/ 3200]\n",
            "loss: 1.310688  [  140/ 3200]\n",
            "loss: 1.188067  [  141/ 3200]\n",
            "loss: 1.337959  [  142/ 3200]\n",
            "loss: 1.260176  [  143/ 3200]\n",
            "loss: 1.269710  [  144/ 3200]\n",
            "loss: 1.316202  [  145/ 3200]\n",
            "loss: 1.251085  [  146/ 3200]\n",
            "loss: 1.364900  [  147/ 3200]\n",
            "loss: 1.240814  [  148/ 3200]\n",
            "loss: 1.237881  [  149/ 3200]\n",
            "loss: 1.219975  [  150/ 3200]\n",
            "loss: 1.239946  [  151/ 3200]\n",
            "loss: 1.343242  [  152/ 3200]\n",
            "loss: 1.166003  [  153/ 3200]\n",
            "loss: 1.340017  [  154/ 3200]\n",
            "loss: 1.266913  [  155/ 3200]\n",
            "loss: 1.236646  [  156/ 3200]\n",
            "loss: 1.255041  [  157/ 3200]\n",
            "loss: 1.267882  [  158/ 3200]\n",
            "loss: 1.304508  [  159/ 3200]\n",
            "loss: 1.278575  [  160/ 3200]\n",
            "loss: 1.298917  [  161/ 3200]\n",
            "loss: 1.301546  [  162/ 3200]\n",
            "loss: 1.172018  [  163/ 3200]\n",
            "loss: 1.302977  [  164/ 3200]\n",
            "loss: 1.311000  [  165/ 3200]\n",
            "loss: 1.270228  [  166/ 3200]\n",
            "loss: 1.237129  [  167/ 3200]\n",
            "loss: 1.281491  [  168/ 3200]\n",
            "loss: 1.236715  [  169/ 3200]\n",
            "loss: 1.305462  [  170/ 3200]\n",
            "loss: 1.238421  [  171/ 3200]\n",
            "loss: 1.245085  [  172/ 3200]\n",
            "loss: 1.341483  [  173/ 3200]\n",
            "loss: 1.234246  [  174/ 3200]\n",
            "loss: 1.261386  [  175/ 3200]\n",
            "loss: 1.262725  [  176/ 3200]\n",
            "loss: 1.321324  [  177/ 3200]\n",
            "loss: 1.288954  [  178/ 3200]\n",
            "loss: 1.264195  [  179/ 3200]\n",
            "loss: 1.224503  [  180/ 3200]\n",
            "loss: 1.234338  [  181/ 3200]\n",
            "loss: 1.242597  [  182/ 3200]\n",
            "loss: 1.176804  [  183/ 3200]\n",
            "loss: 1.309975  [  184/ 3200]\n",
            "loss: 1.266373  [  185/ 3200]\n",
            "loss: 1.265936  [  186/ 3200]\n",
            "loss: 1.266653  [  187/ 3200]\n",
            "loss: 1.293425  [  188/ 3200]\n",
            "loss: 1.263266  [  189/ 3200]\n",
            "loss: 1.278459  [  190/ 3200]\n",
            "loss: 1.269338  [  191/ 3200]\n",
            "loss: 1.241547  [  192/ 3200]\n",
            "loss: 1.172310  [  193/ 3200]\n",
            "loss: 1.273696  [  194/ 3200]\n",
            "loss: 1.422542  [  195/ 3200]\n",
            "loss: 1.224048  [  196/ 3200]\n",
            "loss: 1.253152  [  197/ 3200]\n",
            "loss: 1.220578  [  198/ 3200]\n",
            "loss: 1.332695  [  199/ 3200]\n",
            "Epoch:  12\n",
            "loss: 1.240654  [    0/ 3200]\n",
            "loss: 1.350962  [    1/ 3200]\n",
            "loss: 1.351142  [    2/ 3200]\n",
            "loss: 1.290256  [    3/ 3200]\n",
            "loss: 1.278742  [    4/ 3200]\n",
            "loss: 1.211350  [    5/ 3200]\n",
            "loss: 1.264017  [    6/ 3200]\n",
            "loss: 1.224023  [    7/ 3200]\n",
            "loss: 1.274433  [    8/ 3200]\n",
            "loss: 1.328252  [    9/ 3200]\n",
            "loss: 1.226812  [   10/ 3200]\n",
            "loss: 1.229722  [   11/ 3200]\n",
            "loss: 1.261157  [   12/ 3200]\n",
            "loss: 1.267019  [   13/ 3200]\n",
            "loss: 1.215333  [   14/ 3200]\n",
            "loss: 1.294850  [   15/ 3200]\n",
            "loss: 1.167912  [   16/ 3200]\n",
            "loss: 1.259567  [   17/ 3200]\n",
            "loss: 1.249384  [   18/ 3200]\n",
            "loss: 1.327753  [   19/ 3200]\n",
            "loss: 1.262499  [   20/ 3200]\n",
            "loss: 1.220271  [   21/ 3200]\n",
            "loss: 1.291910  [   22/ 3200]\n",
            "loss: 1.293230  [   23/ 3200]\n",
            "loss: 1.295330  [   24/ 3200]\n",
            "loss: 1.300162  [   25/ 3200]\n",
            "loss: 1.260616  [   26/ 3200]\n",
            "loss: 1.267571  [   27/ 3200]\n",
            "loss: 1.296312  [   28/ 3200]\n",
            "loss: 1.285987  [   29/ 3200]\n",
            "loss: 1.360326  [   30/ 3200]\n",
            "loss: 1.259379  [   31/ 3200]\n",
            "loss: 1.218782  [   32/ 3200]\n",
            "loss: 1.267473  [   33/ 3200]\n",
            "loss: 1.318524  [   34/ 3200]\n",
            "loss: 1.211938  [   35/ 3200]\n",
            "loss: 1.258785  [   36/ 3200]\n",
            "loss: 1.272534  [   37/ 3200]\n",
            "loss: 1.288521  [   38/ 3200]\n",
            "loss: 1.270928  [   39/ 3200]\n",
            "loss: 1.211136  [   40/ 3200]\n",
            "loss: 1.316657  [   41/ 3200]\n",
            "loss: 1.243272  [   42/ 3200]\n",
            "loss: 1.161954  [   43/ 3200]\n",
            "loss: 1.220445  [   44/ 3200]\n",
            "loss: 1.242903  [   45/ 3200]\n",
            "loss: 1.104635  [   46/ 3200]\n",
            "loss: 1.196883  [   47/ 3200]\n",
            "loss: 1.253532  [   48/ 3200]\n",
            "loss: 1.157836  [   49/ 3200]\n",
            "loss: 1.233221  [   50/ 3200]\n",
            "loss: 1.227227  [   51/ 3200]\n",
            "loss: 1.233964  [   52/ 3200]\n",
            "loss: 1.235357  [   53/ 3200]\n",
            "loss: 1.263680  [   54/ 3200]\n",
            "loss: 1.176849  [   55/ 3200]\n",
            "loss: 1.320106  [   56/ 3200]\n",
            "loss: 1.240002  [   57/ 3200]\n",
            "loss: 1.298612  [   58/ 3200]\n",
            "loss: 1.392751  [   59/ 3200]\n",
            "loss: 1.220724  [   60/ 3200]\n",
            "loss: 1.246755  [   61/ 3200]\n",
            "loss: 1.241839  [   62/ 3200]\n",
            "loss: 1.227780  [   63/ 3200]\n",
            "loss: 1.311861  [   64/ 3200]\n",
            "loss: 1.239313  [   65/ 3200]\n",
            "loss: 1.175561  [   66/ 3200]\n",
            "loss: 1.355888  [   67/ 3200]\n",
            "loss: 1.198614  [   68/ 3200]\n",
            "loss: 1.321498  [   69/ 3200]\n",
            "loss: 1.335826  [   70/ 3200]\n",
            "loss: 1.327893  [   71/ 3200]\n",
            "loss: 1.238398  [   72/ 3200]\n",
            "loss: 1.224835  [   73/ 3200]\n",
            "loss: 1.259247  [   74/ 3200]\n",
            "loss: 1.219594  [   75/ 3200]\n",
            "loss: 1.253611  [   76/ 3200]\n",
            "loss: 1.249331  [   77/ 3200]\n",
            "loss: 1.250246  [   78/ 3200]\n",
            "loss: 1.233485  [   79/ 3200]\n",
            "loss: 1.273127  [   80/ 3200]\n",
            "loss: 1.271122  [   81/ 3200]\n",
            "loss: 1.300609  [   82/ 3200]\n",
            "loss: 1.236421  [   83/ 3200]\n",
            "loss: 1.312176  [   84/ 3200]\n",
            "loss: 1.293762  [   85/ 3200]\n",
            "loss: 1.240573  [   86/ 3200]\n",
            "loss: 1.240568  [   87/ 3200]\n",
            "loss: 1.209238  [   88/ 3200]\n",
            "loss: 1.372249  [   89/ 3200]\n",
            "loss: 1.260471  [   90/ 3200]\n",
            "loss: 1.328867  [   91/ 3200]\n",
            "loss: 1.254479  [   92/ 3200]\n",
            "loss: 1.237881  [   93/ 3200]\n",
            "loss: 1.265512  [   94/ 3200]\n",
            "loss: 1.269486  [   95/ 3200]\n",
            "loss: 1.221053  [   96/ 3200]\n",
            "loss: 1.146945  [   97/ 3200]\n",
            "loss: 1.285383  [   98/ 3200]\n",
            "loss: 1.250189  [   99/ 3200]\n",
            "loss: 1.209889  [  100/ 3200]\n",
            "loss: 1.248038  [  101/ 3200]\n",
            "loss: 1.228400  [  102/ 3200]\n",
            "loss: 1.274333  [  103/ 3200]\n",
            "loss: 1.165528  [  104/ 3200]\n",
            "loss: 1.370507  [  105/ 3200]\n",
            "loss: 1.284268  [  106/ 3200]\n",
            "loss: 1.183388  [  107/ 3200]\n",
            "loss: 1.168339  [  108/ 3200]\n",
            "loss: 1.292054  [  109/ 3200]\n",
            "loss: 1.091126  [  110/ 3200]\n",
            "loss: 1.278409  [  111/ 3200]\n",
            "loss: 1.262671  [  112/ 3200]\n",
            "loss: 1.292710  [  113/ 3200]\n",
            "loss: 1.224065  [  114/ 3200]\n",
            "loss: 1.296852  [  115/ 3200]\n",
            "loss: 1.244950  [  116/ 3200]\n",
            "loss: 1.183459  [  117/ 3200]\n",
            "loss: 1.283822  [  118/ 3200]\n",
            "loss: 1.202732  [  119/ 3200]\n",
            "loss: 1.191928  [  120/ 3200]\n",
            "loss: 1.252857  [  121/ 3200]\n",
            "loss: 1.262098  [  122/ 3200]\n",
            "loss: 1.178151  [  123/ 3200]\n",
            "loss: 1.244553  [  124/ 3200]\n",
            "loss: 1.247259  [  125/ 3200]\n",
            "loss: 1.257232  [  126/ 3200]\n",
            "loss: 1.237806  [  127/ 3200]\n",
            "loss: 1.210568  [  128/ 3200]\n",
            "loss: 1.146639  [  129/ 3200]\n",
            "loss: 1.307518  [  130/ 3200]\n",
            "loss: 1.256814  [  131/ 3200]\n",
            "loss: 1.267118  [  132/ 3200]\n",
            "loss: 1.189343  [  133/ 3200]\n",
            "loss: 1.141843  [  134/ 3200]\n",
            "loss: 1.363554  [  135/ 3200]\n",
            "loss: 1.268485  [  136/ 3200]\n",
            "loss: 1.210421  [  137/ 3200]\n",
            "loss: 1.204727  [  138/ 3200]\n",
            "loss: 1.280411  [  139/ 3200]\n",
            "loss: 1.225200  [  140/ 3200]\n",
            "loss: 1.236032  [  141/ 3200]\n",
            "loss: 1.173507  [  142/ 3200]\n",
            "loss: 1.224663  [  143/ 3200]\n",
            "loss: 1.297069  [  144/ 3200]\n",
            "loss: 1.257859  [  145/ 3200]\n",
            "loss: 1.292655  [  146/ 3200]\n",
            "loss: 1.244860  [  147/ 3200]\n",
            "loss: 1.299746  [  148/ 3200]\n",
            "loss: 1.266851  [  149/ 3200]\n",
            "loss: 1.236891  [  150/ 3200]\n",
            "loss: 1.273570  [  151/ 3200]\n",
            "loss: 1.206013  [  152/ 3200]\n",
            "loss: 1.239439  [  153/ 3200]\n",
            "loss: 1.266751  [  154/ 3200]\n",
            "loss: 1.160217  [  155/ 3200]\n",
            "loss: 1.292755  [  156/ 3200]\n",
            "loss: 1.231542  [  157/ 3200]\n",
            "loss: 1.350646  [  158/ 3200]\n",
            "loss: 1.231246  [  159/ 3200]\n",
            "loss: 1.266935  [  160/ 3200]\n",
            "loss: 1.186204  [  161/ 3200]\n",
            "loss: 1.407361  [  162/ 3200]\n",
            "loss: 1.222887  [  163/ 3200]\n",
            "loss: 1.176550  [  164/ 3200]\n",
            "loss: 1.252219  [  165/ 3200]\n",
            "loss: 1.279824  [  166/ 3200]\n",
            "loss: 1.183215  [  167/ 3200]\n",
            "loss: 1.232407  [  168/ 3200]\n",
            "loss: 1.270758  [  169/ 3200]\n",
            "loss: 1.218328  [  170/ 3200]\n",
            "loss: 1.256904  [  171/ 3200]\n",
            "loss: 1.298144  [  172/ 3200]\n",
            "loss: 1.273003  [  173/ 3200]\n",
            "loss: 1.258238  [  174/ 3200]\n",
            "loss: 1.214093  [  175/ 3200]\n",
            "loss: 1.255605  [  176/ 3200]\n",
            "loss: 1.392027  [  177/ 3200]\n",
            "loss: 1.135521  [  178/ 3200]\n",
            "loss: 1.342621  [  179/ 3200]\n",
            "loss: 1.178881  [  180/ 3200]\n",
            "loss: 1.142122  [  181/ 3200]\n",
            "loss: 1.198866  [  182/ 3200]\n",
            "loss: 1.391036  [  183/ 3200]\n",
            "loss: 1.263099  [  184/ 3200]\n",
            "loss: 1.203984  [  185/ 3200]\n",
            "loss: 1.300469  [  186/ 3200]\n",
            "loss: 1.216115  [  187/ 3200]\n",
            "loss: 1.189281  [  188/ 3200]\n",
            "loss: 1.234506  [  189/ 3200]\n",
            "loss: 1.188759  [  190/ 3200]\n",
            "loss: 1.236879  [  191/ 3200]\n",
            "loss: 1.302991  [  192/ 3200]\n",
            "loss: 1.299878  [  193/ 3200]\n",
            "loss: 1.265223  [  194/ 3200]\n",
            "loss: 1.207067  [  195/ 3200]\n",
            "loss: 1.217000  [  196/ 3200]\n",
            "loss: 1.209537  [  197/ 3200]\n",
            "loss: 1.249880  [  198/ 3200]\n",
            "loss: 1.224211  [  199/ 3200]\n",
            "Epoch:  13\n",
            "loss: 1.228853  [    0/ 3200]\n",
            "loss: 1.229213  [    1/ 3200]\n",
            "loss: 1.223679  [    2/ 3200]\n",
            "loss: 1.234154  [    3/ 3200]\n",
            "loss: 1.318805  [    4/ 3200]\n",
            "loss: 1.253009  [    5/ 3200]\n",
            "loss: 1.239110  [    6/ 3200]\n",
            "loss: 1.290747  [    7/ 3200]\n",
            "loss: 1.294856  [    8/ 3200]\n",
            "loss: 1.259933  [    9/ 3200]\n",
            "loss: 1.217433  [   10/ 3200]\n",
            "loss: 1.331773  [   11/ 3200]\n",
            "loss: 1.183358  [   12/ 3200]\n",
            "loss: 1.240718  [   13/ 3200]\n",
            "loss: 1.202422  [   14/ 3200]\n",
            "loss: 1.243554  [   15/ 3200]\n",
            "loss: 1.237721  [   16/ 3200]\n",
            "loss: 1.275662  [   17/ 3200]\n",
            "loss: 1.276790  [   18/ 3200]\n",
            "loss: 1.266485  [   19/ 3200]\n",
            "loss: 1.225675  [   20/ 3200]\n",
            "loss: 1.351232  [   21/ 3200]\n",
            "loss: 1.175979  [   22/ 3200]\n",
            "loss: 1.300085  [   23/ 3200]\n",
            "loss: 1.259220  [   24/ 3200]\n",
            "loss: 1.214426  [   25/ 3200]\n",
            "loss: 1.131724  [   26/ 3200]\n",
            "loss: 1.278288  [   27/ 3200]\n",
            "loss: 1.085349  [   28/ 3200]\n",
            "loss: 1.258141  [   29/ 3200]\n",
            "loss: 1.251568  [   30/ 3200]\n",
            "loss: 1.263760  [   31/ 3200]\n",
            "loss: 1.129698  [   32/ 3200]\n",
            "loss: 1.254566  [   33/ 3200]\n",
            "loss: 1.237463  [   34/ 3200]\n",
            "loss: 1.192375  [   35/ 3200]\n",
            "loss: 1.125408  [   36/ 3200]\n",
            "loss: 1.280370  [   37/ 3200]\n",
            "loss: 1.303827  [   38/ 3200]\n",
            "loss: 1.251144  [   39/ 3200]\n",
            "loss: 1.277653  [   40/ 3200]\n",
            "loss: 1.085836  [   41/ 3200]\n",
            "loss: 1.221270  [   42/ 3200]\n",
            "loss: 1.207798  [   43/ 3200]\n",
            "loss: 1.198007  [   44/ 3200]\n",
            "loss: 1.152721  [   45/ 3200]\n",
            "loss: 1.201146  [   46/ 3200]\n",
            "loss: 1.100097  [   47/ 3200]\n",
            "loss: 1.306566  [   48/ 3200]\n",
            "loss: 1.172898  [   49/ 3200]\n",
            "loss: 1.036631  [   50/ 3200]\n",
            "loss: 1.268456  [   51/ 3200]\n",
            "loss: 1.354686  [   52/ 3200]\n",
            "loss: 1.260679  [   53/ 3200]\n",
            "loss: 1.147705  [   54/ 3200]\n",
            "loss: 1.207134  [   55/ 3200]\n",
            "loss: 1.181792  [   56/ 3200]\n",
            "loss: 1.230927  [   57/ 3200]\n",
            "loss: 1.204948  [   58/ 3200]\n",
            "loss: 1.289744  [   59/ 3200]\n",
            "loss: 1.130057  [   60/ 3200]\n",
            "loss: 1.321824  [   61/ 3200]\n",
            "loss: 1.262864  [   62/ 3200]\n",
            "loss: 1.250044  [   63/ 3200]\n",
            "loss: 1.250735  [   64/ 3200]\n",
            "loss: 1.235054  [   65/ 3200]\n",
            "loss: 1.234468  [   66/ 3200]\n",
            "loss: 1.313413  [   67/ 3200]\n",
            "loss: 1.206097  [   68/ 3200]\n",
            "loss: 1.263638  [   69/ 3200]\n",
            "loss: 1.205323  [   70/ 3200]\n",
            "loss: 1.207231  [   71/ 3200]\n",
            "loss: 1.266263  [   72/ 3200]\n",
            "loss: 1.183483  [   73/ 3200]\n",
            "loss: 1.233733  [   74/ 3200]\n",
            "loss: 1.290120  [   75/ 3200]\n",
            "loss: 1.168844  [   76/ 3200]\n",
            "loss: 1.183889  [   77/ 3200]\n",
            "loss: 1.208716  [   78/ 3200]\n",
            "loss: 1.264607  [   79/ 3200]\n",
            "loss: 1.276586  [   80/ 3200]\n",
            "loss: 1.254060  [   81/ 3200]\n",
            "loss: 1.209367  [   82/ 3200]\n",
            "loss: 1.211305  [   83/ 3200]\n",
            "loss: 1.241932  [   84/ 3200]\n",
            "loss: 1.244899  [   85/ 3200]\n",
            "loss: 1.255356  [   86/ 3200]\n",
            "loss: 1.184231  [   87/ 3200]\n",
            "loss: 1.220590  [   88/ 3200]\n",
            "loss: 1.395481  [   89/ 3200]\n",
            "loss: 1.315633  [   90/ 3200]\n",
            "loss: 1.254735  [   91/ 3200]\n",
            "loss: 1.249293  [   92/ 3200]\n",
            "loss: 1.253473  [   93/ 3200]\n",
            "loss: 1.238274  [   94/ 3200]\n",
            "loss: 1.195330  [   95/ 3200]\n",
            "loss: 1.293089  [   96/ 3200]\n",
            "loss: 1.278923  [   97/ 3200]\n",
            "loss: 1.159665  [   98/ 3200]\n",
            "loss: 1.224077  [   99/ 3200]\n",
            "loss: 1.253777  [  100/ 3200]\n",
            "loss: 1.220830  [  101/ 3200]\n",
            "loss: 1.194183  [  102/ 3200]\n",
            "loss: 1.190066  [  103/ 3200]\n",
            "loss: 1.200753  [  104/ 3200]\n",
            "loss: 1.196054  [  105/ 3200]\n",
            "loss: 1.175849  [  106/ 3200]\n",
            "loss: 1.278034  [  107/ 3200]\n",
            "loss: 1.245735  [  108/ 3200]\n",
            "loss: 1.342694  [  109/ 3200]\n",
            "loss: 1.184646  [  110/ 3200]\n",
            "loss: 1.215134  [  111/ 3200]\n",
            "loss: 1.250447  [  112/ 3200]\n",
            "loss: 1.306270  [  113/ 3200]\n",
            "loss: 1.181775  [  114/ 3200]\n",
            "loss: 1.268125  [  115/ 3200]\n",
            "loss: 1.306384  [  116/ 3200]\n",
            "loss: 1.255263  [  117/ 3200]\n",
            "loss: 1.251933  [  118/ 3200]\n",
            "loss: 1.202045  [  119/ 3200]\n",
            "loss: 1.230467  [  120/ 3200]\n",
            "loss: 1.245478  [  121/ 3200]\n",
            "loss: 1.372314  [  122/ 3200]\n",
            "loss: 1.247024  [  123/ 3200]\n",
            "loss: 1.230061  [  124/ 3200]\n",
            "loss: 1.231283  [  125/ 3200]\n",
            "loss: 1.208907  [  126/ 3200]\n",
            "loss: 1.152250  [  127/ 3200]\n",
            "loss: 1.198619  [  128/ 3200]\n",
            "loss: 1.231823  [  129/ 3200]\n",
            "loss: 1.295297  [  130/ 3200]\n",
            "loss: 1.256636  [  131/ 3200]\n",
            "loss: 1.192212  [  132/ 3200]\n",
            "loss: 1.247218  [  133/ 3200]\n",
            "loss: 1.299312  [  134/ 3200]\n",
            "loss: 1.207405  [  135/ 3200]\n",
            "loss: 1.184304  [  136/ 3200]\n",
            "loss: 1.142455  [  137/ 3200]\n",
            "loss: 1.215276  [  138/ 3200]\n",
            "loss: 1.293765  [  139/ 3200]\n",
            "loss: 1.261802  [  140/ 3200]\n",
            "loss: 1.079616  [  141/ 3200]\n",
            "loss: 1.169922  [  142/ 3200]\n",
            "loss: 1.183768  [  143/ 3200]\n",
            "loss: 1.233265  [  144/ 3200]\n",
            "loss: 1.192301  [  145/ 3200]\n",
            "loss: 1.154706  [  146/ 3200]\n",
            "loss: 1.298173  [  147/ 3200]\n",
            "loss: 1.231339  [  148/ 3200]\n",
            "loss: 1.287240  [  149/ 3200]\n",
            "loss: 1.158559  [  150/ 3200]\n",
            "loss: 1.144029  [  151/ 3200]\n",
            "loss: 1.250993  [  152/ 3200]\n",
            "loss: 1.300392  [  153/ 3200]\n",
            "loss: 1.341272  [  154/ 3200]\n",
            "loss: 1.243623  [  155/ 3200]\n",
            "loss: 1.213634  [  156/ 3200]\n",
            "loss: 1.222782  [  157/ 3200]\n",
            "loss: 1.305259  [  158/ 3200]\n",
            "loss: 1.220326  [  159/ 3200]\n",
            "loss: 1.131523  [  160/ 3200]\n",
            "loss: 1.273913  [  161/ 3200]\n",
            "loss: 1.173002  [  162/ 3200]\n",
            "loss: 1.183307  [  163/ 3200]\n",
            "loss: 1.164701  [  164/ 3200]\n",
            "loss: 1.142510  [  165/ 3200]\n",
            "loss: 1.214748  [  166/ 3200]\n",
            "loss: 1.284913  [  167/ 3200]\n",
            "loss: 1.200117  [  168/ 3200]\n",
            "loss: 1.259108  [  169/ 3200]\n",
            "loss: 1.217315  [  170/ 3200]\n",
            "loss: 1.193582  [  171/ 3200]\n",
            "loss: 1.128184  [  172/ 3200]\n",
            "loss: 1.206068  [  173/ 3200]\n",
            "loss: 1.237760  [  174/ 3200]\n",
            "loss: 1.169688  [  175/ 3200]\n",
            "loss: 1.185693  [  176/ 3200]\n",
            "loss: 1.208776  [  177/ 3200]\n",
            "loss: 1.196623  [  178/ 3200]\n",
            "loss: 1.173663  [  179/ 3200]\n",
            "loss: 1.286418  [  180/ 3200]\n",
            "loss: 1.313132  [  181/ 3200]\n",
            "loss: 1.154247  [  182/ 3200]\n",
            "loss: 1.223810  [  183/ 3200]\n",
            "loss: 1.266784  [  184/ 3200]\n",
            "loss: 1.197432  [  185/ 3200]\n",
            "loss: 1.243982  [  186/ 3200]\n",
            "loss: 1.129600  [  187/ 3200]\n",
            "loss: 1.169636  [  188/ 3200]\n",
            "loss: 1.199443  [  189/ 3200]\n",
            "loss: 1.285591  [  190/ 3200]\n",
            "loss: 1.161101  [  191/ 3200]\n",
            "loss: 1.160287  [  192/ 3200]\n",
            "loss: 1.110917  [  193/ 3200]\n",
            "loss: 1.305111  [  194/ 3200]\n",
            "loss: 1.114292  [  195/ 3200]\n",
            "loss: 1.165650  [  196/ 3200]\n",
            "loss: 1.238734  [  197/ 3200]\n",
            "loss: 1.358488  [  198/ 3200]\n",
            "loss: 1.318053  [  199/ 3200]\n",
            "Epoch:  14\n",
            "loss: 1.190551  [    0/ 3200]\n",
            "loss: 1.252567  [    1/ 3200]\n",
            "loss: 1.158448  [    2/ 3200]\n",
            "loss: 1.231034  [    3/ 3200]\n",
            "loss: 1.151185  [    4/ 3200]\n",
            "loss: 1.275841  [    5/ 3200]\n",
            "loss: 1.231518  [    6/ 3200]\n",
            "loss: 1.312392  [    7/ 3200]\n",
            "loss: 1.277994  [    8/ 3200]\n",
            "loss: 1.204769  [    9/ 3200]\n",
            "loss: 1.288257  [   10/ 3200]\n",
            "loss: 1.232909  [   11/ 3200]\n",
            "loss: 1.169598  [   12/ 3200]\n",
            "loss: 1.268425  [   13/ 3200]\n",
            "loss: 1.205423  [   14/ 3200]\n",
            "loss: 1.123982  [   15/ 3200]\n",
            "loss: 1.236906  [   16/ 3200]\n",
            "loss: 1.276509  [   17/ 3200]\n",
            "loss: 1.290054  [   18/ 3200]\n",
            "loss: 1.207775  [   19/ 3200]\n",
            "loss: 1.211731  [   20/ 3200]\n",
            "loss: 1.073361  [   21/ 3200]\n",
            "loss: 1.194360  [   22/ 3200]\n",
            "loss: 1.176099  [   23/ 3200]\n",
            "loss: 1.200189  [   24/ 3200]\n",
            "loss: 1.284440  [   25/ 3200]\n",
            "loss: 1.153468  [   26/ 3200]\n",
            "loss: 1.022154  [   27/ 3200]\n",
            "loss: 1.144803  [   28/ 3200]\n",
            "loss: 1.255952  [   29/ 3200]\n",
            "loss: 1.224187  [   30/ 3200]\n",
            "loss: 1.262380  [   31/ 3200]\n",
            "loss: 1.202034  [   32/ 3200]\n",
            "loss: 1.193127  [   33/ 3200]\n",
            "loss: 1.133386  [   34/ 3200]\n",
            "loss: 1.213095  [   35/ 3200]\n",
            "loss: 1.237518  [   36/ 3200]\n",
            "loss: 1.281481  [   37/ 3200]\n",
            "loss: 1.241015  [   38/ 3200]\n",
            "loss: 1.167658  [   39/ 3200]\n",
            "loss: 1.227254  [   40/ 3200]\n",
            "loss: 1.145780  [   41/ 3200]\n",
            "loss: 1.253003  [   42/ 3200]\n",
            "loss: 1.221982  [   43/ 3200]\n",
            "loss: 1.145019  [   44/ 3200]\n",
            "loss: 1.163719  [   45/ 3200]\n",
            "loss: 1.094287  [   46/ 3200]\n",
            "loss: 1.252465  [   47/ 3200]\n",
            "loss: 1.310367  [   48/ 3200]\n",
            "loss: 1.117945  [   49/ 3200]\n",
            "loss: 1.321406  [   50/ 3200]\n",
            "loss: 1.259084  [   51/ 3200]\n",
            "loss: 1.258727  [   52/ 3200]\n",
            "loss: 1.245319  [   53/ 3200]\n",
            "loss: 1.299164  [   54/ 3200]\n",
            "loss: 1.186871  [   55/ 3200]\n",
            "loss: 1.216198  [   56/ 3200]\n",
            "loss: 1.143198  [   57/ 3200]\n",
            "loss: 1.275008  [   58/ 3200]\n",
            "loss: 1.242523  [   59/ 3200]\n",
            "loss: 1.194511  [   60/ 3200]\n",
            "loss: 1.210522  [   61/ 3200]\n",
            "loss: 1.210666  [   62/ 3200]\n",
            "loss: 1.216038  [   63/ 3200]\n",
            "loss: 1.230780  [   64/ 3200]\n",
            "loss: 1.126187  [   65/ 3200]\n",
            "loss: 1.046189  [   66/ 3200]\n",
            "loss: 1.260597  [   67/ 3200]\n",
            "loss: 1.280272  [   68/ 3200]\n",
            "loss: 1.163402  [   69/ 3200]\n",
            "loss: 1.150093  [   70/ 3200]\n",
            "loss: 1.301905  [   71/ 3200]\n",
            "loss: 1.110606  [   72/ 3200]\n",
            "loss: 1.153643  [   73/ 3200]\n",
            "loss: 1.251422  [   74/ 3200]\n",
            "loss: 1.227397  [   75/ 3200]\n",
            "loss: 1.129728  [   76/ 3200]\n",
            "loss: 1.237044  [   77/ 3200]\n",
            "loss: 1.170252  [   78/ 3200]\n",
            "loss: 1.232870  [   79/ 3200]\n",
            "loss: 1.184402  [   80/ 3200]\n",
            "loss: 1.213924  [   81/ 3200]\n",
            "loss: 1.132991  [   82/ 3200]\n",
            "loss: 1.188193  [   83/ 3200]\n",
            "loss: 1.267628  [   84/ 3200]\n",
            "loss: 1.121080  [   85/ 3200]\n",
            "loss: 1.164769  [   86/ 3200]\n",
            "loss: 1.186626  [   87/ 3200]\n",
            "loss: 1.069136  [   88/ 3200]\n",
            "loss: 1.246389  [   89/ 3200]\n",
            "loss: 1.217650  [   90/ 3200]\n",
            "loss: 1.224830  [   91/ 3200]\n",
            "loss: 1.243817  [   92/ 3200]\n",
            "loss: 1.306140  [   93/ 3200]\n",
            "loss: 1.137742  [   94/ 3200]\n",
            "loss: 1.228943  [   95/ 3200]\n",
            "loss: 1.209389  [   96/ 3200]\n",
            "loss: 1.145534  [   97/ 3200]\n",
            "loss: 1.176446  [   98/ 3200]\n",
            "loss: 1.170449  [   99/ 3200]\n",
            "loss: 1.167591  [  100/ 3200]\n",
            "loss: 1.253436  [  101/ 3200]\n",
            "loss: 1.232945  [  102/ 3200]\n",
            "loss: 1.167572  [  103/ 3200]\n",
            "loss: 1.265087  [  104/ 3200]\n",
            "loss: 1.177827  [  105/ 3200]\n",
            "loss: 1.168843  [  106/ 3200]\n",
            "loss: 1.233233  [  107/ 3200]\n",
            "loss: 1.242089  [  108/ 3200]\n",
            "loss: 1.220243  [  109/ 3200]\n",
            "loss: 1.260242  [  110/ 3200]\n",
            "loss: 1.186512  [  111/ 3200]\n",
            "loss: 1.200357  [  112/ 3200]\n",
            "loss: 1.270056  [  113/ 3200]\n",
            "loss: 1.190650  [  114/ 3200]\n",
            "loss: 1.280829  [  115/ 3200]\n",
            "loss: 1.138567  [  116/ 3200]\n",
            "loss: 1.131030  [  117/ 3200]\n",
            "loss: 1.301954  [  118/ 3200]\n",
            "loss: 1.149861  [  119/ 3200]\n",
            "loss: 1.181605  [  120/ 3200]\n",
            "loss: 1.160686  [  121/ 3200]\n",
            "loss: 1.175052  [  122/ 3200]\n",
            "loss: 1.254115  [  123/ 3200]\n",
            "loss: 1.087639  [  124/ 3200]\n",
            "loss: 1.260283  [  125/ 3200]\n",
            "loss: 1.184575  [  126/ 3200]\n",
            "loss: 1.186521  [  127/ 3200]\n",
            "loss: 1.357246  [  128/ 3200]\n",
            "loss: 1.156400  [  129/ 3200]\n",
            "loss: 1.205786  [  130/ 3200]\n",
            "loss: 1.168253  [  131/ 3200]\n",
            "loss: 1.267179  [  132/ 3200]\n",
            "loss: 1.115128  [  133/ 3200]\n",
            "loss: 1.266040  [  134/ 3200]\n",
            "loss: 1.177559  [  135/ 3200]\n",
            "loss: 1.142129  [  136/ 3200]\n",
            "loss: 1.268743  [  137/ 3200]\n",
            "loss: 1.273177  [  138/ 3200]\n",
            "loss: 1.351463  [  139/ 3200]\n",
            "loss: 1.312780  [  140/ 3200]\n",
            "loss: 1.214215  [  141/ 3200]\n",
            "loss: 1.192783  [  142/ 3200]\n",
            "loss: 1.201157  [  143/ 3200]\n",
            "loss: 1.225420  [  144/ 3200]\n",
            "loss: 1.172962  [  145/ 3200]\n",
            "loss: 1.243853  [  146/ 3200]\n",
            "loss: 1.254534  [  147/ 3200]\n",
            "loss: 1.183264  [  148/ 3200]\n",
            "loss: 1.250800  [  149/ 3200]\n",
            "loss: 1.176246  [  150/ 3200]\n",
            "loss: 1.130210  [  151/ 3200]\n",
            "loss: 1.162523  [  152/ 3200]\n",
            "loss: 1.226066  [  153/ 3200]\n",
            "loss: 1.181558  [  154/ 3200]\n",
            "loss: 1.205826  [  155/ 3200]\n",
            "loss: 1.287724  [  156/ 3200]\n",
            "loss: 1.221274  [  157/ 3200]\n",
            "loss: 1.175649  [  158/ 3200]\n",
            "loss: 1.257187  [  159/ 3200]\n",
            "loss: 1.285694  [  160/ 3200]\n",
            "loss: 1.208443  [  161/ 3200]\n",
            "loss: 1.143974  [  162/ 3200]\n",
            "loss: 1.118596  [  163/ 3200]\n",
            "loss: 1.269580  [  164/ 3200]\n",
            "loss: 1.138728  [  165/ 3200]\n",
            "loss: 1.188816  [  166/ 3200]\n",
            "loss: 1.246356  [  167/ 3200]\n",
            "loss: 1.130427  [  168/ 3200]\n",
            "loss: 1.229825  [  169/ 3200]\n",
            "loss: 1.134664  [  170/ 3200]\n",
            "loss: 1.247031  [  171/ 3200]\n",
            "loss: 1.220234  [  172/ 3200]\n",
            "loss: 1.140599  [  173/ 3200]\n",
            "loss: 1.112960  [  174/ 3200]\n",
            "loss: 1.197205  [  175/ 3200]\n",
            "loss: 1.341290  [  176/ 3200]\n",
            "loss: 1.113903  [  177/ 3200]\n",
            "loss: 1.185305  [  178/ 3200]\n",
            "loss: 1.190004  [  179/ 3200]\n",
            "loss: 1.125496  [  180/ 3200]\n",
            "loss: 1.229074  [  181/ 3200]\n",
            "loss: 1.216052  [  182/ 3200]\n",
            "loss: 1.204389  [  183/ 3200]\n",
            "loss: 1.078348  [  184/ 3200]\n",
            "loss: 1.221999  [  185/ 3200]\n",
            "loss: 1.285600  [  186/ 3200]\n",
            "loss: 1.179549  [  187/ 3200]\n",
            "loss: 1.005590  [  188/ 3200]\n",
            "loss: 1.240755  [  189/ 3200]\n",
            "loss: 1.322340  [  190/ 3200]\n",
            "loss: 1.268762  [  191/ 3200]\n",
            "loss: 1.189168  [  192/ 3200]\n",
            "loss: 1.095139  [  193/ 3200]\n",
            "loss: 1.274842  [  194/ 3200]\n",
            "loss: 1.183886  [  195/ 3200]\n",
            "loss: 1.142650  [  196/ 3200]\n",
            "loss: 1.350803  [  197/ 3200]\n",
            "loss: 1.067606  [  198/ 3200]\n",
            "loss: 1.179279  [  199/ 3200]\n",
            "Epoch:  15\n",
            "loss: 1.178941  [    0/ 3200]\n",
            "loss: 1.122853  [    1/ 3200]\n",
            "loss: 1.181607  [    2/ 3200]\n",
            "loss: 1.019343  [    3/ 3200]\n",
            "loss: 1.285800  [    4/ 3200]\n",
            "loss: 1.367122  [    5/ 3200]\n",
            "loss: 1.171605  [    6/ 3200]\n",
            "loss: 1.188449  [    7/ 3200]\n",
            "loss: 1.140843  [    8/ 3200]\n",
            "loss: 1.205408  [    9/ 3200]\n",
            "loss: 1.068505  [   10/ 3200]\n",
            "loss: 1.169472  [   11/ 3200]\n",
            "loss: 1.220771  [   12/ 3200]\n",
            "loss: 1.243085  [   13/ 3200]\n",
            "loss: 1.306280  [   14/ 3200]\n",
            "loss: 1.256752  [   15/ 3200]\n",
            "loss: 1.188668  [   16/ 3200]\n",
            "loss: 1.100138  [   17/ 3200]\n",
            "loss: 1.263895  [   18/ 3200]\n",
            "loss: 1.169691  [   19/ 3200]\n",
            "loss: 1.226521  [   20/ 3200]\n",
            "loss: 1.210431  [   21/ 3200]\n",
            "loss: 1.232856  [   22/ 3200]\n",
            "loss: 1.140688  [   23/ 3200]\n",
            "loss: 1.136789  [   24/ 3200]\n",
            "loss: 1.106146  [   25/ 3200]\n",
            "loss: 1.195135  [   26/ 3200]\n",
            "loss: 1.265355  [   27/ 3200]\n",
            "loss: 1.146729  [   28/ 3200]\n",
            "loss: 1.146152  [   29/ 3200]\n",
            "loss: 1.304011  [   30/ 3200]\n",
            "loss: 1.224669  [   31/ 3200]\n",
            "loss: 1.208761  [   32/ 3200]\n",
            "loss: 1.243719  [   33/ 3200]\n",
            "loss: 1.179959  [   34/ 3200]\n",
            "loss: 1.252127  [   35/ 3200]\n",
            "loss: 1.206776  [   36/ 3200]\n",
            "loss: 1.230175  [   37/ 3200]\n",
            "loss: 1.136182  [   38/ 3200]\n",
            "loss: 1.245447  [   39/ 3200]\n",
            "loss: 1.209309  [   40/ 3200]\n",
            "loss: 1.284679  [   41/ 3200]\n",
            "loss: 1.153497  [   42/ 3200]\n",
            "loss: 1.131433  [   43/ 3200]\n",
            "loss: 1.273080  [   44/ 3200]\n",
            "loss: 1.111001  [   45/ 3200]\n",
            "loss: 1.215787  [   46/ 3200]\n",
            "loss: 1.227252  [   47/ 3200]\n",
            "loss: 1.219436  [   48/ 3200]\n",
            "loss: 1.183030  [   49/ 3200]\n",
            "loss: 1.146662  [   50/ 3200]\n",
            "loss: 1.227037  [   51/ 3200]\n",
            "loss: 1.146343  [   52/ 3200]\n",
            "loss: 1.188520  [   53/ 3200]\n",
            "loss: 1.295067  [   54/ 3200]\n",
            "loss: 1.059076  [   55/ 3200]\n",
            "loss: 1.293714  [   56/ 3200]\n",
            "loss: 1.131565  [   57/ 3200]\n",
            "loss: 1.230726  [   58/ 3200]\n",
            "loss: 1.189600  [   59/ 3200]\n",
            "loss: 1.146760  [   60/ 3200]\n",
            "loss: 1.181174  [   61/ 3200]\n",
            "loss: 1.192461  [   62/ 3200]\n",
            "loss: 1.239123  [   63/ 3200]\n",
            "loss: 1.145665  [   64/ 3200]\n",
            "loss: 1.257507  [   65/ 3200]\n",
            "loss: 1.022267  [   66/ 3200]\n",
            "loss: 1.188784  [   67/ 3200]\n",
            "loss: 1.140173  [   68/ 3200]\n",
            "loss: 1.072731  [   69/ 3200]\n",
            "loss: 1.227531  [   70/ 3200]\n",
            "loss: 1.191839  [   71/ 3200]\n",
            "loss: 1.175411  [   72/ 3200]\n",
            "loss: 1.144106  [   73/ 3200]\n",
            "loss: 1.221888  [   74/ 3200]\n",
            "loss: 1.253631  [   75/ 3200]\n",
            "loss: 1.134325  [   76/ 3200]\n",
            "loss: 1.092533  [   77/ 3200]\n",
            "loss: 1.158707  [   78/ 3200]\n",
            "loss: 1.366560  [   79/ 3200]\n",
            "loss: 1.124184  [   80/ 3200]\n",
            "loss: 1.175885  [   81/ 3200]\n",
            "loss: 1.111453  [   82/ 3200]\n",
            "loss: 1.154661  [   83/ 3200]\n",
            "loss: 1.078591  [   84/ 3200]\n",
            "loss: 1.145856  [   85/ 3200]\n",
            "loss: 1.364269  [   86/ 3200]\n",
            "loss: 1.137877  [   87/ 3200]\n",
            "loss: 1.112002  [   88/ 3200]\n",
            "loss: 1.074038  [   89/ 3200]\n",
            "loss: 1.167443  [   90/ 3200]\n",
            "loss: 1.256641  [   91/ 3200]\n",
            "loss: 1.150281  [   92/ 3200]\n",
            "loss: 1.192967  [   93/ 3200]\n",
            "loss: 1.206328  [   94/ 3200]\n",
            "loss: 1.156587  [   95/ 3200]\n",
            "loss: 1.118470  [   96/ 3200]\n",
            "loss: 1.226994  [   97/ 3200]\n",
            "loss: 1.182387  [   98/ 3200]\n",
            "loss: 1.185978  [   99/ 3200]\n",
            "loss: 1.202794  [  100/ 3200]\n",
            "loss: 1.278455  [  101/ 3200]\n",
            "loss: 1.154215  [  102/ 3200]\n",
            "loss: 1.176677  [  103/ 3200]\n",
            "loss: 1.241340  [  104/ 3200]\n",
            "loss: 1.203351  [  105/ 3200]\n",
            "loss: 1.088626  [  106/ 3200]\n",
            "loss: 1.230673  [  107/ 3200]\n",
            "loss: 1.076087  [  108/ 3200]\n",
            "loss: 1.168291  [  109/ 3200]\n",
            "loss: 1.265201  [  110/ 3200]\n",
            "loss: 1.260613  [  111/ 3200]\n",
            "loss: 1.180170  [  112/ 3200]\n",
            "loss: 1.260354  [  113/ 3200]\n",
            "loss: 1.165575  [  114/ 3200]\n",
            "loss: 1.187235  [  115/ 3200]\n",
            "loss: 1.097595  [  116/ 3200]\n",
            "loss: 1.115184  [  117/ 3200]\n",
            "loss: 1.091107  [  118/ 3200]\n",
            "loss: 1.106781  [  119/ 3200]\n",
            "loss: 1.092456  [  120/ 3200]\n",
            "loss: 1.236156  [  121/ 3200]\n",
            "loss: 1.102397  [  122/ 3200]\n",
            "loss: 1.188123  [  123/ 3200]\n",
            "loss: 1.048480  [  124/ 3200]\n",
            "loss: 1.300395  [  125/ 3200]\n",
            "loss: 1.088230  [  126/ 3200]\n",
            "loss: 1.270517  [  127/ 3200]\n",
            "loss: 1.162698  [  128/ 3200]\n",
            "loss: 1.151311  [  129/ 3200]\n",
            "loss: 0.978439  [  130/ 3200]\n",
            "loss: 1.079618  [  131/ 3200]\n",
            "loss: 1.143684  [  132/ 3200]\n",
            "loss: 1.246950  [  133/ 3200]\n",
            "loss: 1.270769  [  134/ 3200]\n",
            "loss: 1.248273  [  135/ 3200]\n",
            "loss: 1.158415  [  136/ 3200]\n",
            "loss: 1.305146  [  137/ 3200]\n",
            "loss: 1.229207  [  138/ 3200]\n",
            "loss: 1.155200  [  139/ 3200]\n",
            "loss: 1.040750  [  140/ 3200]\n",
            "loss: 0.965206  [  141/ 3200]\n",
            "loss: 1.231336  [  142/ 3200]\n",
            "loss: 1.234484  [  143/ 3200]\n",
            "loss: 1.231909  [  144/ 3200]\n",
            "loss: 1.215673  [  145/ 3200]\n",
            "loss: 1.346954  [  146/ 3200]\n",
            "loss: 1.224545  [  147/ 3200]\n",
            "loss: 1.147840  [  148/ 3200]\n",
            "loss: 1.248354  [  149/ 3200]\n",
            "loss: 1.058835  [  150/ 3200]\n",
            "loss: 1.035274  [  151/ 3200]\n",
            "loss: 1.174945  [  152/ 3200]\n",
            "loss: 1.171573  [  153/ 3200]\n",
            "loss: 1.068942  [  154/ 3200]\n",
            "loss: 1.221636  [  155/ 3200]\n",
            "loss: 1.161280  [  156/ 3200]\n",
            "loss: 1.238443  [  157/ 3200]\n",
            "loss: 1.304235  [  158/ 3200]\n",
            "loss: 1.282362  [  159/ 3200]\n",
            "loss: 1.167715  [  160/ 3200]\n",
            "loss: 1.143047  [  161/ 3200]\n",
            "loss: 1.201976  [  162/ 3200]\n",
            "loss: 1.239222  [  163/ 3200]\n",
            "loss: 1.096910  [  164/ 3200]\n",
            "loss: 1.170148  [  165/ 3200]\n",
            "loss: 1.175643  [  166/ 3200]\n",
            "loss: 1.219567  [  167/ 3200]\n",
            "loss: 1.091326  [  168/ 3200]\n",
            "loss: 1.144726  [  169/ 3200]\n",
            "loss: 1.168629  [  170/ 3200]\n",
            "loss: 1.189232  [  171/ 3200]\n",
            "loss: 1.219447  [  172/ 3200]\n",
            "loss: 1.198006  [  173/ 3200]\n",
            "loss: 1.087655  [  174/ 3200]\n",
            "loss: 1.190237  [  175/ 3200]\n",
            "loss: 1.165979  [  176/ 3200]\n",
            "loss: 1.176407  [  177/ 3200]\n",
            "loss: 1.103462  [  178/ 3200]\n",
            "loss: 1.170363  [  179/ 3200]\n",
            "loss: 1.234020  [  180/ 3200]\n",
            "loss: 1.222141  [  181/ 3200]\n",
            "loss: 1.098230  [  182/ 3200]\n",
            "loss: 1.108405  [  183/ 3200]\n",
            "loss: 1.092281  [  184/ 3200]\n",
            "loss: 1.195214  [  185/ 3200]\n",
            "loss: 1.279912  [  186/ 3200]\n",
            "loss: 1.163034  [  187/ 3200]\n",
            "loss: 1.085296  [  188/ 3200]\n",
            "loss: 1.305103  [  189/ 3200]\n",
            "loss: 1.076988  [  190/ 3200]\n",
            "loss: 1.168000  [  191/ 3200]\n",
            "loss: 1.174868  [  192/ 3200]\n",
            "loss: 1.205825  [  193/ 3200]\n",
            "loss: 1.137009  [  194/ 3200]\n",
            "loss: 1.179076  [  195/ 3200]\n",
            "loss: 1.164573  [  196/ 3200]\n",
            "loss: 1.135809  [  197/ 3200]\n",
            "loss: 1.170911  [  198/ 3200]\n",
            "loss: 1.170153  [  199/ 3200]\n",
            "Epoch:  16\n",
            "loss: 1.227606  [    0/ 3200]\n",
            "loss: 1.145375  [    1/ 3200]\n",
            "loss: 1.167238  [    2/ 3200]\n",
            "loss: 1.105776  [    3/ 3200]\n",
            "loss: 1.098634  [    4/ 3200]\n",
            "loss: 1.253941  [    5/ 3200]\n",
            "loss: 1.048803  [    6/ 3200]\n",
            "loss: 1.200223  [    7/ 3200]\n",
            "loss: 1.247676  [    8/ 3200]\n",
            "loss: 1.085355  [    9/ 3200]\n",
            "loss: 1.134101  [   10/ 3200]\n",
            "loss: 1.166718  [   11/ 3200]\n",
            "loss: 1.144450  [   12/ 3200]\n",
            "loss: 1.170978  [   13/ 3200]\n",
            "loss: 1.193187  [   14/ 3200]\n",
            "loss: 1.266891  [   15/ 3200]\n",
            "loss: 1.255737  [   16/ 3200]\n",
            "loss: 1.089275  [   17/ 3200]\n",
            "loss: 1.065857  [   18/ 3200]\n",
            "loss: 1.072025  [   19/ 3200]\n",
            "loss: 1.261267  [   20/ 3200]\n",
            "loss: 1.102724  [   21/ 3200]\n",
            "loss: 1.034096  [   22/ 3200]\n",
            "loss: 1.177965  [   23/ 3200]\n",
            "loss: 1.238600  [   24/ 3200]\n",
            "loss: 1.174804  [   25/ 3200]\n",
            "loss: 1.205510  [   26/ 3200]\n",
            "loss: 1.206315  [   27/ 3200]\n",
            "loss: 1.152877  [   28/ 3200]\n",
            "loss: 1.064387  [   29/ 3200]\n",
            "loss: 1.061672  [   30/ 3200]\n",
            "loss: 1.094586  [   31/ 3200]\n",
            "loss: 1.141713  [   32/ 3200]\n",
            "loss: 1.195090  [   33/ 3200]\n",
            "loss: 1.053270  [   34/ 3200]\n",
            "loss: 1.169922  [   35/ 3200]\n",
            "loss: 1.129870  [   36/ 3200]\n",
            "loss: 1.130630  [   37/ 3200]\n",
            "loss: 1.132799  [   38/ 3200]\n",
            "loss: 1.115957  [   39/ 3200]\n",
            "loss: 1.187745  [   40/ 3200]\n",
            "loss: 1.153432  [   41/ 3200]\n",
            "loss: 1.080450  [   42/ 3200]\n",
            "loss: 1.171386  [   43/ 3200]\n",
            "loss: 1.132435  [   44/ 3200]\n",
            "loss: 1.198555  [   45/ 3200]\n",
            "loss: 1.224370  [   46/ 3200]\n",
            "loss: 1.164718  [   47/ 3200]\n",
            "loss: 1.174101  [   48/ 3200]\n",
            "loss: 1.171594  [   49/ 3200]\n",
            "loss: 1.060691  [   50/ 3200]\n",
            "loss: 1.179900  [   51/ 3200]\n",
            "loss: 1.148425  [   52/ 3200]\n",
            "loss: 1.175869  [   53/ 3200]\n",
            "loss: 1.330305  [   54/ 3200]\n",
            "loss: 1.094803  [   55/ 3200]\n",
            "loss: 1.205564  [   56/ 3200]\n",
            "loss: 1.056343  [   57/ 3200]\n",
            "loss: 1.195564  [   58/ 3200]\n",
            "loss: 1.103886  [   59/ 3200]\n",
            "loss: 1.106239  [   60/ 3200]\n",
            "loss: 1.089362  [   61/ 3200]\n",
            "loss: 1.085648  [   62/ 3200]\n",
            "loss: 1.219227  [   63/ 3200]\n",
            "loss: 1.160254  [   64/ 3200]\n",
            "loss: 1.128041  [   65/ 3200]\n",
            "loss: 1.113864  [   66/ 3200]\n",
            "loss: 1.169893  [   67/ 3200]\n",
            "loss: 1.269834  [   68/ 3200]\n",
            "loss: 1.073228  [   69/ 3200]\n",
            "loss: 1.275135  [   70/ 3200]\n",
            "loss: 1.057505  [   71/ 3200]\n",
            "loss: 1.208625  [   72/ 3200]\n",
            "loss: 1.295868  [   73/ 3200]\n",
            "loss: 1.157405  [   74/ 3200]\n",
            "loss: 1.199818  [   75/ 3200]\n",
            "loss: 1.293896  [   76/ 3200]\n",
            "loss: 1.285039  [   77/ 3200]\n",
            "loss: 1.151353  [   78/ 3200]\n",
            "loss: 1.122854  [   79/ 3200]\n",
            "loss: 1.095658  [   80/ 3200]\n",
            "loss: 1.146952  [   81/ 3200]\n",
            "loss: 1.170303  [   82/ 3200]\n",
            "loss: 1.140969  [   83/ 3200]\n",
            "loss: 1.038077  [   84/ 3200]\n",
            "loss: 0.871563  [   85/ 3200]\n",
            "loss: 1.144954  [   86/ 3200]\n",
            "loss: 1.340822  [   87/ 3200]\n",
            "loss: 0.963995  [   88/ 3200]\n",
            "loss: 1.154407  [   89/ 3200]\n",
            "loss: 1.120237  [   90/ 3200]\n",
            "loss: 1.146954  [   91/ 3200]\n",
            "loss: 1.174224  [   92/ 3200]\n",
            "loss: 1.072551  [   93/ 3200]\n",
            "loss: 1.189492  [   94/ 3200]\n",
            "loss: 1.152418  [   95/ 3200]\n",
            "loss: 1.169293  [   96/ 3200]\n",
            "loss: 1.166592  [   97/ 3200]\n",
            "loss: 1.170782  [   98/ 3200]\n",
            "loss: 1.123743  [   99/ 3200]\n",
            "loss: 1.228600  [  100/ 3200]\n",
            "loss: 1.200979  [  101/ 3200]\n",
            "loss: 1.065154  [  102/ 3200]\n",
            "loss: 1.328240  [  103/ 3200]\n",
            "loss: 1.029527  [  104/ 3200]\n",
            "loss: 1.265345  [  105/ 3200]\n",
            "loss: 1.175882  [  106/ 3200]\n",
            "loss: 1.017376  [  107/ 3200]\n",
            "loss: 1.255689  [  108/ 3200]\n",
            "loss: 1.123877  [  109/ 3200]\n",
            "loss: 1.136873  [  110/ 3200]\n",
            "loss: 1.152666  [  111/ 3200]\n",
            "loss: 1.118812  [  112/ 3200]\n",
            "loss: 1.170945  [  113/ 3200]\n",
            "loss: 1.144820  [  114/ 3200]\n",
            "loss: 1.171184  [  115/ 3200]\n",
            "loss: 1.083435  [  116/ 3200]\n",
            "loss: 1.242308  [  117/ 3200]\n",
            "loss: 1.293426  [  118/ 3200]\n",
            "loss: 1.176961  [  119/ 3200]\n",
            "loss: 1.268154  [  120/ 3200]\n",
            "loss: 1.198479  [  121/ 3200]\n",
            "loss: 1.044320  [  122/ 3200]\n",
            "loss: 1.220336  [  123/ 3200]\n",
            "loss: 1.288448  [  124/ 3200]\n",
            "loss: 1.232276  [  125/ 3200]\n",
            "loss: 1.072669  [  126/ 3200]\n",
            "loss: 1.185662  [  127/ 3200]\n",
            "loss: 1.160004  [  128/ 3200]\n",
            "loss: 1.180143  [  129/ 3200]\n",
            "loss: 1.197446  [  130/ 3200]\n",
            "loss: 1.038063  [  131/ 3200]\n",
            "loss: 1.204500  [  132/ 3200]\n",
            "loss: 1.167125  [  133/ 3200]\n",
            "loss: 1.125123  [  134/ 3200]\n",
            "loss: 1.034973  [  135/ 3200]\n",
            "loss: 1.220552  [  136/ 3200]\n",
            "loss: 0.806930  [  137/ 3200]\n",
            "loss: 1.378381  [  138/ 3200]\n",
            "loss: 1.054715  [  139/ 3200]\n",
            "loss: 1.069063  [  140/ 3200]\n",
            "loss: 1.228512  [  141/ 3200]\n",
            "loss: 1.306353  [  142/ 3200]\n",
            "loss: 1.154806  [  143/ 3200]\n",
            "loss: 1.140161  [  144/ 3200]\n",
            "loss: 1.210816  [  145/ 3200]\n",
            "loss: 1.244821  [  146/ 3200]\n",
            "loss: 1.099321  [  147/ 3200]\n",
            "loss: 1.172753  [  148/ 3200]\n",
            "loss: 1.144790  [  149/ 3200]\n",
            "loss: 1.128231  [  150/ 3200]\n",
            "loss: 1.231310  [  151/ 3200]\n",
            "loss: 1.082475  [  152/ 3200]\n",
            "loss: 1.138592  [  153/ 3200]\n",
            "loss: 1.227923  [  154/ 3200]\n",
            "loss: 1.108721  [  155/ 3200]\n",
            "loss: 1.123868  [  156/ 3200]\n",
            "loss: 1.270126  [  157/ 3200]\n",
            "loss: 1.326806  [  158/ 3200]\n",
            "loss: 1.173595  [  159/ 3200]\n",
            "loss: 1.185862  [  160/ 3200]\n",
            "loss: 1.116718  [  161/ 3200]\n",
            "loss: 1.219047  [  162/ 3200]\n",
            "loss: 1.199950  [  163/ 3200]\n",
            "loss: 1.207202  [  164/ 3200]\n",
            "loss: 1.178874  [  165/ 3200]\n",
            "loss: 1.040085  [  166/ 3200]\n",
            "loss: 1.207289  [  167/ 3200]\n",
            "loss: 1.283797  [  168/ 3200]\n",
            "loss: 1.119510  [  169/ 3200]\n",
            "loss: 1.069420  [  170/ 3200]\n",
            "loss: 1.278162  [  171/ 3200]\n",
            "loss: 1.124290  [  172/ 3200]\n",
            "loss: 1.230678  [  173/ 3200]\n",
            "loss: 1.187636  [  174/ 3200]\n",
            "loss: 1.231289  [  175/ 3200]\n",
            "loss: 1.201097  [  176/ 3200]\n",
            "loss: 1.120478  [  177/ 3200]\n",
            "loss: 1.171165  [  178/ 3200]\n",
            "loss: 1.137168  [  179/ 3200]\n",
            "loss: 1.236350  [  180/ 3200]\n",
            "loss: 1.020072  [  181/ 3200]\n",
            "loss: 1.051689  [  182/ 3200]\n",
            "loss: 1.267913  [  183/ 3200]\n",
            "loss: 1.215487  [  184/ 3200]\n",
            "loss: 1.032400  [  185/ 3200]\n",
            "loss: 0.944190  [  186/ 3200]\n",
            "loss: 0.926459  [  187/ 3200]\n",
            "loss: 0.920755  [  188/ 3200]\n",
            "loss: 1.424998  [  189/ 3200]\n",
            "loss: 0.971330  [  190/ 3200]\n",
            "loss: 0.977170  [  191/ 3200]\n",
            "loss: 1.106557  [  192/ 3200]\n",
            "loss: 1.172270  [  193/ 3200]\n",
            "loss: 1.322105  [  194/ 3200]\n",
            "loss: 1.137297  [  195/ 3200]\n",
            "loss: 1.052464  [  196/ 3200]\n",
            "loss: 1.195563  [  197/ 3200]\n",
            "loss: 1.166575  [  198/ 3200]\n",
            "loss: 1.294244  [  199/ 3200]\n",
            "Epoch:  17\n",
            "loss: 1.095995  [    0/ 3200]\n",
            "loss: 1.217188  [    1/ 3200]\n",
            "loss: 1.192956  [    2/ 3200]\n",
            "loss: 1.128476  [    3/ 3200]\n",
            "loss: 1.104793  [    4/ 3200]\n",
            "loss: 1.173041  [    5/ 3200]\n",
            "loss: 1.100593  [    6/ 3200]\n",
            "loss: 1.202421  [    7/ 3200]\n",
            "loss: 1.123314  [    8/ 3200]\n",
            "loss: 1.181277  [    9/ 3200]\n",
            "loss: 1.203606  [   10/ 3200]\n",
            "loss: 1.129555  [   11/ 3200]\n",
            "loss: 1.155805  [   12/ 3200]\n",
            "loss: 1.302692  [   13/ 3200]\n",
            "loss: 1.166875  [   14/ 3200]\n",
            "loss: 0.906733  [   15/ 3200]\n",
            "loss: 1.237393  [   16/ 3200]\n",
            "loss: 1.095027  [   17/ 3200]\n",
            "loss: 0.926636  [   18/ 3200]\n",
            "loss: 1.166099  [   19/ 3200]\n",
            "loss: 1.360974  [   20/ 3200]\n",
            "loss: 1.142319  [   21/ 3200]\n",
            "loss: 1.223727  [   22/ 3200]\n",
            "loss: 1.195637  [   23/ 3200]\n",
            "loss: 1.070122  [   24/ 3200]\n",
            "loss: 1.200100  [   25/ 3200]\n",
            "loss: 1.077353  [   26/ 3200]\n",
            "loss: 1.133186  [   27/ 3200]\n",
            "loss: 1.137479  [   28/ 3200]\n",
            "loss: 0.961600  [   29/ 3200]\n",
            "loss: 1.137139  [   30/ 3200]\n",
            "loss: 1.247344  [   31/ 3200]\n",
            "loss: 1.264114  [   32/ 3200]\n",
            "loss: 1.150848  [   33/ 3200]\n",
            "loss: 1.136097  [   34/ 3200]\n",
            "loss: 1.135915  [   35/ 3200]\n",
            "loss: 1.078836  [   36/ 3200]\n",
            "loss: 1.202698  [   37/ 3200]\n",
            "loss: 1.143104  [   38/ 3200]\n",
            "loss: 1.191388  [   39/ 3200]\n",
            "loss: 1.115884  [   40/ 3200]\n",
            "loss: 1.270637  [   41/ 3200]\n",
            "loss: 1.247141  [   42/ 3200]\n",
            "loss: 1.196551  [   43/ 3200]\n",
            "loss: 1.194848  [   44/ 3200]\n",
            "loss: 1.118754  [   45/ 3200]\n",
            "loss: 1.102516  [   46/ 3200]\n",
            "loss: 1.126714  [   47/ 3200]\n",
            "loss: 1.172935  [   48/ 3200]\n",
            "loss: 1.220521  [   49/ 3200]\n",
            "loss: 0.924911  [   50/ 3200]\n",
            "loss: 1.156368  [   51/ 3200]\n",
            "loss: 1.148824  [   52/ 3200]\n",
            "loss: 1.241243  [   53/ 3200]\n",
            "loss: 1.190600  [   54/ 3200]\n",
            "loss: 1.189280  [   55/ 3200]\n",
            "loss: 1.131890  [   56/ 3200]\n",
            "loss: 1.097184  [   57/ 3200]\n",
            "loss: 1.051495  [   58/ 3200]\n",
            "loss: 1.111687  [   59/ 3200]\n",
            "loss: 1.054586  [   60/ 3200]\n",
            "loss: 1.071373  [   61/ 3200]\n",
            "loss: 1.141062  [   62/ 3200]\n",
            "loss: 1.163373  [   63/ 3200]\n",
            "loss: 1.080808  [   64/ 3200]\n",
            "loss: 1.189146  [   65/ 3200]\n",
            "loss: 1.157172  [   66/ 3200]\n",
            "loss: 1.137503  [   67/ 3200]\n",
            "loss: 0.984078  [   68/ 3200]\n",
            "loss: 1.137082  [   69/ 3200]\n",
            "loss: 1.050790  [   70/ 3200]\n",
            "loss: 1.192500  [   71/ 3200]\n",
            "loss: 1.095940  [   72/ 3200]\n",
            "loss: 1.208137  [   73/ 3200]\n",
            "loss: 1.320486  [   74/ 3200]\n",
            "loss: 1.081386  [   75/ 3200]\n",
            "loss: 1.049462  [   76/ 3200]\n",
            "loss: 1.147409  [   77/ 3200]\n",
            "loss: 1.086535  [   78/ 3200]\n",
            "loss: 1.129112  [   79/ 3200]\n",
            "loss: 1.079892  [   80/ 3200]\n",
            "loss: 1.111844  [   81/ 3200]\n",
            "loss: 1.127326  [   82/ 3200]\n",
            "loss: 1.193686  [   83/ 3200]\n",
            "loss: 1.084620  [   84/ 3200]\n",
            "loss: 1.222946  [   85/ 3200]\n",
            "loss: 1.120449  [   86/ 3200]\n",
            "loss: 0.989947  [   87/ 3200]\n",
            "loss: 1.235204  [   88/ 3200]\n",
            "loss: 1.090789  [   89/ 3200]\n",
            "loss: 1.264320  [   90/ 3200]\n",
            "loss: 1.036997  [   91/ 3200]\n",
            "loss: 0.911743  [   92/ 3200]\n",
            "loss: 1.072000  [   93/ 3200]\n",
            "loss: 1.186582  [   94/ 3200]\n",
            "loss: 1.155380  [   95/ 3200]\n",
            "loss: 1.116050  [   96/ 3200]\n",
            "loss: 1.134650  [   97/ 3200]\n",
            "loss: 1.034588  [   98/ 3200]\n",
            "loss: 1.129125  [   99/ 3200]\n",
            "loss: 1.212291  [  100/ 3200]\n",
            "loss: 1.122074  [  101/ 3200]\n",
            "loss: 1.105625  [  102/ 3200]\n",
            "loss: 1.048720  [  103/ 3200]\n",
            "loss: 1.113900  [  104/ 3200]\n",
            "loss: 1.089729  [  105/ 3200]\n",
            "loss: 1.229208  [  106/ 3200]\n",
            "loss: 1.187661  [  107/ 3200]\n",
            "loss: 1.203049  [  108/ 3200]\n",
            "loss: 1.143635  [  109/ 3200]\n",
            "loss: 1.113073  [  110/ 3200]\n",
            "loss: 1.039207  [  111/ 3200]\n",
            "loss: 1.186609  [  112/ 3200]\n",
            "loss: 1.085808  [  113/ 3200]\n",
            "loss: 1.142660  [  114/ 3200]\n",
            "loss: 1.039780  [  115/ 3200]\n",
            "loss: 0.907062  [  116/ 3200]\n",
            "loss: 1.272967  [  117/ 3200]\n",
            "loss: 1.030845  [  118/ 3200]\n",
            "loss: 1.151705  [  119/ 3200]\n",
            "loss: 1.101288  [  120/ 3200]\n",
            "loss: 1.040106  [  121/ 3200]\n",
            "loss: 1.127129  [  122/ 3200]\n",
            "loss: 1.195756  [  123/ 3200]\n",
            "loss: 0.960144  [  124/ 3200]\n",
            "loss: 1.169971  [  125/ 3200]\n",
            "loss: 1.229576  [  126/ 3200]\n",
            "loss: 1.162496  [  127/ 3200]\n",
            "loss: 1.078376  [  128/ 3200]\n",
            "loss: 1.069280  [  129/ 3200]\n",
            "loss: 1.186177  [  130/ 3200]\n",
            "loss: 1.201544  [  131/ 3200]\n",
            "loss: 1.194515  [  132/ 3200]\n",
            "loss: 1.300851  [  133/ 3200]\n",
            "loss: 1.302053  [  134/ 3200]\n",
            "loss: 1.266488  [  135/ 3200]\n",
            "loss: 1.191595  [  136/ 3200]\n",
            "loss: 1.094482  [  137/ 3200]\n",
            "loss: 0.984504  [  138/ 3200]\n",
            "loss: 1.150630  [  139/ 3200]\n",
            "loss: 1.206331  [  140/ 3200]\n",
            "loss: 1.105561  [  141/ 3200]\n",
            "loss: 1.102531  [  142/ 3200]\n",
            "loss: 1.190223  [  143/ 3200]\n",
            "loss: 1.182998  [  144/ 3200]\n",
            "loss: 1.049905  [  145/ 3200]\n",
            "loss: 1.145048  [  146/ 3200]\n",
            "loss: 1.205927  [  147/ 3200]\n",
            "loss: 1.209419  [  148/ 3200]\n",
            "loss: 1.066647  [  149/ 3200]\n",
            "loss: 1.224115  [  150/ 3200]\n",
            "loss: 1.045034  [  151/ 3200]\n",
            "loss: 1.117960  [  152/ 3200]\n",
            "loss: 1.033733  [  153/ 3200]\n",
            "loss: 1.147318  [  154/ 3200]\n",
            "loss: 1.094457  [  155/ 3200]\n",
            "loss: 1.038214  [  156/ 3200]\n",
            "loss: 1.116930  [  157/ 3200]\n",
            "loss: 1.037123  [  158/ 3200]\n",
            "loss: 1.110009  [  159/ 3200]\n",
            "loss: 0.968327  [  160/ 3200]\n",
            "loss: 1.107795  [  161/ 3200]\n",
            "loss: 1.200111  [  162/ 3200]\n",
            "loss: 1.102991  [  163/ 3200]\n",
            "loss: 1.189542  [  164/ 3200]\n",
            "loss: 1.232475  [  165/ 3200]\n",
            "loss: 1.201784  [  166/ 3200]\n",
            "loss: 0.946278  [  167/ 3200]\n",
            "loss: 1.031661  [  168/ 3200]\n",
            "loss: 1.113265  [  169/ 3200]\n",
            "loss: 1.015656  [  170/ 3200]\n",
            "loss: 0.999794  [  171/ 3200]\n",
            "loss: 1.141856  [  172/ 3200]\n",
            "loss: 1.088732  [  173/ 3200]\n",
            "loss: 1.202123  [  174/ 3200]\n",
            "loss: 1.218284  [  175/ 3200]\n",
            "loss: 1.162861  [  176/ 3200]\n",
            "loss: 1.305457  [  177/ 3200]\n",
            "loss: 1.109839  [  178/ 3200]\n",
            "loss: 1.160949  [  179/ 3200]\n",
            "loss: 1.089610  [  180/ 3200]\n",
            "loss: 0.965781  [  181/ 3200]\n",
            "loss: 0.979961  [  182/ 3200]\n",
            "loss: 1.109786  [  183/ 3200]\n",
            "loss: 1.166461  [  184/ 3200]\n",
            "loss: 1.167213  [  185/ 3200]\n",
            "loss: 1.078800  [  186/ 3200]\n",
            "loss: 1.088256  [  187/ 3200]\n",
            "loss: 1.050718  [  188/ 3200]\n",
            "loss: 1.079429  [  189/ 3200]\n",
            "loss: 1.057331  [  190/ 3200]\n",
            "loss: 1.093511  [  191/ 3200]\n",
            "loss: 1.104147  [  192/ 3200]\n",
            "loss: 1.372991  [  193/ 3200]\n",
            "loss: 1.123279  [  194/ 3200]\n",
            "loss: 1.226015  [  195/ 3200]\n",
            "loss: 1.026251  [  196/ 3200]\n",
            "loss: 1.121489  [  197/ 3200]\n",
            "loss: 1.272098  [  198/ 3200]\n",
            "loss: 1.181231  [  199/ 3200]\n",
            "Epoch:  18\n",
            "loss: 1.231360  [    0/ 3200]\n",
            "loss: 1.148445  [    1/ 3200]\n",
            "loss: 1.126516  [    2/ 3200]\n",
            "loss: 1.249566  [    3/ 3200]\n",
            "loss: 1.121959  [    4/ 3200]\n",
            "loss: 1.338444  [    5/ 3200]\n",
            "loss: 1.074249  [    6/ 3200]\n",
            "loss: 1.087731  [    7/ 3200]\n",
            "loss: 1.058038  [    8/ 3200]\n",
            "loss: 1.036659  [    9/ 3200]\n",
            "loss: 1.055190  [   10/ 3200]\n",
            "loss: 1.022231  [   11/ 3200]\n",
            "loss: 1.115760  [   12/ 3200]\n",
            "loss: 0.927773  [   13/ 3200]\n",
            "loss: 1.001761  [   14/ 3200]\n",
            "loss: 1.070603  [   15/ 3200]\n",
            "loss: 1.141911  [   16/ 3200]\n",
            "loss: 1.110514  [   17/ 3200]\n",
            "loss: 1.213772  [   18/ 3200]\n",
            "loss: 1.153189  [   19/ 3200]\n",
            "loss: 1.001482  [   20/ 3200]\n",
            "loss: 1.197175  [   21/ 3200]\n",
            "loss: 0.933425  [   22/ 3200]\n",
            "loss: 1.022301  [   23/ 3200]\n",
            "loss: 1.186023  [   24/ 3200]\n",
            "loss: 1.082402  [   25/ 3200]\n",
            "loss: 0.959978  [   26/ 3200]\n",
            "loss: 1.122518  [   27/ 3200]\n",
            "loss: 1.181736  [   28/ 3200]\n",
            "loss: 1.161548  [   29/ 3200]\n",
            "loss: 1.147089  [   30/ 3200]\n",
            "loss: 1.122054  [   31/ 3200]\n",
            "loss: 1.001945  [   32/ 3200]\n",
            "loss: 1.085673  [   33/ 3200]\n",
            "loss: 1.149468  [   34/ 3200]\n",
            "loss: 1.106328  [   35/ 3200]\n",
            "loss: 0.964247  [   36/ 3200]\n",
            "loss: 1.077701  [   37/ 3200]\n",
            "loss: 1.076494  [   38/ 3200]\n",
            "loss: 1.069445  [   39/ 3200]\n",
            "loss: 1.188723  [   40/ 3200]\n",
            "loss: 1.093843  [   41/ 3200]\n",
            "loss: 1.165303  [   42/ 3200]\n",
            "loss: 1.036988  [   43/ 3200]\n",
            "loss: 1.158190  [   44/ 3200]\n",
            "loss: 1.081184  [   45/ 3200]\n",
            "loss: 1.074233  [   46/ 3200]\n",
            "loss: 1.113869  [   47/ 3200]\n",
            "loss: 1.249018  [   48/ 3200]\n",
            "loss: 0.981748  [   49/ 3200]\n",
            "loss: 1.267821  [   50/ 3200]\n",
            "loss: 1.206769  [   51/ 3200]\n",
            "loss: 1.134515  [   52/ 3200]\n",
            "loss: 1.270396  [   53/ 3200]\n",
            "loss: 1.281813  [   54/ 3200]\n",
            "loss: 1.136017  [   55/ 3200]\n",
            "loss: 1.213639  [   56/ 3200]\n",
            "loss: 1.198558  [   57/ 3200]\n",
            "loss: 1.106012  [   58/ 3200]\n",
            "loss: 1.366439  [   59/ 3200]\n",
            "loss: 1.338027  [   60/ 3200]\n",
            "loss: 1.047330  [   61/ 3200]\n",
            "loss: 1.091815  [   62/ 3200]\n",
            "loss: 1.103836  [   63/ 3200]\n",
            "loss: 1.291339  [   64/ 3200]\n",
            "loss: 1.056135  [   65/ 3200]\n",
            "loss: 1.101899  [   66/ 3200]\n",
            "loss: 1.146837  [   67/ 3200]\n",
            "loss: 1.048603  [   68/ 3200]\n",
            "loss: 1.014784  [   69/ 3200]\n",
            "loss: 1.115070  [   70/ 3200]\n",
            "loss: 1.111358  [   71/ 3200]\n",
            "loss: 1.010996  [   72/ 3200]\n",
            "loss: 1.103346  [   73/ 3200]\n",
            "loss: 0.935117  [   74/ 3200]\n",
            "loss: 1.145030  [   75/ 3200]\n",
            "loss: 1.248740  [   76/ 3200]\n",
            "loss: 1.297588  [   77/ 3200]\n",
            "loss: 1.304191  [   78/ 3200]\n",
            "loss: 1.170600  [   79/ 3200]\n",
            "loss: 1.066978  [   80/ 3200]\n",
            "loss: 0.957579  [   81/ 3200]\n",
            "loss: 1.002223  [   82/ 3200]\n",
            "loss: 1.070042  [   83/ 3200]\n",
            "loss: 1.259141  [   84/ 3200]\n",
            "loss: 1.079553  [   85/ 3200]\n",
            "loss: 1.068019  [   86/ 3200]\n",
            "loss: 1.000204  [   87/ 3200]\n",
            "loss: 1.059304  [   88/ 3200]\n",
            "loss: 1.061501  [   89/ 3200]\n",
            "loss: 1.257202  [   90/ 3200]\n",
            "loss: 1.127504  [   91/ 3200]\n",
            "loss: 1.152861  [   92/ 3200]\n",
            "loss: 1.181077  [   93/ 3200]\n",
            "loss: 1.034880  [   94/ 3200]\n",
            "loss: 1.209526  [   95/ 3200]\n",
            "loss: 1.109153  [   96/ 3200]\n",
            "loss: 1.113660  [   97/ 3200]\n",
            "loss: 1.226110  [   98/ 3200]\n",
            "loss: 1.053025  [   99/ 3200]\n",
            "loss: 0.971066  [  100/ 3200]\n",
            "loss: 1.090833  [  101/ 3200]\n",
            "loss: 1.039540  [  102/ 3200]\n",
            "loss: 1.032761  [  103/ 3200]\n",
            "loss: 1.182037  [  104/ 3200]\n",
            "loss: 1.023207  [  105/ 3200]\n",
            "loss: 1.167750  [  106/ 3200]\n",
            "loss: 1.079490  [  107/ 3200]\n",
            "loss: 0.997633  [  108/ 3200]\n",
            "loss: 1.165469  [  109/ 3200]\n",
            "loss: 1.124332  [  110/ 3200]\n",
            "loss: 1.004180  [  111/ 3200]\n",
            "loss: 1.035242  [  112/ 3200]\n",
            "loss: 1.181743  [  113/ 3200]\n",
            "loss: 1.159557  [  114/ 3200]\n",
            "loss: 1.089389  [  115/ 3200]\n",
            "loss: 1.042142  [  116/ 3200]\n",
            "loss: 1.034576  [  117/ 3200]\n",
            "loss: 1.109077  [  118/ 3200]\n",
            "loss: 1.029813  [  119/ 3200]\n",
            "loss: 1.038614  [  120/ 3200]\n",
            "loss: 1.350835  [  121/ 3200]\n",
            "loss: 0.970382  [  122/ 3200]\n",
            "loss: 1.068954  [  123/ 3200]\n",
            "loss: 1.052531  [  124/ 3200]\n",
            "loss: 1.135520  [  125/ 3200]\n",
            "loss: 1.093156  [  126/ 3200]\n",
            "loss: 1.037299  [  127/ 3200]\n",
            "loss: 1.249355  [  128/ 3200]\n",
            "loss: 1.102024  [  129/ 3200]\n",
            "loss: 1.056153  [  130/ 3200]\n",
            "loss: 1.148332  [  131/ 3200]\n",
            "loss: 1.252733  [  132/ 3200]\n",
            "loss: 1.236060  [  133/ 3200]\n",
            "loss: 1.006637  [  134/ 3200]\n",
            "loss: 1.033780  [  135/ 3200]\n",
            "loss: 1.155478  [  136/ 3200]\n",
            "loss: 1.036797  [  137/ 3200]\n",
            "loss: 1.072571  [  138/ 3200]\n",
            "loss: 0.994328  [  139/ 3200]\n",
            "loss: 1.139185  [  140/ 3200]\n",
            "loss: 1.113087  [  141/ 3200]\n",
            "loss: 1.245286  [  142/ 3200]\n",
            "loss: 1.166101  [  143/ 3200]\n",
            "loss: 1.256474  [  144/ 3200]\n",
            "loss: 1.137446  [  145/ 3200]\n",
            "loss: 1.122438  [  146/ 3200]\n",
            "loss: 1.094065  [  147/ 3200]\n",
            "loss: 1.138068  [  148/ 3200]\n",
            "loss: 0.986629  [  149/ 3200]\n",
            "loss: 1.130227  [  150/ 3200]\n",
            "loss: 1.182420  [  151/ 3200]\n",
            "loss: 0.865110  [  152/ 3200]\n",
            "loss: 0.983669  [  153/ 3200]\n",
            "loss: 1.163622  [  154/ 3200]\n",
            "loss: 1.286484  [  155/ 3200]\n",
            "loss: 1.209447  [  156/ 3200]\n",
            "loss: 1.052461  [  157/ 3200]\n",
            "loss: 1.149778  [  158/ 3200]\n",
            "loss: 0.934590  [  159/ 3200]\n",
            "loss: 1.194750  [  160/ 3200]\n",
            "loss: 1.052290  [  161/ 3200]\n",
            "loss: 1.196822  [  162/ 3200]\n",
            "loss: 1.080755  [  163/ 3200]\n",
            "loss: 0.936564  [  164/ 3200]\n",
            "loss: 1.170615  [  165/ 3200]\n",
            "loss: 1.099272  [  166/ 3200]\n",
            "loss: 1.158256  [  167/ 3200]\n",
            "loss: 0.995846  [  168/ 3200]\n",
            "loss: 1.200138  [  169/ 3200]\n",
            "loss: 1.010719  [  170/ 3200]\n",
            "loss: 1.238223  [  171/ 3200]\n",
            "loss: 1.031392  [  172/ 3200]\n",
            "loss: 1.185546  [  173/ 3200]\n",
            "loss: 1.099602  [  174/ 3200]\n",
            "loss: 1.139338  [  175/ 3200]\n",
            "loss: 1.044569  [  176/ 3200]\n",
            "loss: 1.053623  [  177/ 3200]\n",
            "loss: 1.032424  [  178/ 3200]\n",
            "loss: 1.172384  [  179/ 3200]\n",
            "loss: 1.286190  [  180/ 3200]\n",
            "loss: 1.146402  [  181/ 3200]\n",
            "loss: 1.150369  [  182/ 3200]\n",
            "loss: 1.193915  [  183/ 3200]\n",
            "loss: 1.004571  [  184/ 3200]\n",
            "loss: 1.177904  [  185/ 3200]\n",
            "loss: 1.055402  [  186/ 3200]\n",
            "loss: 1.027039  [  187/ 3200]\n",
            "loss: 1.063555  [  188/ 3200]\n",
            "loss: 1.119443  [  189/ 3200]\n",
            "loss: 1.160222  [  190/ 3200]\n",
            "loss: 0.916130  [  191/ 3200]\n",
            "loss: 1.072432  [  192/ 3200]\n",
            "loss: 1.164940  [  193/ 3200]\n",
            "loss: 1.304470  [  194/ 3200]\n",
            "loss: 1.083151  [  195/ 3200]\n",
            "loss: 0.960772  [  196/ 3200]\n",
            "loss: 1.190143  [  197/ 3200]\n",
            "loss: 0.991060  [  198/ 3200]\n",
            "loss: 1.110043  [  199/ 3200]\n",
            "Epoch:  19\n",
            "loss: 1.010195  [    0/ 3200]\n",
            "loss: 1.028015  [    1/ 3200]\n",
            "loss: 1.173630  [    2/ 3200]\n",
            "loss: 1.038401  [    3/ 3200]\n",
            "loss: 1.099759  [    4/ 3200]\n",
            "loss: 0.951577  [    5/ 3200]\n",
            "loss: 1.009632  [    6/ 3200]\n",
            "loss: 1.174327  [    7/ 3200]\n",
            "loss: 1.266254  [    8/ 3200]\n",
            "loss: 1.124524  [    9/ 3200]\n",
            "loss: 1.087775  [   10/ 3200]\n",
            "loss: 1.075119  [   11/ 3200]\n",
            "loss: 1.077512  [   12/ 3200]\n",
            "loss: 1.023512  [   13/ 3200]\n",
            "loss: 1.100018  [   14/ 3200]\n",
            "loss: 1.099156  [   15/ 3200]\n",
            "loss: 1.157787  [   16/ 3200]\n",
            "loss: 0.993488  [   17/ 3200]\n",
            "loss: 1.218728  [   18/ 3200]\n",
            "loss: 1.121411  [   19/ 3200]\n",
            "loss: 1.050964  [   20/ 3200]\n",
            "loss: 1.214811  [   21/ 3200]\n",
            "loss: 1.126917  [   22/ 3200]\n",
            "loss: 1.049119  [   23/ 3200]\n",
            "loss: 1.217110  [   24/ 3200]\n",
            "loss: 0.860359  [   25/ 3200]\n",
            "loss: 1.146602  [   26/ 3200]\n",
            "loss: 1.103309  [   27/ 3200]\n",
            "loss: 0.952055  [   28/ 3200]\n",
            "loss: 1.057600  [   29/ 3200]\n",
            "loss: 1.211675  [   30/ 3200]\n",
            "loss: 1.193991  [   31/ 3200]\n",
            "loss: 1.220263  [   32/ 3200]\n",
            "loss: 1.231043  [   33/ 3200]\n",
            "loss: 1.064770  [   34/ 3200]\n",
            "loss: 1.252341  [   35/ 3200]\n",
            "loss: 1.009375  [   36/ 3200]\n",
            "loss: 1.135939  [   37/ 3200]\n",
            "loss: 0.954143  [   38/ 3200]\n",
            "loss: 1.211749  [   39/ 3200]\n",
            "loss: 1.057580  [   40/ 3200]\n",
            "loss: 1.130189  [   41/ 3200]\n",
            "loss: 1.044024  [   42/ 3200]\n",
            "loss: 1.111857  [   43/ 3200]\n",
            "loss: 1.046646  [   44/ 3200]\n",
            "loss: 1.060302  [   45/ 3200]\n",
            "loss: 1.046036  [   46/ 3200]\n",
            "loss: 1.200088  [   47/ 3200]\n",
            "loss: 1.130186  [   48/ 3200]\n",
            "loss: 1.112971  [   49/ 3200]\n",
            "loss: 1.166110  [   50/ 3200]\n",
            "loss: 0.922619  [   51/ 3200]\n",
            "loss: 1.222118  [   52/ 3200]\n",
            "loss: 1.064636  [   53/ 3200]\n",
            "loss: 1.085275  [   54/ 3200]\n",
            "loss: 1.026686  [   55/ 3200]\n",
            "loss: 1.089179  [   56/ 3200]\n",
            "loss: 1.010448  [   57/ 3200]\n",
            "loss: 1.093413  [   58/ 3200]\n",
            "loss: 1.189302  [   59/ 3200]\n",
            "loss: 1.160200  [   60/ 3200]\n",
            "loss: 1.012700  [   61/ 3200]\n",
            "loss: 1.141429  [   62/ 3200]\n",
            "loss: 1.122962  [   63/ 3200]\n",
            "loss: 1.276647  [   64/ 3200]\n",
            "loss: 1.062754  [   65/ 3200]\n",
            "loss: 1.167916  [   66/ 3200]\n",
            "loss: 1.158593  [   67/ 3200]\n",
            "loss: 1.086724  [   68/ 3200]\n",
            "loss: 1.196498  [   69/ 3200]\n",
            "loss: 1.156170  [   70/ 3200]\n",
            "loss: 1.199770  [   71/ 3200]\n",
            "loss: 1.023432  [   72/ 3200]\n",
            "loss: 1.042965  [   73/ 3200]\n",
            "loss: 1.000692  [   74/ 3200]\n",
            "loss: 1.084538  [   75/ 3200]\n",
            "loss: 1.211650  [   76/ 3200]\n",
            "loss: 1.137864  [   77/ 3200]\n",
            "loss: 1.084549  [   78/ 3200]\n",
            "loss: 1.158961  [   79/ 3200]\n",
            "loss: 1.262707  [   80/ 3200]\n",
            "loss: 0.991994  [   81/ 3200]\n",
            "loss: 1.117878  [   82/ 3200]\n",
            "loss: 1.242935  [   83/ 3200]\n",
            "loss: 0.886228  [   84/ 3200]\n",
            "loss: 0.947826  [   85/ 3200]\n",
            "loss: 1.178137  [   86/ 3200]\n",
            "loss: 0.880308  [   87/ 3200]\n",
            "loss: 1.201920  [   88/ 3200]\n",
            "loss: 0.962300  [   89/ 3200]\n",
            "loss: 1.094793  [   90/ 3200]\n",
            "loss: 1.008373  [   91/ 3200]\n",
            "loss: 0.971689  [   92/ 3200]\n",
            "loss: 1.069544  [   93/ 3200]\n",
            "loss: 1.057650  [   94/ 3200]\n",
            "loss: 1.124273  [   95/ 3200]\n",
            "loss: 1.055731  [   96/ 3200]\n",
            "loss: 0.979247  [   97/ 3200]\n",
            "loss: 1.178218  [   98/ 3200]\n",
            "loss: 1.009361  [   99/ 3200]\n",
            "loss: 1.205045  [  100/ 3200]\n",
            "loss: 0.881937  [  101/ 3200]\n",
            "loss: 1.120039  [  102/ 3200]\n",
            "loss: 1.092690  [  103/ 3200]\n",
            "loss: 1.101153  [  104/ 3200]\n",
            "loss: 1.236694  [  105/ 3200]\n",
            "loss: 0.968637  [  106/ 3200]\n",
            "loss: 1.026960  [  107/ 3200]\n",
            "loss: 1.173594  [  108/ 3200]\n",
            "loss: 1.110199  [  109/ 3200]\n",
            "loss: 0.994485  [  110/ 3200]\n",
            "loss: 0.980911  [  111/ 3200]\n",
            "loss: 0.894065  [  112/ 3200]\n",
            "loss: 1.004282  [  113/ 3200]\n",
            "loss: 0.850597  [  114/ 3200]\n",
            "loss: 1.079597  [  115/ 3200]\n",
            "loss: 1.137108  [  116/ 3200]\n",
            "loss: 1.004160  [  117/ 3200]\n",
            "loss: 1.097681  [  118/ 3200]\n",
            "loss: 1.099440  [  119/ 3200]\n",
            "loss: 1.165711  [  120/ 3200]\n",
            "loss: 1.067489  [  121/ 3200]\n",
            "loss: 1.045033  [  122/ 3200]\n",
            "loss: 1.051650  [  123/ 3200]\n",
            "loss: 1.083103  [  124/ 3200]\n",
            "loss: 0.961149  [  125/ 3200]\n",
            "loss: 1.133344  [  126/ 3200]\n",
            "loss: 1.100280  [  127/ 3200]\n",
            "loss: 1.244259  [  128/ 3200]\n",
            "loss: 1.008270  [  129/ 3200]\n",
            "loss: 1.136406  [  130/ 3200]\n",
            "loss: 1.175730  [  131/ 3200]\n",
            "loss: 0.978007  [  132/ 3200]\n",
            "loss: 1.088994  [  133/ 3200]\n",
            "loss: 1.430030  [  134/ 3200]\n",
            "loss: 1.008705  [  135/ 3200]\n",
            "loss: 1.092579  [  136/ 3200]\n",
            "loss: 1.050441  [  137/ 3200]\n",
            "loss: 1.198197  [  138/ 3200]\n",
            "loss: 1.108489  [  139/ 3200]\n",
            "loss: 1.004605  [  140/ 3200]\n",
            "loss: 1.126973  [  141/ 3200]\n",
            "loss: 1.142990  [  142/ 3200]\n",
            "loss: 1.042922  [  143/ 3200]\n",
            "loss: 1.100673  [  144/ 3200]\n",
            "loss: 0.905995  [  145/ 3200]\n",
            "loss: 0.961492  [  146/ 3200]\n",
            "loss: 1.059706  [  147/ 3200]\n",
            "loss: 1.264207  [  148/ 3200]\n",
            "loss: 1.088065  [  149/ 3200]\n",
            "loss: 1.257483  [  150/ 3200]\n",
            "loss: 1.103226  [  151/ 3200]\n",
            "loss: 0.970428  [  152/ 3200]\n",
            "loss: 1.230535  [  153/ 3200]\n",
            "loss: 0.982764  [  154/ 3200]\n",
            "loss: 1.104698  [  155/ 3200]\n",
            "loss: 0.939614  [  156/ 3200]\n",
            "loss: 0.860405  [  157/ 3200]\n",
            "loss: 1.108121  [  158/ 3200]\n",
            "loss: 1.126740  [  159/ 3200]\n",
            "loss: 1.031011  [  160/ 3200]\n",
            "loss: 0.956796  [  161/ 3200]\n",
            "loss: 1.055379  [  162/ 3200]\n",
            "loss: 1.156801  [  163/ 3200]\n",
            "loss: 1.175001  [  164/ 3200]\n",
            "loss: 1.065926  [  165/ 3200]\n",
            "loss: 1.046855  [  166/ 3200]\n",
            "loss: 1.057864  [  167/ 3200]\n",
            "loss: 1.151788  [  168/ 3200]\n",
            "loss: 1.045781  [  169/ 3200]\n",
            "loss: 1.207531  [  170/ 3200]\n",
            "loss: 1.135198  [  171/ 3200]\n",
            "loss: 1.162437  [  172/ 3200]\n",
            "loss: 0.921765  [  173/ 3200]\n",
            "loss: 0.958280  [  174/ 3200]\n",
            "loss: 0.947128  [  175/ 3200]\n",
            "loss: 1.243974  [  176/ 3200]\n",
            "loss: 1.060205  [  177/ 3200]\n",
            "loss: 1.107172  [  178/ 3200]\n",
            "loss: 1.001192  [  179/ 3200]\n",
            "loss: 1.097436  [  180/ 3200]\n",
            "loss: 0.960726  [  181/ 3200]\n",
            "loss: 1.064018  [  182/ 3200]\n",
            "loss: 1.057014  [  183/ 3200]\n",
            "loss: 1.010861  [  184/ 3200]\n",
            "loss: 1.114904  [  185/ 3200]\n",
            "loss: 1.007511  [  186/ 3200]\n",
            "loss: 1.028375  [  187/ 3200]\n",
            "loss: 0.991274  [  188/ 3200]\n",
            "loss: 1.024392  [  189/ 3200]\n",
            "loss: 1.163048  [  190/ 3200]\n",
            "loss: 0.939691  [  191/ 3200]\n",
            "loss: 0.842601  [  192/ 3200]\n",
            "loss: 0.948662  [  193/ 3200]\n",
            "loss: 1.249122  [  194/ 3200]\n",
            "loss: 1.252216  [  195/ 3200]\n",
            "loss: 1.169379  [  196/ 3200]\n",
            "loss: 0.972071  [  197/ 3200]\n",
            "loss: 1.215410  [  198/ 3200]\n",
            "loss: 1.017134  [  199/ 3200]\n",
            "Epoch:  20\n",
            "loss: 0.943227  [    0/ 3200]\n",
            "loss: 1.072397  [    1/ 3200]\n",
            "loss: 1.170411  [    2/ 3200]\n",
            "loss: 1.173039  [    3/ 3200]\n",
            "loss: 1.174220  [    4/ 3200]\n",
            "loss: 0.972678  [    5/ 3200]\n",
            "loss: 1.112059  [    6/ 3200]\n",
            "loss: 0.934191  [    7/ 3200]\n",
            "loss: 0.849413  [    8/ 3200]\n",
            "loss: 0.883387  [    9/ 3200]\n",
            "loss: 1.217541  [   10/ 3200]\n",
            "loss: 1.008072  [   11/ 3200]\n",
            "loss: 0.999105  [   12/ 3200]\n",
            "loss: 1.251826  [   13/ 3200]\n",
            "loss: 1.157655  [   14/ 3200]\n",
            "loss: 0.955401  [   15/ 3200]\n",
            "loss: 1.127468  [   16/ 3200]\n",
            "loss: 1.256687  [   17/ 3200]\n",
            "loss: 1.058768  [   18/ 3200]\n",
            "loss: 1.014152  [   19/ 3200]\n",
            "loss: 0.893246  [   20/ 3200]\n",
            "loss: 1.059625  [   21/ 3200]\n",
            "loss: 1.039729  [   22/ 3200]\n",
            "loss: 1.131720  [   23/ 3200]\n",
            "loss: 0.966946  [   24/ 3200]\n",
            "loss: 1.196837  [   25/ 3200]\n",
            "loss: 1.040931  [   26/ 3200]\n",
            "loss: 1.049507  [   27/ 3200]\n",
            "loss: 0.970115  [   28/ 3200]\n",
            "loss: 0.945726  [   29/ 3200]\n",
            "loss: 1.086944  [   30/ 3200]\n",
            "loss: 1.026297  [   31/ 3200]\n",
            "loss: 0.974303  [   32/ 3200]\n",
            "loss: 1.311437  [   33/ 3200]\n",
            "loss: 1.100452  [   34/ 3200]\n",
            "loss: 1.036553  [   35/ 3200]\n",
            "loss: 0.979058  [   36/ 3200]\n",
            "loss: 1.101508  [   37/ 3200]\n",
            "loss: 1.074604  [   38/ 3200]\n",
            "loss: 0.978747  [   39/ 3200]\n",
            "loss: 0.919524  [   40/ 3200]\n",
            "loss: 0.878323  [   41/ 3200]\n",
            "loss: 0.917052  [   42/ 3200]\n",
            "loss: 1.189128  [   43/ 3200]\n",
            "loss: 0.929939  [   44/ 3200]\n",
            "loss: 0.986075  [   45/ 3200]\n",
            "loss: 0.931988  [   46/ 3200]\n",
            "loss: 1.213637  [   47/ 3200]\n",
            "loss: 1.088552  [   48/ 3200]\n",
            "loss: 0.985838  [   49/ 3200]\n",
            "loss: 0.977204  [   50/ 3200]\n",
            "loss: 1.062515  [   51/ 3200]\n",
            "loss: 1.065575  [   52/ 3200]\n",
            "loss: 0.986839  [   53/ 3200]\n",
            "loss: 0.914970  [   54/ 3200]\n",
            "loss: 0.970940  [   55/ 3200]\n",
            "loss: 1.114459  [   56/ 3200]\n",
            "loss: 1.169803  [   57/ 3200]\n",
            "loss: 1.065574  [   58/ 3200]\n",
            "loss: 1.363562  [   59/ 3200]\n",
            "loss: 1.128823  [   60/ 3200]\n",
            "loss: 1.087236  [   61/ 3200]\n",
            "loss: 1.131609  [   62/ 3200]\n",
            "loss: 1.128075  [   63/ 3200]\n",
            "loss: 1.178942  [   64/ 3200]\n",
            "loss: 0.978240  [   65/ 3200]\n",
            "loss: 1.045243  [   66/ 3200]\n",
            "loss: 0.948563  [   67/ 3200]\n",
            "loss: 1.072951  [   68/ 3200]\n",
            "loss: 0.994786  [   69/ 3200]\n",
            "loss: 0.978052  [   70/ 3200]\n",
            "loss: 0.970099  [   71/ 3200]\n",
            "loss: 1.040098  [   72/ 3200]\n",
            "loss: 1.041875  [   73/ 3200]\n",
            "loss: 1.046591  [   74/ 3200]\n",
            "loss: 1.054314  [   75/ 3200]\n",
            "loss: 0.995089  [   76/ 3200]\n",
            "loss: 1.267063  [   77/ 3200]\n",
            "loss: 1.025022  [   78/ 3200]\n",
            "loss: 1.186975  [   79/ 3200]\n",
            "loss: 1.215164  [   80/ 3200]\n",
            "loss: 1.145962  [   81/ 3200]\n",
            "loss: 0.991896  [   82/ 3200]\n",
            "loss: 0.937330  [   83/ 3200]\n",
            "loss: 0.990861  [   84/ 3200]\n",
            "loss: 1.066769  [   85/ 3200]\n",
            "loss: 1.104122  [   86/ 3200]\n",
            "loss: 1.215569  [   87/ 3200]\n",
            "loss: 0.997607  [   88/ 3200]\n",
            "loss: 1.036343  [   89/ 3200]\n",
            "loss: 1.078385  [   90/ 3200]\n",
            "loss: 1.119723  [   91/ 3200]\n",
            "loss: 1.018454  [   92/ 3200]\n",
            "loss: 0.950442  [   93/ 3200]\n",
            "loss: 1.164550  [   94/ 3200]\n",
            "loss: 1.000367  [   95/ 3200]\n",
            "loss: 1.121533  [   96/ 3200]\n",
            "loss: 1.075412  [   97/ 3200]\n",
            "loss: 1.243908  [   98/ 3200]\n",
            "loss: 1.043095  [   99/ 3200]\n",
            "loss: 0.888217  [  100/ 3200]\n",
            "loss: 0.866895  [  101/ 3200]\n",
            "loss: 0.990812  [  102/ 3200]\n",
            "loss: 0.964198  [  103/ 3200]\n",
            "loss: 1.241851  [  104/ 3200]\n",
            "loss: 0.925157  [  105/ 3200]\n",
            "loss: 1.174878  [  106/ 3200]\n",
            "loss: 0.943960  [  107/ 3200]\n",
            "loss: 1.102047  [  108/ 3200]\n",
            "loss: 1.265100  [  109/ 3200]\n",
            "loss: 1.124258  [  110/ 3200]\n",
            "loss: 1.144137  [  111/ 3200]\n",
            "loss: 1.210161  [  112/ 3200]\n",
            "loss: 1.140876  [  113/ 3200]\n",
            "loss: 1.095640  [  114/ 3200]\n",
            "loss: 1.061190  [  115/ 3200]\n",
            "loss: 1.098067  [  116/ 3200]\n",
            "loss: 0.886087  [  117/ 3200]\n",
            "loss: 1.225220  [  118/ 3200]\n",
            "loss: 1.027922  [  119/ 3200]\n",
            "loss: 1.043811  [  120/ 3200]\n",
            "loss: 0.853013  [  121/ 3200]\n",
            "loss: 1.242336  [  122/ 3200]\n",
            "loss: 1.115140  [  123/ 3200]\n",
            "loss: 1.007461  [  124/ 3200]\n",
            "loss: 1.127726  [  125/ 3200]\n",
            "loss: 1.047934  [  126/ 3200]\n",
            "loss: 0.954348  [  127/ 3200]\n",
            "loss: 1.056989  [  128/ 3200]\n",
            "loss: 1.121564  [  129/ 3200]\n",
            "loss: 1.129123  [  130/ 3200]\n",
            "loss: 1.127897  [  131/ 3200]\n",
            "loss: 0.997098  [  132/ 3200]\n",
            "loss: 1.033610  [  133/ 3200]\n",
            "loss: 1.287644  [  134/ 3200]\n",
            "loss: 1.253418  [  135/ 3200]\n",
            "loss: 1.105575  [  136/ 3200]\n",
            "loss: 0.884217  [  137/ 3200]\n",
            "loss: 1.172571  [  138/ 3200]\n",
            "loss: 1.105176  [  139/ 3200]\n",
            "loss: 1.118293  [  140/ 3200]\n",
            "loss: 0.920583  [  141/ 3200]\n",
            "loss: 1.116555  [  142/ 3200]\n",
            "loss: 1.071522  [  143/ 3200]\n",
            "loss: 1.164669  [  144/ 3200]\n",
            "loss: 1.199379  [  145/ 3200]\n",
            "loss: 1.079385  [  146/ 3200]\n",
            "loss: 1.185076  [  147/ 3200]\n",
            "loss: 0.980423  [  148/ 3200]\n",
            "loss: 1.073784  [  149/ 3200]\n",
            "loss: 1.025163  [  150/ 3200]\n",
            "loss: 1.125608  [  151/ 3200]\n",
            "loss: 0.976343  [  152/ 3200]\n",
            "loss: 1.254444  [  153/ 3200]\n",
            "loss: 1.084337  [  154/ 3200]\n",
            "loss: 0.987555  [  155/ 3200]\n",
            "loss: 1.319238  [  156/ 3200]\n",
            "loss: 0.913705  [  157/ 3200]\n",
            "loss: 1.083984  [  158/ 3200]\n",
            "loss: 0.936383  [  159/ 3200]\n",
            "loss: 1.042528  [  160/ 3200]\n",
            "loss: 0.982266  [  161/ 3200]\n",
            "loss: 0.998616  [  162/ 3200]\n",
            "loss: 1.194280  [  163/ 3200]\n",
            "loss: 1.160391  [  164/ 3200]\n",
            "loss: 1.099483  [  165/ 3200]\n",
            "loss: 1.013012  [  166/ 3200]\n",
            "loss: 1.191738  [  167/ 3200]\n",
            "loss: 0.957395  [  168/ 3200]\n",
            "loss: 0.925366  [  169/ 3200]\n",
            "loss: 1.146467  [  170/ 3200]\n",
            "loss: 1.035201  [  171/ 3200]\n",
            "loss: 0.987886  [  172/ 3200]\n",
            "loss: 1.130968  [  173/ 3200]\n",
            "loss: 0.895732  [  174/ 3200]\n",
            "loss: 0.941770  [  175/ 3200]\n",
            "loss: 0.905098  [  176/ 3200]\n",
            "loss: 1.105585  [  177/ 3200]\n",
            "loss: 0.899716  [  178/ 3200]\n",
            "loss: 1.140751  [  179/ 3200]\n",
            "loss: 1.283366  [  180/ 3200]\n",
            "loss: 1.002115  [  181/ 3200]\n",
            "loss: 1.065631  [  182/ 3200]\n",
            "loss: 1.129818  [  183/ 3200]\n",
            "loss: 0.996236  [  184/ 3200]\n",
            "loss: 1.089571  [  185/ 3200]\n",
            "loss: 1.095356  [  186/ 3200]\n",
            "loss: 1.128726  [  187/ 3200]\n",
            "loss: 1.039954  [  188/ 3200]\n",
            "loss: 1.114013  [  189/ 3200]\n",
            "loss: 0.939174  [  190/ 3200]\n",
            "loss: 1.406172  [  191/ 3200]\n",
            "loss: 1.150127  [  192/ 3200]\n",
            "loss: 1.101188  [  193/ 3200]\n",
            "loss: 1.115025  [  194/ 3200]\n",
            "loss: 0.936305  [  195/ 3200]\n",
            "loss: 1.199286  [  196/ 3200]\n",
            "loss: 1.039034  [  197/ 3200]\n",
            "loss: 1.081105  [  198/ 3200]\n",
            "loss: 1.020642  [  199/ 3200]\n",
            "Epoch:  21\n",
            "loss: 1.012709  [    0/ 3200]\n",
            "loss: 0.847579  [    1/ 3200]\n",
            "loss: 1.088868  [    2/ 3200]\n",
            "loss: 1.192254  [    3/ 3200]\n",
            "loss: 1.105805  [    4/ 3200]\n",
            "loss: 1.111381  [    5/ 3200]\n",
            "loss: 1.051295  [    6/ 3200]\n",
            "loss: 1.073273  [    7/ 3200]\n",
            "loss: 1.070012  [    8/ 3200]\n",
            "loss: 1.180342  [    9/ 3200]\n",
            "loss: 1.112931  [   10/ 3200]\n",
            "loss: 0.899552  [   11/ 3200]\n",
            "loss: 1.041223  [   12/ 3200]\n",
            "loss: 1.124511  [   13/ 3200]\n",
            "loss: 1.014655  [   14/ 3200]\n",
            "loss: 0.831256  [   15/ 3200]\n",
            "loss: 1.131128  [   16/ 3200]\n",
            "loss: 1.109349  [   17/ 3200]\n",
            "loss: 1.132041  [   18/ 3200]\n",
            "loss: 0.983069  [   19/ 3200]\n",
            "loss: 0.942810  [   20/ 3200]\n",
            "loss: 1.079889  [   21/ 3200]\n",
            "loss: 0.906262  [   22/ 3200]\n",
            "loss: 1.185790  [   23/ 3200]\n",
            "loss: 0.804486  [   24/ 3200]\n",
            "loss: 1.020252  [   25/ 3200]\n",
            "loss: 0.987558  [   26/ 3200]\n",
            "loss: 1.032227  [   27/ 3200]\n",
            "loss: 1.307687  [   28/ 3200]\n",
            "loss: 1.098131  [   29/ 3200]\n",
            "loss: 0.638348  [   30/ 3200]\n",
            "loss: 1.045732  [   31/ 3200]\n",
            "loss: 0.983148  [   32/ 3200]\n",
            "loss: 1.145359  [   33/ 3200]\n",
            "loss: 1.138095  [   34/ 3200]\n",
            "loss: 1.060158  [   35/ 3200]\n",
            "loss: 1.009031  [   36/ 3200]\n",
            "loss: 1.239391  [   37/ 3200]\n",
            "loss: 0.958831  [   38/ 3200]\n",
            "loss: 1.048579  [   39/ 3200]\n",
            "loss: 1.223095  [   40/ 3200]\n",
            "loss: 0.979781  [   41/ 3200]\n",
            "loss: 0.875499  [   42/ 3200]\n",
            "loss: 0.999290  [   43/ 3200]\n",
            "loss: 0.950707  [   44/ 3200]\n",
            "loss: 1.079053  [   45/ 3200]\n",
            "loss: 1.059555  [   46/ 3200]\n",
            "loss: 0.971982  [   47/ 3200]\n",
            "loss: 1.000133  [   48/ 3200]\n",
            "loss: 1.044363  [   49/ 3200]\n",
            "loss: 0.998508  [   50/ 3200]\n",
            "loss: 0.984527  [   51/ 3200]\n",
            "loss: 1.022823  [   52/ 3200]\n",
            "loss: 1.171375  [   53/ 3200]\n",
            "loss: 1.027486  [   54/ 3200]\n",
            "loss: 1.322086  [   55/ 3200]\n",
            "loss: 1.058907  [   56/ 3200]\n",
            "loss: 1.067424  [   57/ 3200]\n",
            "loss: 1.275277  [   58/ 3200]\n",
            "loss: 1.107685  [   59/ 3200]\n",
            "loss: 1.135233  [   60/ 3200]\n",
            "loss: 1.157074  [   61/ 3200]\n",
            "loss: 1.390341  [   62/ 3200]\n",
            "loss: 1.087521  [   63/ 3200]\n",
            "loss: 0.995617  [   64/ 3200]\n",
            "loss: 1.004890  [   65/ 3200]\n",
            "loss: 0.918492  [   66/ 3200]\n",
            "loss: 1.141356  [   67/ 3200]\n",
            "loss: 1.049996  [   68/ 3200]\n",
            "loss: 0.963701  [   69/ 3200]\n",
            "loss: 1.107229  [   70/ 3200]\n",
            "loss: 1.002876  [   71/ 3200]\n",
            "loss: 0.852773  [   72/ 3200]\n",
            "loss: 1.316783  [   73/ 3200]\n",
            "loss: 0.912270  [   74/ 3200]\n",
            "loss: 1.179328  [   75/ 3200]\n",
            "loss: 1.171505  [   76/ 3200]\n",
            "loss: 1.161907  [   77/ 3200]\n",
            "loss: 1.093825  [   78/ 3200]\n",
            "loss: 1.120705  [   79/ 3200]\n",
            "loss: 1.199486  [   80/ 3200]\n",
            "loss: 1.126614  [   81/ 3200]\n",
            "loss: 1.222003  [   82/ 3200]\n",
            "loss: 0.887776  [   83/ 3200]\n",
            "loss: 1.107537  [   84/ 3200]\n",
            "loss: 1.108344  [   85/ 3200]\n",
            "loss: 1.126432  [   86/ 3200]\n",
            "loss: 1.201127  [   87/ 3200]\n",
            "loss: 1.033755  [   88/ 3200]\n",
            "loss: 0.988263  [   89/ 3200]\n",
            "loss: 0.967119  [   90/ 3200]\n",
            "loss: 1.348789  [   91/ 3200]\n",
            "loss: 1.047023  [   92/ 3200]\n",
            "loss: 0.808179  [   93/ 3200]\n",
            "loss: 1.144065  [   94/ 3200]\n",
            "loss: 1.016487  [   95/ 3200]\n",
            "loss: 1.077579  [   96/ 3200]\n",
            "loss: 0.905470  [   97/ 3200]\n",
            "loss: 1.192609  [   98/ 3200]\n",
            "loss: 0.915313  [   99/ 3200]\n",
            "loss: 1.163940  [  100/ 3200]\n",
            "loss: 1.079991  [  101/ 3200]\n",
            "loss: 0.941293  [  102/ 3200]\n",
            "loss: 1.013478  [  103/ 3200]\n",
            "loss: 1.017022  [  104/ 3200]\n",
            "loss: 1.106988  [  105/ 3200]\n",
            "loss: 0.879750  [  106/ 3200]\n",
            "loss: 1.062704  [  107/ 3200]\n",
            "loss: 1.123011  [  108/ 3200]\n",
            "loss: 1.091176  [  109/ 3200]\n",
            "loss: 0.934802  [  110/ 3200]\n",
            "loss: 1.059021  [  111/ 3200]\n",
            "loss: 0.918639  [  112/ 3200]\n",
            "loss: 0.933398  [  113/ 3200]\n",
            "loss: 1.056766  [  114/ 3200]\n",
            "loss: 1.283220  [  115/ 3200]\n",
            "loss: 0.912261  [  116/ 3200]\n",
            "loss: 1.108228  [  117/ 3200]\n",
            "loss: 1.142825  [  118/ 3200]\n",
            "loss: 1.041722  [  119/ 3200]\n",
            "loss: 1.061433  [  120/ 3200]\n",
            "loss: 0.980020  [  121/ 3200]\n",
            "loss: 0.934756  [  122/ 3200]\n",
            "loss: 0.950528  [  123/ 3200]\n",
            "loss: 0.937446  [  124/ 3200]\n",
            "loss: 0.958728  [  125/ 3200]\n",
            "loss: 0.937404  [  126/ 3200]\n",
            "loss: 0.944886  [  127/ 3200]\n",
            "loss: 1.042376  [  128/ 3200]\n",
            "loss: 1.042195  [  129/ 3200]\n",
            "loss: 1.121876  [  130/ 3200]\n",
            "loss: 1.268134  [  131/ 3200]\n",
            "loss: 1.099887  [  132/ 3200]\n",
            "loss: 1.257109  [  133/ 3200]\n",
            "loss: 1.043989  [  134/ 3200]\n",
            "loss: 1.053963  [  135/ 3200]\n",
            "loss: 0.867865  [  136/ 3200]\n",
            "loss: 0.992682  [  137/ 3200]\n",
            "loss: 0.979401  [  138/ 3200]\n",
            "loss: 1.045178  [  139/ 3200]\n",
            "loss: 0.986928  [  140/ 3200]\n",
            "loss: 1.016798  [  141/ 3200]\n",
            "loss: 0.907645  [  142/ 3200]\n",
            "loss: 0.874051  [  143/ 3200]\n",
            "loss: 1.064551  [  144/ 3200]\n",
            "loss: 1.176088  [  145/ 3200]\n",
            "loss: 1.065305  [  146/ 3200]\n",
            "loss: 1.156562  [  147/ 3200]\n",
            "loss: 0.913954  [  148/ 3200]\n",
            "loss: 1.092722  [  149/ 3200]\n",
            "loss: 1.085737  [  150/ 3200]\n",
            "loss: 1.031387  [  151/ 3200]\n",
            "loss: 0.982401  [  152/ 3200]\n",
            "loss: 0.957038  [  153/ 3200]\n",
            "loss: 1.062628  [  154/ 3200]\n",
            "loss: 0.926727  [  155/ 3200]\n",
            "loss: 0.950454  [  156/ 3200]\n",
            "loss: 1.202599  [  157/ 3200]\n",
            "loss: 1.010483  [  158/ 3200]\n",
            "loss: 0.903397  [  159/ 3200]\n",
            "loss: 1.088847  [  160/ 3200]\n",
            "loss: 1.025840  [  161/ 3200]\n",
            "loss: 1.035820  [  162/ 3200]\n",
            "loss: 0.984819  [  163/ 3200]\n",
            "loss: 1.125202  [  164/ 3200]\n",
            "loss: 1.012624  [  165/ 3200]\n",
            "loss: 0.828578  [  166/ 3200]\n",
            "loss: 1.172366  [  167/ 3200]\n",
            "loss: 1.013711  [  168/ 3200]\n",
            "loss: 0.983056  [  169/ 3200]\n",
            "loss: 1.041566  [  170/ 3200]\n",
            "loss: 1.099194  [  171/ 3200]\n",
            "loss: 1.081404  [  172/ 3200]\n",
            "loss: 1.052592  [  173/ 3200]\n",
            "loss: 1.092180  [  174/ 3200]\n",
            "loss: 0.965389  [  175/ 3200]\n",
            "loss: 0.949605  [  176/ 3200]\n",
            "loss: 0.942414  [  177/ 3200]\n",
            "loss: 1.045615  [  178/ 3200]\n",
            "loss: 0.934142  [  179/ 3200]\n",
            "loss: 1.180296  [  180/ 3200]\n",
            "loss: 1.152736  [  181/ 3200]\n",
            "loss: 0.983242  [  182/ 3200]\n",
            "loss: 0.895623  [  183/ 3200]\n",
            "loss: 1.284626  [  184/ 3200]\n",
            "loss: 1.003807  [  185/ 3200]\n",
            "loss: 1.071329  [  186/ 3200]\n",
            "loss: 1.174765  [  187/ 3200]\n",
            "loss: 1.018685  [  188/ 3200]\n",
            "loss: 1.015971  [  189/ 3200]\n",
            "loss: 0.953793  [  190/ 3200]\n",
            "loss: 0.896686  [  191/ 3200]\n",
            "loss: 1.084822  [  192/ 3200]\n",
            "loss: 0.947418  [  193/ 3200]\n",
            "loss: 1.226464  [  194/ 3200]\n",
            "loss: 1.002695  [  195/ 3200]\n",
            "loss: 1.112659  [  196/ 3200]\n",
            "loss: 0.957842  [  197/ 3200]\n",
            "loss: 0.924661  [  198/ 3200]\n",
            "loss: 0.979183  [  199/ 3200]\n",
            "Epoch:  22\n",
            "loss: 1.066696  [    0/ 3200]\n",
            "loss: 0.969442  [    1/ 3200]\n",
            "loss: 1.226227  [    2/ 3200]\n",
            "loss: 0.946990  [    3/ 3200]\n",
            "loss: 1.267405  [    4/ 3200]\n",
            "loss: 1.083599  [    5/ 3200]\n",
            "loss: 1.000271  [    6/ 3200]\n",
            "loss: 0.893867  [    7/ 3200]\n",
            "loss: 1.132569  [    8/ 3200]\n",
            "loss: 1.103600  [    9/ 3200]\n",
            "loss: 1.011222  [   10/ 3200]\n",
            "loss: 1.065759  [   11/ 3200]\n",
            "loss: 0.953406  [   12/ 3200]\n",
            "loss: 0.906097  [   13/ 3200]\n",
            "loss: 0.987091  [   14/ 3200]\n",
            "loss: 0.838973  [   15/ 3200]\n",
            "loss: 1.007429  [   16/ 3200]\n",
            "loss: 0.931179  [   17/ 3200]\n",
            "loss: 1.161173  [   18/ 3200]\n",
            "loss: 1.098107  [   19/ 3200]\n",
            "loss: 0.982941  [   20/ 3200]\n",
            "loss: 1.076993  [   21/ 3200]\n",
            "loss: 1.048527  [   22/ 3200]\n",
            "loss: 1.301471  [   23/ 3200]\n",
            "loss: 1.071367  [   24/ 3200]\n",
            "loss: 1.047155  [   25/ 3200]\n",
            "loss: 0.758651  [   26/ 3200]\n",
            "loss: 0.999985  [   27/ 3200]\n",
            "loss: 1.051636  [   28/ 3200]\n",
            "loss: 0.868279  [   29/ 3200]\n",
            "loss: 0.992431  [   30/ 3200]\n",
            "loss: 0.943176  [   31/ 3200]\n",
            "loss: 1.065309  [   32/ 3200]\n",
            "loss: 0.912885  [   33/ 3200]\n",
            "loss: 0.844078  [   34/ 3200]\n",
            "loss: 0.993275  [   35/ 3200]\n",
            "loss: 0.950240  [   36/ 3200]\n",
            "loss: 0.865978  [   37/ 3200]\n",
            "loss: 1.283362  [   38/ 3200]\n",
            "loss: 1.253875  [   39/ 3200]\n",
            "loss: 1.172143  [   40/ 3200]\n",
            "loss: 1.047535  [   41/ 3200]\n",
            "loss: 1.011566  [   42/ 3200]\n",
            "loss: 1.194120  [   43/ 3200]\n",
            "loss: 1.033752  [   44/ 3200]\n",
            "loss: 1.107278  [   45/ 3200]\n",
            "loss: 1.087608  [   46/ 3200]\n",
            "loss: 1.040250  [   47/ 3200]\n",
            "loss: 0.895804  [   48/ 3200]\n",
            "loss: 0.833440  [   49/ 3200]\n",
            "loss: 1.157279  [   50/ 3200]\n",
            "loss: 0.906269  [   51/ 3200]\n",
            "loss: 0.910775  [   52/ 3200]\n",
            "loss: 1.284626  [   53/ 3200]\n",
            "loss: 1.113632  [   54/ 3200]\n",
            "loss: 1.112379  [   55/ 3200]\n",
            "loss: 0.897357  [   56/ 3200]\n",
            "loss: 0.967814  [   57/ 3200]\n",
            "loss: 1.159145  [   58/ 3200]\n",
            "loss: 1.249861  [   59/ 3200]\n",
            "loss: 1.118132  [   60/ 3200]\n",
            "loss: 0.963971  [   61/ 3200]\n",
            "loss: 1.092991  [   62/ 3200]\n",
            "loss: 1.208835  [   63/ 3200]\n",
            "loss: 0.915984  [   64/ 3200]\n",
            "loss: 1.235057  [   65/ 3200]\n",
            "loss: 1.013438  [   66/ 3200]\n",
            "loss: 1.017172  [   67/ 3200]\n",
            "loss: 0.941726  [   68/ 3200]\n",
            "loss: 0.982344  [   69/ 3200]\n",
            "loss: 0.964630  [   70/ 3200]\n",
            "loss: 1.139017  [   71/ 3200]\n",
            "loss: 0.990894  [   72/ 3200]\n",
            "loss: 1.154976  [   73/ 3200]\n",
            "loss: 0.896296  [   74/ 3200]\n",
            "loss: 0.981522  [   75/ 3200]\n",
            "loss: 0.940115  [   76/ 3200]\n",
            "loss: 1.184552  [   77/ 3200]\n",
            "loss: 1.135798  [   78/ 3200]\n",
            "loss: 1.052275  [   79/ 3200]\n",
            "loss: 1.030075  [   80/ 3200]\n",
            "loss: 0.888697  [   81/ 3200]\n",
            "loss: 1.215290  [   82/ 3200]\n",
            "loss: 0.828703  [   83/ 3200]\n",
            "loss: 0.936824  [   84/ 3200]\n",
            "loss: 0.740342  [   85/ 3200]\n",
            "loss: 0.863156  [   86/ 3200]\n",
            "loss: 0.846123  [   87/ 3200]\n",
            "loss: 1.009214  [   88/ 3200]\n",
            "loss: 1.131278  [   89/ 3200]\n",
            "loss: 0.755759  [   90/ 3200]\n",
            "loss: 0.897442  [   91/ 3200]\n",
            "loss: 1.107481  [   92/ 3200]\n",
            "loss: 1.100080  [   93/ 3200]\n",
            "loss: 1.314690  [   94/ 3200]\n",
            "loss: 1.024573  [   95/ 3200]\n",
            "loss: 1.036736  [   96/ 3200]\n",
            "loss: 1.035942  [   97/ 3200]\n",
            "loss: 0.993577  [   98/ 3200]\n",
            "loss: 1.042055  [   99/ 3200]\n",
            "loss: 1.102532  [  100/ 3200]\n",
            "loss: 1.160713  [  101/ 3200]\n",
            "loss: 1.020119  [  102/ 3200]\n",
            "loss: 0.891179  [  103/ 3200]\n",
            "loss: 0.860210  [  104/ 3200]\n",
            "loss: 0.942269  [  105/ 3200]\n",
            "loss: 0.927795  [  106/ 3200]\n",
            "loss: 0.959645  [  107/ 3200]\n",
            "loss: 0.879489  [  108/ 3200]\n",
            "loss: 0.975852  [  109/ 3200]\n",
            "loss: 1.066668  [  110/ 3200]\n",
            "loss: 1.173747  [  111/ 3200]\n",
            "loss: 0.832202  [  112/ 3200]\n",
            "loss: 0.936038  [  113/ 3200]\n",
            "loss: 1.070371  [  114/ 3200]\n",
            "loss: 0.857040  [  115/ 3200]\n",
            "loss: 1.026429  [  116/ 3200]\n",
            "loss: 0.986101  [  117/ 3200]\n",
            "loss: 1.043473  [  118/ 3200]\n",
            "loss: 1.021677  [  119/ 3200]\n",
            "loss: 1.156754  [  120/ 3200]\n",
            "loss: 1.016551  [  121/ 3200]\n",
            "loss: 1.036010  [  122/ 3200]\n",
            "loss: 1.208328  [  123/ 3200]\n",
            "loss: 0.945781  [  124/ 3200]\n",
            "loss: 0.908268  [  125/ 3200]\n",
            "loss: 1.131153  [  126/ 3200]\n",
            "loss: 1.052164  [  127/ 3200]\n",
            "loss: 1.142493  [  128/ 3200]\n",
            "loss: 1.147647  [  129/ 3200]\n",
            "loss: 1.011154  [  130/ 3200]\n",
            "loss: 1.061941  [  131/ 3200]\n",
            "loss: 1.043865  [  132/ 3200]\n",
            "loss: 0.982128  [  133/ 3200]\n",
            "loss: 1.132187  [  134/ 3200]\n",
            "loss: 1.001709  [  135/ 3200]\n",
            "loss: 1.016528  [  136/ 3200]\n",
            "loss: 1.061674  [  137/ 3200]\n",
            "loss: 1.130394  [  138/ 3200]\n",
            "loss: 1.265851  [  139/ 3200]\n",
            "loss: 1.088902  [  140/ 3200]\n",
            "loss: 0.992955  [  141/ 3200]\n",
            "loss: 0.987927  [  142/ 3200]\n",
            "loss: 1.228210  [  143/ 3200]\n",
            "loss: 1.360974  [  144/ 3200]\n",
            "loss: 1.016019  [  145/ 3200]\n",
            "loss: 1.085693  [  146/ 3200]\n",
            "loss: 0.909051  [  147/ 3200]\n",
            "loss: 0.972855  [  148/ 3200]\n",
            "loss: 0.888681  [  149/ 3200]\n",
            "loss: 1.102825  [  150/ 3200]\n",
            "loss: 1.048650  [  151/ 3200]\n",
            "loss: 1.186953  [  152/ 3200]\n",
            "loss: 0.920699  [  153/ 3200]\n",
            "loss: 0.969118  [  154/ 3200]\n",
            "loss: 0.994400  [  155/ 3200]\n",
            "loss: 0.996096  [  156/ 3200]\n",
            "loss: 1.039510  [  157/ 3200]\n",
            "loss: 1.167553  [  158/ 3200]\n",
            "loss: 0.928939  [  159/ 3200]\n",
            "loss: 1.185072  [  160/ 3200]\n",
            "loss: 1.245839  [  161/ 3200]\n",
            "loss: 0.930751  [  162/ 3200]\n",
            "loss: 1.215341  [  163/ 3200]\n",
            "loss: 1.017178  [  164/ 3200]\n",
            "loss: 1.043106  [  165/ 3200]\n",
            "loss: 0.990450  [  166/ 3200]\n",
            "loss: 1.033938  [  167/ 3200]\n",
            "loss: 1.030131  [  168/ 3200]\n",
            "loss: 1.097187  [  169/ 3200]\n",
            "loss: 1.209219  [  170/ 3200]\n",
            "loss: 1.188650  [  171/ 3200]\n",
            "loss: 1.160992  [  172/ 3200]\n",
            "loss: 0.800897  [  173/ 3200]\n",
            "loss: 0.864263  [  174/ 3200]\n",
            "loss: 0.762083  [  175/ 3200]\n",
            "loss: 1.118996  [  176/ 3200]\n",
            "loss: 1.090268  [  177/ 3200]\n",
            "loss: 1.208910  [  178/ 3200]\n",
            "loss: 1.021961  [  179/ 3200]\n",
            "loss: 1.064247  [  180/ 3200]\n",
            "loss: 1.116650  [  181/ 3200]\n",
            "loss: 0.936218  [  182/ 3200]\n",
            "loss: 1.117423  [  183/ 3200]\n",
            "loss: 0.967935  [  184/ 3200]\n",
            "loss: 1.082201  [  185/ 3200]\n",
            "loss: 1.073640  [  186/ 3200]\n",
            "loss: 0.932970  [  187/ 3200]\n",
            "loss: 1.155012  [  188/ 3200]\n",
            "loss: 1.043739  [  189/ 3200]\n",
            "loss: 1.111175  [  190/ 3200]\n",
            "loss: 0.844610  [  191/ 3200]\n",
            "loss: 1.136392  [  192/ 3200]\n",
            "loss: 0.955684  [  193/ 3200]\n",
            "loss: 1.079146  [  194/ 3200]\n",
            "loss: 0.940166  [  195/ 3200]\n",
            "loss: 0.761229  [  196/ 3200]\n",
            "loss: 0.916408  [  197/ 3200]\n",
            "loss: 0.928837  [  198/ 3200]\n",
            "loss: 1.036161  [  199/ 3200]\n",
            "Epoch:  23\n",
            "loss: 1.031735  [    0/ 3200]\n",
            "loss: 0.930096  [    1/ 3200]\n",
            "loss: 1.128144  [    2/ 3200]\n",
            "loss: 0.932681  [    3/ 3200]\n",
            "loss: 1.026583  [    4/ 3200]\n",
            "loss: 1.043026  [    5/ 3200]\n",
            "loss: 1.131165  [    6/ 3200]\n",
            "loss: 0.933326  [    7/ 3200]\n",
            "loss: 0.856840  [    8/ 3200]\n",
            "loss: 1.052209  [    9/ 3200]\n",
            "loss: 1.152675  [   10/ 3200]\n",
            "loss: 1.123682  [   11/ 3200]\n",
            "loss: 0.901630  [   12/ 3200]\n",
            "loss: 0.807645  [   13/ 3200]\n",
            "loss: 0.901086  [   14/ 3200]\n",
            "loss: 1.050084  [   15/ 3200]\n",
            "loss: 1.123896  [   16/ 3200]\n",
            "loss: 1.247498  [   17/ 3200]\n",
            "loss: 1.061145  [   18/ 3200]\n",
            "loss: 1.011156  [   19/ 3200]\n",
            "loss: 1.079599  [   20/ 3200]\n",
            "loss: 1.040711  [   21/ 3200]\n",
            "loss: 0.824684  [   22/ 3200]\n",
            "loss: 0.867189  [   23/ 3200]\n",
            "loss: 1.158909  [   24/ 3200]\n",
            "loss: 0.898387  [   25/ 3200]\n",
            "loss: 1.055768  [   26/ 3200]\n",
            "loss: 0.858834  [   27/ 3200]\n",
            "loss: 0.906485  [   28/ 3200]\n",
            "loss: 0.986318  [   29/ 3200]\n",
            "loss: 1.031922  [   30/ 3200]\n",
            "loss: 1.049865  [   31/ 3200]\n",
            "loss: 1.021979  [   32/ 3200]\n",
            "loss: 0.985467  [   33/ 3200]\n",
            "loss: 0.954965  [   34/ 3200]\n",
            "loss: 1.055749  [   35/ 3200]\n",
            "loss: 1.051877  [   36/ 3200]\n",
            "loss: 0.904639  [   37/ 3200]\n",
            "loss: 0.971090  [   38/ 3200]\n",
            "loss: 0.946522  [   39/ 3200]\n",
            "loss: 1.111182  [   40/ 3200]\n",
            "loss: 0.941196  [   41/ 3200]\n",
            "loss: 0.920741  [   42/ 3200]\n",
            "loss: 1.184946  [   43/ 3200]\n",
            "loss: 1.338754  [   44/ 3200]\n",
            "loss: 1.077105  [   45/ 3200]\n",
            "loss: 0.892967  [   46/ 3200]\n",
            "loss: 1.202367  [   47/ 3200]\n",
            "loss: 1.163770  [   48/ 3200]\n",
            "loss: 1.001661  [   49/ 3200]\n",
            "loss: 1.075803  [   50/ 3200]\n",
            "loss: 1.027198  [   51/ 3200]\n",
            "loss: 0.980321  [   52/ 3200]\n",
            "loss: 0.919083  [   53/ 3200]\n",
            "loss: 1.167704  [   54/ 3200]\n",
            "loss: 0.903369  [   55/ 3200]\n",
            "loss: 0.907957  [   56/ 3200]\n",
            "loss: 1.136526  [   57/ 3200]\n",
            "loss: 0.913743  [   58/ 3200]\n",
            "loss: 1.104615  [   59/ 3200]\n",
            "loss: 0.924691  [   60/ 3200]\n",
            "loss: 0.964505  [   61/ 3200]\n",
            "loss: 0.945292  [   62/ 3200]\n",
            "loss: 1.019999  [   63/ 3200]\n",
            "loss: 1.217856  [   64/ 3200]\n",
            "loss: 1.105681  [   65/ 3200]\n",
            "loss: 0.980842  [   66/ 3200]\n",
            "loss: 0.933674  [   67/ 3200]\n",
            "loss: 0.970846  [   68/ 3200]\n",
            "loss: 1.014190  [   69/ 3200]\n",
            "loss: 0.940827  [   70/ 3200]\n",
            "loss: 1.137124  [   71/ 3200]\n",
            "loss: 1.106871  [   72/ 3200]\n",
            "loss: 0.942512  [   73/ 3200]\n",
            "loss: 1.121659  [   74/ 3200]\n",
            "loss: 0.925226  [   75/ 3200]\n",
            "loss: 0.789021  [   76/ 3200]\n",
            "loss: 1.222359  [   77/ 3200]\n",
            "loss: 1.097820  [   78/ 3200]\n",
            "loss: 0.952211  [   79/ 3200]\n",
            "loss: 0.899241  [   80/ 3200]\n",
            "loss: 1.037252  [   81/ 3200]\n",
            "loss: 0.906143  [   82/ 3200]\n",
            "loss: 1.153106  [   83/ 3200]\n",
            "loss: 0.913582  [   84/ 3200]\n",
            "loss: 0.987028  [   85/ 3200]\n",
            "loss: 0.908669  [   86/ 3200]\n",
            "loss: 0.949513  [   87/ 3200]\n",
            "loss: 1.139941  [   88/ 3200]\n",
            "loss: 1.065623  [   89/ 3200]\n",
            "loss: 0.967254  [   90/ 3200]\n",
            "loss: 0.946931  [   91/ 3200]\n",
            "loss: 0.873420  [   92/ 3200]\n",
            "loss: 0.987054  [   93/ 3200]\n",
            "loss: 0.825718  [   94/ 3200]\n",
            "loss: 1.129102  [   95/ 3200]\n",
            "loss: 1.047486  [   96/ 3200]\n",
            "loss: 1.074060  [   97/ 3200]\n",
            "loss: 1.205800  [   98/ 3200]\n",
            "loss: 1.031148  [   99/ 3200]\n",
            "loss: 0.866438  [  100/ 3200]\n",
            "loss: 0.873621  [  101/ 3200]\n",
            "loss: 1.377068  [  102/ 3200]\n",
            "loss: 0.915666  [  103/ 3200]\n",
            "loss: 1.098107  [  104/ 3200]\n",
            "loss: 0.974219  [  105/ 3200]\n",
            "loss: 0.918066  [  106/ 3200]\n",
            "loss: 0.979487  [  107/ 3200]\n",
            "loss: 1.012590  [  108/ 3200]\n",
            "loss: 0.996693  [  109/ 3200]\n",
            "loss: 1.246502  [  110/ 3200]\n",
            "loss: 0.998181  [  111/ 3200]\n",
            "loss: 1.072641  [  112/ 3200]\n",
            "loss: 0.875115  [  113/ 3200]\n",
            "loss: 1.116017  [  114/ 3200]\n",
            "loss: 1.174394  [  115/ 3200]\n",
            "loss: 1.004624  [  116/ 3200]\n",
            "loss: 1.135439  [  117/ 3200]\n",
            "loss: 1.089839  [  118/ 3200]\n",
            "loss: 1.043434  [  119/ 3200]\n",
            "loss: 0.926611  [  120/ 3200]\n",
            "loss: 1.110227  [  121/ 3200]\n",
            "loss: 1.087653  [  122/ 3200]\n",
            "loss: 0.993959  [  123/ 3200]\n",
            "loss: 0.946646  [  124/ 3200]\n",
            "loss: 0.897501  [  125/ 3200]\n",
            "loss: 1.033360  [  126/ 3200]\n",
            "loss: 0.968178  [  127/ 3200]\n",
            "loss: 0.933128  [  128/ 3200]\n",
            "loss: 0.876148  [  129/ 3200]\n",
            "loss: 0.967221  [  130/ 3200]\n",
            "loss: 0.857779  [  131/ 3200]\n",
            "loss: 0.835928  [  132/ 3200]\n",
            "loss: 1.077127  [  133/ 3200]\n",
            "loss: 0.856031  [  134/ 3200]\n",
            "loss: 1.121476  [  135/ 3200]\n",
            "loss: 0.902748  [  136/ 3200]\n",
            "loss: 1.025839  [  137/ 3200]\n",
            "loss: 1.379332  [  138/ 3200]\n",
            "loss: 0.906977  [  139/ 3200]\n",
            "loss: 1.063213  [  140/ 3200]\n",
            "loss: 1.115917  [  141/ 3200]\n",
            "loss: 0.873870  [  142/ 3200]\n",
            "loss: 0.847742  [  143/ 3200]\n",
            "loss: 1.217314  [  144/ 3200]\n",
            "loss: 0.807859  [  145/ 3200]\n",
            "loss: 1.065038  [  146/ 3200]\n",
            "loss: 0.862461  [  147/ 3200]\n",
            "loss: 0.961915  [  148/ 3200]\n",
            "loss: 1.118543  [  149/ 3200]\n",
            "loss: 0.768160  [  150/ 3200]\n",
            "loss: 1.103432  [  151/ 3200]\n",
            "loss: 0.938961  [  152/ 3200]\n",
            "loss: 0.835233  [  153/ 3200]\n",
            "loss: 0.990576  [  154/ 3200]\n",
            "loss: 1.192226  [  155/ 3200]\n",
            "loss: 1.068468  [  156/ 3200]\n",
            "loss: 0.866377  [  157/ 3200]\n",
            "loss: 1.019743  [  158/ 3200]\n",
            "loss: 1.031901  [  159/ 3200]\n",
            "loss: 1.019217  [  160/ 3200]\n",
            "loss: 1.193516  [  161/ 3200]\n",
            "loss: 0.929719  [  162/ 3200]\n",
            "loss: 1.138116  [  163/ 3200]\n",
            "loss: 1.051050  [  164/ 3200]\n",
            "loss: 1.044086  [  165/ 3200]\n",
            "loss: 0.943037  [  166/ 3200]\n",
            "loss: 0.711991  [  167/ 3200]\n",
            "loss: 1.024277  [  168/ 3200]\n",
            "loss: 0.979373  [  169/ 3200]\n",
            "loss: 0.941374  [  170/ 3200]\n",
            "loss: 1.048896  [  171/ 3200]\n",
            "loss: 0.906828  [  172/ 3200]\n",
            "loss: 1.070429  [  173/ 3200]\n",
            "loss: 1.078789  [  174/ 3200]\n",
            "loss: 1.098145  [  175/ 3200]\n",
            "loss: 0.880237  [  176/ 3200]\n",
            "loss: 1.038121  [  177/ 3200]\n",
            "loss: 1.332122  [  178/ 3200]\n",
            "loss: 0.887471  [  179/ 3200]\n",
            "loss: 1.114785  [  180/ 3200]\n",
            "loss: 0.977124  [  181/ 3200]\n",
            "loss: 1.119175  [  182/ 3200]\n",
            "loss: 0.950646  [  183/ 3200]\n",
            "loss: 1.022164  [  184/ 3200]\n",
            "loss: 1.168159  [  185/ 3200]\n",
            "loss: 1.053614  [  186/ 3200]\n",
            "loss: 1.065724  [  187/ 3200]\n",
            "loss: 0.922759  [  188/ 3200]\n",
            "loss: 0.950341  [  189/ 3200]\n",
            "loss: 1.120213  [  190/ 3200]\n",
            "loss: 1.211863  [  191/ 3200]\n",
            "loss: 1.120352  [  192/ 3200]\n",
            "loss: 1.120978  [  193/ 3200]\n",
            "loss: 0.962298  [  194/ 3200]\n",
            "loss: 1.141630  [  195/ 3200]\n",
            "loss: 0.949372  [  196/ 3200]\n",
            "loss: 0.983509  [  197/ 3200]\n",
            "loss: 0.745247  [  198/ 3200]\n",
            "loss: 1.024024  [  199/ 3200]\n",
            "Epoch:  24\n",
            "loss: 1.009904  [    0/ 3200]\n",
            "loss: 1.055517  [    1/ 3200]\n",
            "loss: 0.748535  [    2/ 3200]\n",
            "loss: 1.127184  [    3/ 3200]\n",
            "loss: 1.101181  [    4/ 3200]\n",
            "loss: 1.210578  [    5/ 3200]\n",
            "loss: 0.979742  [    6/ 3200]\n",
            "loss: 1.052530  [    7/ 3200]\n",
            "loss: 0.971961  [    8/ 3200]\n",
            "loss: 1.096032  [    9/ 3200]\n",
            "loss: 1.020481  [   10/ 3200]\n",
            "loss: 1.040392  [   11/ 3200]\n",
            "loss: 0.926652  [   12/ 3200]\n",
            "loss: 0.924332  [   13/ 3200]\n",
            "loss: 1.011554  [   14/ 3200]\n",
            "loss: 0.955519  [   15/ 3200]\n",
            "loss: 0.863827  [   16/ 3200]\n",
            "loss: 1.131852  [   17/ 3200]\n",
            "loss: 1.071799  [   18/ 3200]\n",
            "loss: 0.831016  [   19/ 3200]\n",
            "loss: 1.127138  [   20/ 3200]\n",
            "loss: 0.843632  [   21/ 3200]\n",
            "loss: 1.065286  [   22/ 3200]\n",
            "loss: 0.769290  [   23/ 3200]\n",
            "loss: 0.731261  [   24/ 3200]\n",
            "loss: 1.098570  [   25/ 3200]\n",
            "loss: 1.083744  [   26/ 3200]\n",
            "loss: 1.094654  [   27/ 3200]\n",
            "loss: 1.018604  [   28/ 3200]\n",
            "loss: 0.912152  [   29/ 3200]\n",
            "loss: 1.104106  [   30/ 3200]\n",
            "loss: 1.015309  [   31/ 3200]\n",
            "loss: 0.961936  [   32/ 3200]\n",
            "loss: 1.080352  [   33/ 3200]\n",
            "loss: 0.898285  [   34/ 3200]\n",
            "loss: 0.949559  [   35/ 3200]\n",
            "loss: 1.087295  [   36/ 3200]\n",
            "loss: 0.888807  [   37/ 3200]\n",
            "loss: 0.935699  [   38/ 3200]\n",
            "loss: 1.304274  [   39/ 3200]\n",
            "loss: 1.119554  [   40/ 3200]\n",
            "loss: 1.138483  [   41/ 3200]\n",
            "loss: 0.885431  [   42/ 3200]\n",
            "loss: 1.058493  [   43/ 3200]\n",
            "loss: 1.183560  [   44/ 3200]\n",
            "loss: 0.818980  [   45/ 3200]\n",
            "loss: 0.826805  [   46/ 3200]\n",
            "loss: 0.816295  [   47/ 3200]\n",
            "loss: 0.902638  [   48/ 3200]\n",
            "loss: 1.352647  [   49/ 3200]\n",
            "loss: 1.290394  [   50/ 3200]\n",
            "loss: 1.005690  [   51/ 3200]\n",
            "loss: 0.860885  [   52/ 3200]\n",
            "loss: 1.015619  [   53/ 3200]\n",
            "loss: 1.024824  [   54/ 3200]\n",
            "loss: 1.028990  [   55/ 3200]\n",
            "loss: 1.142207  [   56/ 3200]\n",
            "loss: 0.999291  [   57/ 3200]\n",
            "loss: 1.202641  [   58/ 3200]\n",
            "loss: 0.758968  [   59/ 3200]\n",
            "loss: 1.122226  [   60/ 3200]\n",
            "loss: 0.963217  [   61/ 3200]\n",
            "loss: 0.712659  [   62/ 3200]\n",
            "loss: 1.118060  [   63/ 3200]\n",
            "loss: 0.918332  [   64/ 3200]\n",
            "loss: 0.979958  [   65/ 3200]\n",
            "loss: 0.879535  [   66/ 3200]\n",
            "loss: 0.944798  [   67/ 3200]\n",
            "loss: 0.755272  [   68/ 3200]\n",
            "loss: 1.098337  [   69/ 3200]\n",
            "loss: 0.940453  [   70/ 3200]\n",
            "loss: 0.967083  [   71/ 3200]\n",
            "loss: 0.913441  [   72/ 3200]\n",
            "loss: 0.781705  [   73/ 3200]\n",
            "loss: 1.112441  [   74/ 3200]\n",
            "loss: 1.177386  [   75/ 3200]\n",
            "loss: 0.996522  [   76/ 3200]\n",
            "loss: 1.098112  [   77/ 3200]\n",
            "loss: 1.238907  [   78/ 3200]\n",
            "loss: 0.949784  [   79/ 3200]\n",
            "loss: 1.061219  [   80/ 3200]\n",
            "loss: 0.972898  [   81/ 3200]\n",
            "loss: 0.925620  [   82/ 3200]\n",
            "loss: 0.910192  [   83/ 3200]\n",
            "loss: 1.091519  [   84/ 3200]\n",
            "loss: 1.074087  [   85/ 3200]\n",
            "loss: 1.064229  [   86/ 3200]\n",
            "loss: 0.898512  [   87/ 3200]\n",
            "loss: 0.912015  [   88/ 3200]\n",
            "loss: 0.767796  [   89/ 3200]\n",
            "loss: 1.014574  [   90/ 3200]\n",
            "loss: 0.990368  [   91/ 3200]\n",
            "loss: 0.923963  [   92/ 3200]\n",
            "loss: 0.866217  [   93/ 3200]\n",
            "loss: 1.024912  [   94/ 3200]\n",
            "loss: 0.995833  [   95/ 3200]\n",
            "loss: 0.908116  [   96/ 3200]\n",
            "loss: 1.182430  [   97/ 3200]\n",
            "loss: 1.038229  [   98/ 3200]\n",
            "loss: 0.758516  [   99/ 3200]\n",
            "loss: 0.771313  [  100/ 3200]\n",
            "loss: 1.106631  [  101/ 3200]\n",
            "loss: 0.993697  [  102/ 3200]\n",
            "loss: 0.990444  [  103/ 3200]\n",
            "loss: 0.874173  [  104/ 3200]\n",
            "loss: 0.958685  [  105/ 3200]\n",
            "loss: 1.053339  [  106/ 3200]\n",
            "loss: 1.270317  [  107/ 3200]\n",
            "loss: 1.175371  [  108/ 3200]\n",
            "loss: 0.944518  [  109/ 3200]\n",
            "loss: 1.195971  [  110/ 3200]\n",
            "loss: 1.243665  [  111/ 3200]\n",
            "loss: 0.943552  [  112/ 3200]\n",
            "loss: 0.754946  [  113/ 3200]\n",
            "loss: 0.805328  [  114/ 3200]\n",
            "loss: 1.041808  [  115/ 3200]\n",
            "loss: 0.983834  [  116/ 3200]\n",
            "loss: 1.028193  [  117/ 3200]\n",
            "loss: 0.870222  [  118/ 3200]\n",
            "loss: 0.747059  [  119/ 3200]\n",
            "loss: 0.726052  [  120/ 3200]\n",
            "loss: 1.108527  [  121/ 3200]\n",
            "loss: 0.913335  [  122/ 3200]\n",
            "loss: 0.975379  [  123/ 3200]\n",
            "loss: 0.876594  [  124/ 3200]\n",
            "loss: 1.265567  [  125/ 3200]\n",
            "loss: 1.051679  [  126/ 3200]\n",
            "loss: 0.948350  [  127/ 3200]\n",
            "loss: 0.998510  [  128/ 3200]\n",
            "loss: 1.075513  [  129/ 3200]\n",
            "loss: 1.061576  [  130/ 3200]\n",
            "loss: 0.912768  [  131/ 3200]\n",
            "loss: 1.105815  [  132/ 3200]\n",
            "loss: 1.055888  [  133/ 3200]\n",
            "loss: 1.169127  [  134/ 3200]\n",
            "loss: 0.918797  [  135/ 3200]\n",
            "loss: 0.981285  [  136/ 3200]\n",
            "loss: 1.133585  [  137/ 3200]\n",
            "loss: 1.019043  [  138/ 3200]\n",
            "loss: 1.094177  [  139/ 3200]\n",
            "loss: 0.865428  [  140/ 3200]\n",
            "loss: 1.160949  [  141/ 3200]\n",
            "loss: 0.924166  [  142/ 3200]\n",
            "loss: 0.867653  [  143/ 3200]\n",
            "loss: 1.096615  [  144/ 3200]\n",
            "loss: 1.089955  [  145/ 3200]\n",
            "loss: 1.062708  [  146/ 3200]\n",
            "loss: 1.146433  [  147/ 3200]\n",
            "loss: 1.121940  [  148/ 3200]\n",
            "loss: 1.011590  [  149/ 3200]\n",
            "loss: 0.839513  [  150/ 3200]\n",
            "loss: 1.027109  [  151/ 3200]\n",
            "loss: 0.958856  [  152/ 3200]\n",
            "loss: 0.666233  [  153/ 3200]\n",
            "loss: 1.142374  [  154/ 3200]\n",
            "loss: 0.968978  [  155/ 3200]\n",
            "loss: 0.974336  [  156/ 3200]\n",
            "loss: 1.013776  [  157/ 3200]\n",
            "loss: 1.042921  [  158/ 3200]\n",
            "loss: 1.103613  [  159/ 3200]\n",
            "loss: 0.766039  [  160/ 3200]\n",
            "loss: 1.030938  [  161/ 3200]\n",
            "loss: 1.292351  [  162/ 3200]\n",
            "loss: 0.916587  [  163/ 3200]\n",
            "loss: 1.126393  [  164/ 3200]\n",
            "loss: 0.931960  [  165/ 3200]\n",
            "loss: 0.913493  [  166/ 3200]\n",
            "loss: 0.951213  [  167/ 3200]\n",
            "loss: 1.158617  [  168/ 3200]\n",
            "loss: 1.007046  [  169/ 3200]\n",
            "loss: 1.285270  [  170/ 3200]\n",
            "loss: 0.815726  [  171/ 3200]\n",
            "loss: 1.261798  [  172/ 3200]\n",
            "loss: 0.963184  [  173/ 3200]\n",
            "loss: 0.989521  [  174/ 3200]\n",
            "loss: 0.967168  [  175/ 3200]\n",
            "loss: 0.927354  [  176/ 3200]\n",
            "loss: 1.117849  [  177/ 3200]\n",
            "loss: 1.040710  [  178/ 3200]\n",
            "loss: 1.183749  [  179/ 3200]\n",
            "loss: 0.853189  [  180/ 3200]\n",
            "loss: 0.966402  [  181/ 3200]\n",
            "loss: 0.956587  [  182/ 3200]\n",
            "loss: 1.322992  [  183/ 3200]\n",
            "loss: 1.057683  [  184/ 3200]\n",
            "loss: 0.934582  [  185/ 3200]\n",
            "loss: 0.948105  [  186/ 3200]\n",
            "loss: 1.149085  [  187/ 3200]\n",
            "loss: 1.127265  [  188/ 3200]\n",
            "loss: 0.999965  [  189/ 3200]\n",
            "loss: 1.071587  [  190/ 3200]\n",
            "loss: 1.144059  [  191/ 3200]\n",
            "loss: 1.015134  [  192/ 3200]\n",
            "loss: 0.751698  [  193/ 3200]\n",
            "loss: 0.981328  [  194/ 3200]\n",
            "loss: 1.076404  [  195/ 3200]\n",
            "loss: 0.980561  [  196/ 3200]\n",
            "loss: 0.972176  [  197/ 3200]\n",
            "loss: 0.965391  [  198/ 3200]\n",
            "loss: 1.072410  [  199/ 3200]\n",
            "Epoch:  25\n",
            "loss: 1.108595  [    0/ 3200]\n",
            "loss: 0.916590  [    1/ 3200]\n",
            "loss: 0.975513  [    2/ 3200]\n",
            "loss: 1.101534  [    3/ 3200]\n",
            "loss: 0.960299  [    4/ 3200]\n",
            "loss: 0.840771  [    5/ 3200]\n",
            "loss: 1.240533  [    6/ 3200]\n",
            "loss: 0.816709  [    7/ 3200]\n",
            "loss: 0.964463  [    8/ 3200]\n",
            "loss: 1.052158  [    9/ 3200]\n",
            "loss: 0.882172  [   10/ 3200]\n",
            "loss: 0.907971  [   11/ 3200]\n",
            "loss: 1.242214  [   12/ 3200]\n",
            "loss: 1.063008  [   13/ 3200]\n",
            "loss: 1.009652  [   14/ 3200]\n",
            "loss: 0.895306  [   15/ 3200]\n",
            "loss: 0.774495  [   16/ 3200]\n",
            "loss: 0.994312  [   17/ 3200]\n",
            "loss: 0.981629  [   18/ 3200]\n",
            "loss: 0.895962  [   19/ 3200]\n",
            "loss: 1.209810  [   20/ 3200]\n",
            "loss: 1.170508  [   21/ 3200]\n",
            "loss: 1.041133  [   22/ 3200]\n",
            "loss: 0.875165  [   23/ 3200]\n",
            "loss: 0.846054  [   24/ 3200]\n",
            "loss: 1.105028  [   25/ 3200]\n",
            "loss: 0.823023  [   26/ 3200]\n",
            "loss: 1.018188  [   27/ 3200]\n",
            "loss: 1.065514  [   28/ 3200]\n",
            "loss: 0.989779  [   29/ 3200]\n",
            "loss: 1.311104  [   30/ 3200]\n",
            "loss: 1.002193  [   31/ 3200]\n",
            "loss: 0.991595  [   32/ 3200]\n",
            "loss: 1.023501  [   33/ 3200]\n",
            "loss: 0.831738  [   34/ 3200]\n",
            "loss: 0.984332  [   35/ 3200]\n",
            "loss: 0.766349  [   36/ 3200]\n",
            "loss: 0.975031  [   37/ 3200]\n",
            "loss: 0.965096  [   38/ 3200]\n",
            "loss: 0.969885  [   39/ 3200]\n",
            "loss: 0.930192  [   40/ 3200]\n",
            "loss: 1.240912  [   41/ 3200]\n",
            "loss: 1.009560  [   42/ 3200]\n",
            "loss: 0.852519  [   43/ 3200]\n",
            "loss: 0.891706  [   44/ 3200]\n",
            "loss: 0.798902  [   45/ 3200]\n",
            "loss: 0.903637  [   46/ 3200]\n",
            "loss: 0.867074  [   47/ 3200]\n",
            "loss: 0.963159  [   48/ 3200]\n",
            "loss: 0.879391  [   49/ 3200]\n",
            "loss: 1.093507  [   50/ 3200]\n",
            "loss: 1.033295  [   51/ 3200]\n",
            "loss: 1.121906  [   52/ 3200]\n",
            "loss: 0.981657  [   53/ 3200]\n",
            "loss: 0.946589  [   54/ 3200]\n",
            "loss: 0.939578  [   55/ 3200]\n",
            "loss: 1.047951  [   56/ 3200]\n",
            "loss: 0.924279  [   57/ 3200]\n",
            "loss: 0.873316  [   58/ 3200]\n",
            "loss: 1.069695  [   59/ 3200]\n",
            "loss: 0.886089  [   60/ 3200]\n",
            "loss: 1.031193  [   61/ 3200]\n",
            "loss: 0.914314  [   62/ 3200]\n",
            "loss: 1.261661  [   63/ 3200]\n",
            "loss: 0.898368  [   64/ 3200]\n",
            "loss: 0.924684  [   65/ 3200]\n",
            "loss: 1.077281  [   66/ 3200]\n",
            "loss: 0.779612  [   67/ 3200]\n",
            "loss: 0.839576  [   68/ 3200]\n",
            "loss: 1.223186  [   69/ 3200]\n",
            "loss: 1.052282  [   70/ 3200]\n",
            "loss: 1.134915  [   71/ 3200]\n",
            "loss: 0.989411  [   72/ 3200]\n",
            "loss: 1.045688  [   73/ 3200]\n",
            "loss: 0.971332  [   74/ 3200]\n",
            "loss: 1.424246  [   75/ 3200]\n",
            "loss: 0.905567  [   76/ 3200]\n",
            "loss: 1.189183  [   77/ 3200]\n",
            "loss: 1.118941  [   78/ 3200]\n",
            "loss: 1.044119  [   79/ 3200]\n",
            "loss: 1.023663  [   80/ 3200]\n",
            "loss: 0.757697  [   81/ 3200]\n",
            "loss: 0.823516  [   82/ 3200]\n",
            "loss: 1.117388  [   83/ 3200]\n",
            "loss: 0.935410  [   84/ 3200]\n",
            "loss: 0.923443  [   85/ 3200]\n",
            "loss: 0.984268  [   86/ 3200]\n",
            "loss: 0.799482  [   87/ 3200]\n",
            "loss: 0.990185  [   88/ 3200]\n",
            "loss: 0.937599  [   89/ 3200]\n",
            "loss: 0.806609  [   90/ 3200]\n",
            "loss: 1.172847  [   91/ 3200]\n",
            "loss: 0.913516  [   92/ 3200]\n",
            "loss: 0.983609  [   93/ 3200]\n",
            "loss: 1.040598  [   94/ 3200]\n",
            "loss: 0.716014  [   95/ 3200]\n",
            "loss: 0.746927  [   96/ 3200]\n",
            "loss: 0.868934  [   97/ 3200]\n",
            "loss: 0.994249  [   98/ 3200]\n",
            "loss: 1.211346  [   99/ 3200]\n",
            "loss: 1.063318  [  100/ 3200]\n",
            "loss: 1.029046  [  101/ 3200]\n",
            "loss: 1.229824  [  102/ 3200]\n",
            "loss: 0.933053  [  103/ 3200]\n",
            "loss: 1.044818  [  104/ 3200]\n",
            "loss: 1.069340  [  105/ 3200]\n",
            "loss: 0.919327  [  106/ 3200]\n",
            "loss: 0.805197  [  107/ 3200]\n",
            "loss: 0.614002  [  108/ 3200]\n",
            "loss: 1.261021  [  109/ 3200]\n",
            "loss: 1.090455  [  110/ 3200]\n",
            "loss: 1.072201  [  111/ 3200]\n",
            "loss: 0.890663  [  112/ 3200]\n",
            "loss: 0.842160  [  113/ 3200]\n",
            "loss: 0.684799  [  114/ 3200]\n",
            "loss: 1.068449  [  115/ 3200]\n",
            "loss: 0.939113  [  116/ 3200]\n",
            "loss: 0.901354  [  117/ 3200]\n",
            "loss: 0.975222  [  118/ 3200]\n",
            "loss: 1.129961  [  119/ 3200]\n",
            "loss: 0.955136  [  120/ 3200]\n",
            "loss: 1.122248  [  121/ 3200]\n",
            "loss: 1.041182  [  122/ 3200]\n",
            "loss: 1.007455  [  123/ 3200]\n",
            "loss: 1.013674  [  124/ 3200]\n",
            "loss: 0.995017  [  125/ 3200]\n",
            "loss: 0.936533  [  126/ 3200]\n",
            "loss: 1.117846  [  127/ 3200]\n",
            "loss: 0.979940  [  128/ 3200]\n",
            "loss: 0.938058  [  129/ 3200]\n",
            "loss: 1.234263  [  130/ 3200]\n",
            "loss: 0.808115  [  131/ 3200]\n",
            "loss: 1.028487  [  132/ 3200]\n",
            "loss: 0.946393  [  133/ 3200]\n",
            "loss: 0.930451  [  134/ 3200]\n",
            "loss: 0.875434  [  135/ 3200]\n",
            "loss: 1.158236  [  136/ 3200]\n",
            "loss: 0.875389  [  137/ 3200]\n",
            "loss: 1.091323  [  138/ 3200]\n",
            "loss: 0.918016  [  139/ 3200]\n",
            "loss: 0.884737  [  140/ 3200]\n",
            "loss: 1.003968  [  141/ 3200]\n",
            "loss: 0.990348  [  142/ 3200]\n",
            "loss: 1.123393  [  143/ 3200]\n",
            "loss: 0.992546  [  144/ 3200]\n",
            "loss: 0.970198  [  145/ 3200]\n",
            "loss: 1.238455  [  146/ 3200]\n",
            "loss: 1.022220  [  147/ 3200]\n",
            "loss: 0.983041  [  148/ 3200]\n",
            "loss: 1.185823  [  149/ 3200]\n",
            "loss: 1.053572  [  150/ 3200]\n",
            "loss: 0.945979  [  151/ 3200]\n",
            "loss: 1.019727  [  152/ 3200]\n",
            "loss: 1.226177  [  153/ 3200]\n",
            "loss: 1.103974  [  154/ 3200]\n",
            "loss: 0.957238  [  155/ 3200]\n",
            "loss: 0.884598  [  156/ 3200]\n",
            "loss: 1.132360  [  157/ 3200]\n",
            "loss: 0.861260  [  158/ 3200]\n",
            "loss: 0.870232  [  159/ 3200]\n",
            "loss: 0.892307  [  160/ 3200]\n",
            "loss: 0.921632  [  161/ 3200]\n",
            "loss: 0.878255  [  162/ 3200]\n",
            "loss: 0.808909  [  163/ 3200]\n",
            "loss: 0.987601  [  164/ 3200]\n",
            "loss: 1.141334  [  165/ 3200]\n",
            "loss: 1.112926  [  166/ 3200]\n",
            "loss: 1.115960  [  167/ 3200]\n",
            "loss: 0.974104  [  168/ 3200]\n",
            "loss: 1.156885  [  169/ 3200]\n",
            "loss: 1.210030  [  170/ 3200]\n",
            "loss: 0.750173  [  171/ 3200]\n",
            "loss: 0.681651  [  172/ 3200]\n",
            "loss: 1.056813  [  173/ 3200]\n",
            "loss: 0.764461  [  174/ 3200]\n",
            "loss: 1.328167  [  175/ 3200]\n",
            "loss: 0.809373  [  176/ 3200]\n",
            "loss: 1.054782  [  177/ 3200]\n",
            "loss: 1.173887  [  178/ 3200]\n",
            "loss: 1.153600  [  179/ 3200]\n",
            "loss: 0.761593  [  180/ 3200]\n",
            "loss: 0.853028  [  181/ 3200]\n",
            "loss: 0.898372  [  182/ 3200]\n",
            "loss: 1.025894  [  183/ 3200]\n",
            "loss: 1.238039  [  184/ 3200]\n",
            "loss: 1.200203  [  185/ 3200]\n",
            "loss: 0.830856  [  186/ 3200]\n",
            "loss: 1.046868  [  187/ 3200]\n",
            "loss: 0.822134  [  188/ 3200]\n",
            "loss: 1.160415  [  189/ 3200]\n",
            "loss: 0.793735  [  190/ 3200]\n",
            "loss: 0.893384  [  191/ 3200]\n",
            "loss: 1.012732  [  192/ 3200]\n",
            "loss: 1.140782  [  193/ 3200]\n",
            "loss: 0.950852  [  194/ 3200]\n",
            "loss: 0.930170  [  195/ 3200]\n",
            "loss: 1.049337  [  196/ 3200]\n",
            "loss: 0.849743  [  197/ 3200]\n",
            "loss: 1.007041  [  198/ 3200]\n",
            "loss: 1.033495  [  199/ 3200]\n",
            "Epoch:  26\n",
            "loss: 1.030424  [    0/ 3200]\n",
            "loss: 0.879938  [    1/ 3200]\n",
            "loss: 1.339926  [    2/ 3200]\n",
            "loss: 1.046815  [    3/ 3200]\n",
            "loss: 1.330170  [    4/ 3200]\n",
            "loss: 0.951220  [    5/ 3200]\n",
            "loss: 1.010309  [    6/ 3200]\n",
            "loss: 1.034113  [    7/ 3200]\n",
            "loss: 0.947140  [    8/ 3200]\n",
            "loss: 1.381367  [    9/ 3200]\n",
            "loss: 1.152077  [   10/ 3200]\n",
            "loss: 0.906115  [   11/ 3200]\n",
            "loss: 1.009047  [   12/ 3200]\n",
            "loss: 0.762544  [   13/ 3200]\n",
            "loss: 1.014946  [   14/ 3200]\n",
            "loss: 0.981100  [   15/ 3200]\n",
            "loss: 1.114083  [   16/ 3200]\n",
            "loss: 0.932194  [   17/ 3200]\n",
            "loss: 1.003188  [   18/ 3200]\n",
            "loss: 0.830363  [   19/ 3200]\n",
            "loss: 1.106596  [   20/ 3200]\n",
            "loss: 0.955046  [   21/ 3200]\n",
            "loss: 0.760933  [   22/ 3200]\n",
            "loss: 0.822525  [   23/ 3200]\n",
            "loss: 1.021304  [   24/ 3200]\n",
            "loss: 0.818135  [   25/ 3200]\n",
            "loss: 1.105238  [   26/ 3200]\n",
            "loss: 0.931318  [   27/ 3200]\n",
            "loss: 0.761144  [   28/ 3200]\n",
            "loss: 1.198856  [   29/ 3200]\n",
            "loss: 0.823043  [   30/ 3200]\n",
            "loss: 0.780222  [   31/ 3200]\n",
            "loss: 1.102301  [   32/ 3200]\n",
            "loss: 0.860408  [   33/ 3200]\n",
            "loss: 0.917444  [   34/ 3200]\n",
            "loss: 0.828620  [   35/ 3200]\n",
            "loss: 0.626167  [   36/ 3200]\n",
            "loss: 0.883769  [   37/ 3200]\n",
            "loss: 1.060543  [   38/ 3200]\n",
            "loss: 0.799259  [   39/ 3200]\n",
            "loss: 0.999150  [   40/ 3200]\n",
            "loss: 1.128731  [   41/ 3200]\n",
            "loss: 0.904307  [   42/ 3200]\n",
            "loss: 0.684942  [   43/ 3200]\n",
            "loss: 1.061838  [   44/ 3200]\n",
            "loss: 0.897860  [   45/ 3200]\n",
            "loss: 0.941072  [   46/ 3200]\n",
            "loss: 0.806012  [   47/ 3200]\n",
            "loss: 1.176432  [   48/ 3200]\n",
            "loss: 1.069345  [   49/ 3200]\n",
            "loss: 1.100934  [   50/ 3200]\n",
            "loss: 0.837470  [   51/ 3200]\n",
            "loss: 0.945999  [   52/ 3200]\n",
            "loss: 0.955730  [   53/ 3200]\n",
            "loss: 1.122757  [   54/ 3200]\n",
            "loss: 1.118539  [   55/ 3200]\n",
            "loss: 0.787347  [   56/ 3200]\n",
            "loss: 0.692298  [   57/ 3200]\n",
            "loss: 1.079131  [   58/ 3200]\n",
            "loss: 0.923517  [   59/ 3200]\n",
            "loss: 0.777397  [   60/ 3200]\n",
            "loss: 1.199474  [   61/ 3200]\n",
            "loss: 0.983525  [   62/ 3200]\n",
            "loss: 1.084199  [   63/ 3200]\n",
            "loss: 1.130473  [   64/ 3200]\n",
            "loss: 0.976997  [   65/ 3200]\n",
            "loss: 0.898285  [   66/ 3200]\n",
            "loss: 0.784249  [   67/ 3200]\n",
            "loss: 1.060547  [   68/ 3200]\n",
            "loss: 0.872267  [   69/ 3200]\n",
            "loss: 0.774506  [   70/ 3200]\n",
            "loss: 1.282808  [   71/ 3200]\n",
            "loss: 1.068366  [   72/ 3200]\n",
            "loss: 0.628027  [   73/ 3200]\n",
            "loss: 0.877632  [   74/ 3200]\n",
            "loss: 0.954989  [   75/ 3200]\n",
            "loss: 1.337048  [   76/ 3200]\n",
            "loss: 1.028609  [   77/ 3200]\n",
            "loss: 0.753021  [   78/ 3200]\n",
            "loss: 1.355375  [   79/ 3200]\n",
            "loss: 1.354383  [   80/ 3200]\n",
            "loss: 1.259440  [   81/ 3200]\n",
            "loss: 1.016536  [   82/ 3200]\n",
            "loss: 1.055022  [   83/ 3200]\n",
            "loss: 1.162093  [   84/ 3200]\n",
            "loss: 0.917666  [   85/ 3200]\n",
            "loss: 0.892926  [   86/ 3200]\n",
            "loss: 0.938899  [   87/ 3200]\n",
            "loss: 1.208711  [   88/ 3200]\n",
            "loss: 1.108100  [   89/ 3200]\n",
            "loss: 1.000978  [   90/ 3200]\n",
            "loss: 0.968906  [   91/ 3200]\n",
            "loss: 0.817506  [   92/ 3200]\n",
            "loss: 1.026351  [   93/ 3200]\n",
            "loss: 0.890423  [   94/ 3200]\n",
            "loss: 0.814642  [   95/ 3200]\n",
            "loss: 1.167407  [   96/ 3200]\n",
            "loss: 1.304499  [   97/ 3200]\n",
            "loss: 1.129441  [   98/ 3200]\n",
            "loss: 0.979718  [   99/ 3200]\n",
            "loss: 1.003951  [  100/ 3200]\n",
            "loss: 1.110896  [  101/ 3200]\n",
            "loss: 0.868693  [  102/ 3200]\n",
            "loss: 0.884536  [  103/ 3200]\n",
            "loss: 0.842446  [  104/ 3200]\n",
            "loss: 1.222252  [  105/ 3200]\n",
            "loss: 1.241667  [  106/ 3200]\n",
            "loss: 1.059125  [  107/ 3200]\n",
            "loss: 0.787440  [  108/ 3200]\n",
            "loss: 0.799418  [  109/ 3200]\n",
            "loss: 0.957189  [  110/ 3200]\n",
            "loss: 1.413741  [  111/ 3200]\n",
            "loss: 1.144082  [  112/ 3200]\n",
            "loss: 1.247815  [  113/ 3200]\n",
            "loss: 0.869289  [  114/ 3200]\n",
            "loss: 0.892658  [  115/ 3200]\n",
            "loss: 1.097207  [  116/ 3200]\n",
            "loss: 0.764932  [  117/ 3200]\n",
            "loss: 0.929653  [  118/ 3200]\n",
            "loss: 1.243035  [  119/ 3200]\n",
            "loss: 1.211409  [  120/ 3200]\n",
            "loss: 0.969319  [  121/ 3200]\n",
            "loss: 0.815538  [  122/ 3200]\n",
            "loss: 0.864235  [  123/ 3200]\n",
            "loss: 0.850452  [  124/ 3200]\n",
            "loss: 0.730604  [  125/ 3200]\n",
            "loss: 1.012935  [  126/ 3200]\n",
            "loss: 1.162825  [  127/ 3200]\n",
            "loss: 1.242410  [  128/ 3200]\n",
            "loss: 0.976499  [  129/ 3200]\n",
            "loss: 0.881403  [  130/ 3200]\n",
            "loss: 0.909880  [  131/ 3200]\n",
            "loss: 0.968907  [  132/ 3200]\n",
            "loss: 1.128601  [  133/ 3200]\n",
            "loss: 0.969884  [  134/ 3200]\n",
            "loss: 0.984198  [  135/ 3200]\n",
            "loss: 0.897814  [  136/ 3200]\n",
            "loss: 0.970006  [  137/ 3200]\n",
            "loss: 0.901111  [  138/ 3200]\n",
            "loss: 0.893525  [  139/ 3200]\n",
            "loss: 1.145946  [  140/ 3200]\n",
            "loss: 0.883975  [  141/ 3200]\n",
            "loss: 0.918321  [  142/ 3200]\n",
            "loss: 0.883305  [  143/ 3200]\n",
            "loss: 1.007650  [  144/ 3200]\n",
            "loss: 0.839573  [  145/ 3200]\n",
            "loss: 1.010466  [  146/ 3200]\n",
            "loss: 0.742934  [  147/ 3200]\n",
            "loss: 1.046103  [  148/ 3200]\n",
            "loss: 0.904445  [  149/ 3200]\n",
            "loss: 0.928775  [  150/ 3200]\n",
            "loss: 1.143267  [  151/ 3200]\n",
            "loss: 1.271641  [  152/ 3200]\n",
            "loss: 0.897603  [  153/ 3200]\n",
            "loss: 1.104317  [  154/ 3200]\n",
            "loss: 0.725304  [  155/ 3200]\n",
            "loss: 1.230853  [  156/ 3200]\n",
            "loss: 1.040508  [  157/ 3200]\n",
            "loss: 1.152198  [  158/ 3200]\n",
            "loss: 1.075545  [  159/ 3200]\n",
            "loss: 0.814328  [  160/ 3200]\n",
            "loss: 1.084461  [  161/ 3200]\n",
            "loss: 1.336727  [  162/ 3200]\n",
            "loss: 0.896226  [  163/ 3200]\n",
            "loss: 0.816940  [  164/ 3200]\n",
            "loss: 1.070253  [  165/ 3200]\n",
            "loss: 0.908573  [  166/ 3200]\n",
            "loss: 0.933578  [  167/ 3200]\n",
            "loss: 0.610122  [  168/ 3200]\n",
            "loss: 1.069566  [  169/ 3200]\n",
            "loss: 0.906864  [  170/ 3200]\n",
            "loss: 0.923710  [  171/ 3200]\n",
            "loss: 0.900064  [  172/ 3200]\n",
            "loss: 1.070109  [  173/ 3200]\n",
            "loss: 0.912321  [  174/ 3200]\n",
            "loss: 1.027815  [  175/ 3200]\n",
            "loss: 1.291656  [  176/ 3200]\n",
            "loss: 1.117827  [  177/ 3200]\n",
            "loss: 1.347417  [  178/ 3200]\n",
            "loss: 0.719736  [  179/ 3200]\n",
            "loss: 1.015230  [  180/ 3200]\n",
            "loss: 0.850937  [  181/ 3200]\n",
            "loss: 0.913901  [  182/ 3200]\n",
            "loss: 1.188768  [  183/ 3200]\n",
            "loss: 0.936716  [  184/ 3200]\n",
            "loss: 1.054912  [  185/ 3200]\n",
            "loss: 0.811128  [  186/ 3200]\n",
            "loss: 0.979311  [  187/ 3200]\n",
            "loss: 0.967723  [  188/ 3200]\n",
            "loss: 1.039112  [  189/ 3200]\n",
            "loss: 1.115474  [  190/ 3200]\n",
            "loss: 0.989643  [  191/ 3200]\n",
            "loss: 0.777246  [  192/ 3200]\n",
            "loss: 0.783684  [  193/ 3200]\n",
            "loss: 1.054736  [  194/ 3200]\n",
            "loss: 0.941102  [  195/ 3200]\n",
            "loss: 0.969724  [  196/ 3200]\n",
            "loss: 1.133251  [  197/ 3200]\n",
            "loss: 0.864339  [  198/ 3200]\n",
            "loss: 0.885141  [  199/ 3200]\n",
            "Epoch:  27\n",
            "loss: 0.843935  [    0/ 3200]\n",
            "loss: 1.188667  [    1/ 3200]\n",
            "loss: 1.080145  [    2/ 3200]\n",
            "loss: 1.167391  [    3/ 3200]\n",
            "loss: 0.887784  [    4/ 3200]\n",
            "loss: 0.864975  [    5/ 3200]\n",
            "loss: 0.793052  [    6/ 3200]\n",
            "loss: 0.937675  [    7/ 3200]\n",
            "loss: 0.814587  [    8/ 3200]\n",
            "loss: 0.948388  [    9/ 3200]\n",
            "loss: 1.042705  [   10/ 3200]\n",
            "loss: 1.058862  [   11/ 3200]\n",
            "loss: 0.947784  [   12/ 3200]\n",
            "loss: 1.128285  [   13/ 3200]\n",
            "loss: 1.193648  [   14/ 3200]\n",
            "loss: 1.032302  [   15/ 3200]\n",
            "loss: 1.015551  [   16/ 3200]\n",
            "loss: 0.815046  [   17/ 3200]\n",
            "loss: 0.961388  [   18/ 3200]\n",
            "loss: 1.028966  [   19/ 3200]\n",
            "loss: 0.825525  [   20/ 3200]\n",
            "loss: 1.262094  [   21/ 3200]\n",
            "loss: 0.974403  [   22/ 3200]\n",
            "loss: 0.918697  [   23/ 3200]\n",
            "loss: 0.981528  [   24/ 3200]\n",
            "loss: 1.218146  [   25/ 3200]\n",
            "loss: 1.053827  [   26/ 3200]\n",
            "loss: 1.008967  [   27/ 3200]\n",
            "loss: 1.309742  [   28/ 3200]\n",
            "loss: 0.759092  [   29/ 3200]\n",
            "loss: 0.905082  [   30/ 3200]\n",
            "loss: 1.072577  [   31/ 3200]\n",
            "loss: 1.154118  [   32/ 3200]\n",
            "loss: 0.763046  [   33/ 3200]\n",
            "loss: 0.983034  [   34/ 3200]\n",
            "loss: 1.070836  [   35/ 3200]\n",
            "loss: 0.790229  [   36/ 3200]\n",
            "loss: 0.955966  [   37/ 3200]\n",
            "loss: 0.781828  [   38/ 3200]\n",
            "loss: 0.858742  [   39/ 3200]\n",
            "loss: 0.613150  [   40/ 3200]\n",
            "loss: 0.973517  [   41/ 3200]\n",
            "loss: 1.106796  [   42/ 3200]\n",
            "loss: 1.086790  [   43/ 3200]\n",
            "loss: 0.930123  [   44/ 3200]\n",
            "loss: 0.995255  [   45/ 3200]\n",
            "loss: 1.128801  [   46/ 3200]\n",
            "loss: 1.144217  [   47/ 3200]\n",
            "loss: 0.842699  [   48/ 3200]\n",
            "loss: 0.991386  [   49/ 3200]\n",
            "loss: 0.948262  [   50/ 3200]\n",
            "loss: 0.993111  [   51/ 3200]\n",
            "loss: 0.790878  [   52/ 3200]\n",
            "loss: 1.227740  [   53/ 3200]\n",
            "loss: 0.969940  [   54/ 3200]\n",
            "loss: 0.944641  [   55/ 3200]\n",
            "loss: 1.133834  [   56/ 3200]\n",
            "loss: 1.062517  [   57/ 3200]\n",
            "loss: 1.357896  [   58/ 3200]\n",
            "loss: 0.805276  [   59/ 3200]\n",
            "loss: 1.009348  [   60/ 3200]\n",
            "loss: 0.988139  [   61/ 3200]\n",
            "loss: 0.952915  [   62/ 3200]\n",
            "loss: 1.014604  [   63/ 3200]\n",
            "loss: 0.987510  [   64/ 3200]\n",
            "loss: 1.237414  [   65/ 3200]\n",
            "loss: 0.809941  [   66/ 3200]\n",
            "loss: 0.868604  [   67/ 3200]\n",
            "loss: 0.762115  [   68/ 3200]\n",
            "loss: 0.844286  [   69/ 3200]\n",
            "loss: 0.872026  [   70/ 3200]\n",
            "loss: 1.124364  [   71/ 3200]\n",
            "loss: 0.843139  [   72/ 3200]\n",
            "loss: 1.011586  [   73/ 3200]\n",
            "loss: 0.994344  [   74/ 3200]\n",
            "loss: 0.983947  [   75/ 3200]\n",
            "loss: 0.821643  [   76/ 3200]\n",
            "loss: 0.774740  [   77/ 3200]\n",
            "loss: 0.956199  [   78/ 3200]\n",
            "loss: 0.939900  [   79/ 3200]\n",
            "loss: 1.013991  [   80/ 3200]\n",
            "loss: 0.836540  [   81/ 3200]\n",
            "loss: 1.053084  [   82/ 3200]\n",
            "loss: 0.838933  [   83/ 3200]\n",
            "loss: 0.858612  [   84/ 3200]\n",
            "loss: 0.642248  [   85/ 3200]\n",
            "loss: 0.772251  [   86/ 3200]\n",
            "loss: 0.933737  [   87/ 3200]\n",
            "loss: 0.894988  [   88/ 3200]\n",
            "loss: 1.168412  [   89/ 3200]\n",
            "loss: 0.904553  [   90/ 3200]\n",
            "loss: 0.836863  [   91/ 3200]\n",
            "loss: 1.253681  [   92/ 3200]\n",
            "loss: 1.101647  [   93/ 3200]\n",
            "loss: 1.042752  [   94/ 3200]\n",
            "loss: 1.071053  [   95/ 3200]\n",
            "loss: 0.814678  [   96/ 3200]\n",
            "loss: 0.869780  [   97/ 3200]\n",
            "loss: 1.060397  [   98/ 3200]\n",
            "loss: 0.911936  [   99/ 3200]\n",
            "loss: 1.020622  [  100/ 3200]\n",
            "loss: 0.981980  [  101/ 3200]\n",
            "loss: 1.004852  [  102/ 3200]\n",
            "loss: 1.096283  [  103/ 3200]\n",
            "loss: 0.971967  [  104/ 3200]\n",
            "loss: 0.814463  [  105/ 3200]\n",
            "loss: 1.051043  [  106/ 3200]\n",
            "loss: 1.043863  [  107/ 3200]\n",
            "loss: 1.060990  [  108/ 3200]\n",
            "loss: 1.060789  [  109/ 3200]\n",
            "loss: 0.779966  [  110/ 3200]\n",
            "loss: 0.951630  [  111/ 3200]\n",
            "loss: 1.184795  [  112/ 3200]\n",
            "loss: 1.149682  [  113/ 3200]\n",
            "loss: 0.842446  [  114/ 3200]\n",
            "loss: 1.008052  [  115/ 3200]\n",
            "loss: 1.077692  [  116/ 3200]\n",
            "loss: 1.256974  [  117/ 3200]\n",
            "loss: 0.901985  [  118/ 3200]\n",
            "loss: 0.967947  [  119/ 3200]\n",
            "loss: 0.988722  [  120/ 3200]\n",
            "loss: 0.826383  [  121/ 3200]\n",
            "loss: 0.907753  [  122/ 3200]\n",
            "loss: 0.967718  [  123/ 3200]\n",
            "loss: 0.817369  [  124/ 3200]\n",
            "loss: 1.036857  [  125/ 3200]\n",
            "loss: 1.190312  [  126/ 3200]\n",
            "loss: 0.842684  [  127/ 3200]\n",
            "loss: 1.063904  [  128/ 3200]\n",
            "loss: 0.883977  [  129/ 3200]\n",
            "loss: 0.829947  [  130/ 3200]\n",
            "loss: 1.026822  [  131/ 3200]\n",
            "loss: 0.969095  [  132/ 3200]\n",
            "loss: 0.971504  [  133/ 3200]\n",
            "loss: 0.901268  [  134/ 3200]\n",
            "loss: 1.080156  [  135/ 3200]\n",
            "loss: 0.825689  [  136/ 3200]\n",
            "loss: 0.888327  [  137/ 3200]\n",
            "loss: 1.045215  [  138/ 3200]\n",
            "loss: 0.783165  [  139/ 3200]\n",
            "loss: 1.384034  [  140/ 3200]\n",
            "loss: 0.874187  [  141/ 3200]\n",
            "loss: 1.193198  [  142/ 3200]\n",
            "loss: 1.059730  [  143/ 3200]\n",
            "loss: 0.892895  [  144/ 3200]\n",
            "loss: 1.028986  [  145/ 3200]\n",
            "loss: 1.010008  [  146/ 3200]\n",
            "loss: 1.108331  [  147/ 3200]\n",
            "loss: 1.247390  [  148/ 3200]\n",
            "loss: 1.035652  [  149/ 3200]\n",
            "loss: 0.898945  [  150/ 3200]\n",
            "loss: 0.932703  [  151/ 3200]\n",
            "loss: 1.200114  [  152/ 3200]\n",
            "loss: 0.862562  [  153/ 3200]\n",
            "loss: 1.177389  [  154/ 3200]\n",
            "loss: 0.780617  [  155/ 3200]\n",
            "loss: 0.951255  [  156/ 3200]\n",
            "loss: 0.912487  [  157/ 3200]\n",
            "loss: 0.860428  [  158/ 3200]\n",
            "loss: 0.723505  [  159/ 3200]\n",
            "loss: 0.832010  [  160/ 3200]\n",
            "loss: 0.993521  [  161/ 3200]\n",
            "loss: 0.831234  [  162/ 3200]\n",
            "loss: 0.981837  [  163/ 3200]\n",
            "loss: 0.963560  [  164/ 3200]\n",
            "loss: 0.908594  [  165/ 3200]\n",
            "loss: 0.955370  [  166/ 3200]\n",
            "loss: 0.768514  [  167/ 3200]\n",
            "loss: 1.080588  [  168/ 3200]\n",
            "loss: 0.887278  [  169/ 3200]\n",
            "loss: 1.240132  [  170/ 3200]\n",
            "loss: 0.900142  [  171/ 3200]\n",
            "loss: 0.705075  [  172/ 3200]\n",
            "loss: 0.998101  [  173/ 3200]\n",
            "loss: 0.902029  [  174/ 3200]\n",
            "loss: 1.110194  [  175/ 3200]\n",
            "loss: 0.904483  [  176/ 3200]\n",
            "loss: 0.905104  [  177/ 3200]\n",
            "loss: 1.206163  [  178/ 3200]\n",
            "loss: 0.931084  [  179/ 3200]\n",
            "loss: 1.072867  [  180/ 3200]\n",
            "loss: 1.070416  [  181/ 3200]\n",
            "loss: 0.957950  [  182/ 3200]\n",
            "loss: 0.982636  [  183/ 3200]\n",
            "loss: 1.032211  [  184/ 3200]\n",
            "loss: 0.991544  [  185/ 3200]\n",
            "loss: 1.028608  [  186/ 3200]\n",
            "loss: 1.040042  [  187/ 3200]\n",
            "loss: 0.883806  [  188/ 3200]\n",
            "loss: 1.118509  [  189/ 3200]\n",
            "loss: 0.778704  [  190/ 3200]\n",
            "loss: 0.861963  [  191/ 3200]\n",
            "loss: 1.000722  [  192/ 3200]\n",
            "loss: 0.974016  [  193/ 3200]\n",
            "loss: 0.707653  [  194/ 3200]\n",
            "loss: 1.119552  [  195/ 3200]\n",
            "loss: 0.883504  [  196/ 3200]\n",
            "loss: 0.775791  [  197/ 3200]\n",
            "loss: 0.832366  [  198/ 3200]\n",
            "loss: 0.916529  [  199/ 3200]\n",
            "Epoch:  28\n",
            "loss: 1.045154  [    0/ 3200]\n",
            "loss: 0.971533  [    1/ 3200]\n",
            "loss: 0.827641  [    2/ 3200]\n",
            "loss: 1.016224  [    3/ 3200]\n",
            "loss: 0.767181  [    4/ 3200]\n",
            "loss: 1.148440  [    5/ 3200]\n",
            "loss: 0.926057  [    6/ 3200]\n",
            "loss: 0.983462  [    7/ 3200]\n",
            "loss: 0.612109  [    8/ 3200]\n",
            "loss: 0.914574  [    9/ 3200]\n",
            "loss: 0.858365  [   10/ 3200]\n",
            "loss: 0.885281  [   11/ 3200]\n",
            "loss: 1.044016  [   12/ 3200]\n",
            "loss: 0.879141  [   13/ 3200]\n",
            "loss: 1.015534  [   14/ 3200]\n",
            "loss: 0.868200  [   15/ 3200]\n",
            "loss: 1.110735  [   16/ 3200]\n",
            "loss: 0.863212  [   17/ 3200]\n",
            "loss: 0.973656  [   18/ 3200]\n",
            "loss: 0.942242  [   19/ 3200]\n",
            "loss: 0.981328  [   20/ 3200]\n",
            "loss: 0.920989  [   21/ 3200]\n",
            "loss: 0.952145  [   22/ 3200]\n",
            "loss: 1.096086  [   23/ 3200]\n",
            "loss: 0.742102  [   24/ 3200]\n",
            "loss: 1.092437  [   25/ 3200]\n",
            "loss: 0.895331  [   26/ 3200]\n",
            "loss: 0.825917  [   27/ 3200]\n",
            "loss: 1.103845  [   28/ 3200]\n",
            "loss: 0.970959  [   29/ 3200]\n",
            "loss: 0.707151  [   30/ 3200]\n",
            "loss: 0.841031  [   31/ 3200]\n",
            "loss: 1.086734  [   32/ 3200]\n",
            "loss: 0.887966  [   33/ 3200]\n",
            "loss: 1.190517  [   34/ 3200]\n",
            "loss: 1.082292  [   35/ 3200]\n",
            "loss: 1.053367  [   36/ 3200]\n",
            "loss: 1.099276  [   37/ 3200]\n",
            "loss: 1.036634  [   38/ 3200]\n",
            "loss: 0.831995  [   39/ 3200]\n",
            "loss: 0.682813  [   40/ 3200]\n",
            "loss: 0.965807  [   41/ 3200]\n",
            "loss: 0.995694  [   42/ 3200]\n",
            "loss: 0.972996  [   43/ 3200]\n",
            "loss: 0.664880  [   44/ 3200]\n",
            "loss: 0.913278  [   45/ 3200]\n",
            "loss: 1.273149  [   46/ 3200]\n",
            "loss: 0.875405  [   47/ 3200]\n",
            "loss: 0.985051  [   48/ 3200]\n",
            "loss: 1.025991  [   49/ 3200]\n",
            "loss: 0.962018  [   50/ 3200]\n",
            "loss: 0.907026  [   51/ 3200]\n",
            "loss: 1.149729  [   52/ 3200]\n",
            "loss: 0.732248  [   53/ 3200]\n",
            "loss: 1.047266  [   54/ 3200]\n",
            "loss: 0.787275  [   55/ 3200]\n",
            "loss: 0.956280  [   56/ 3200]\n",
            "loss: 1.070686  [   57/ 3200]\n",
            "loss: 1.210639  [   58/ 3200]\n",
            "loss: 1.028988  [   59/ 3200]\n",
            "loss: 0.882507  [   60/ 3200]\n",
            "loss: 0.711697  [   61/ 3200]\n",
            "loss: 0.840254  [   62/ 3200]\n",
            "loss: 0.924590  [   63/ 3200]\n",
            "loss: 1.156823  [   64/ 3200]\n",
            "loss: 0.925303  [   65/ 3200]\n",
            "loss: 0.822211  [   66/ 3200]\n",
            "loss: 1.242619  [   67/ 3200]\n",
            "loss: 0.965487  [   68/ 3200]\n",
            "loss: 1.068851  [   69/ 3200]\n",
            "loss: 0.931809  [   70/ 3200]\n",
            "loss: 1.196377  [   71/ 3200]\n",
            "loss: 1.043038  [   72/ 3200]\n",
            "loss: 0.966509  [   73/ 3200]\n",
            "loss: 1.005628  [   74/ 3200]\n",
            "loss: 1.043915  [   75/ 3200]\n",
            "loss: 0.991935  [   76/ 3200]\n",
            "loss: 0.981831  [   77/ 3200]\n",
            "loss: 1.159214  [   78/ 3200]\n",
            "loss: 1.170958  [   79/ 3200]\n",
            "loss: 1.298855  [   80/ 3200]\n",
            "loss: 0.776418  [   81/ 3200]\n",
            "loss: 0.900352  [   82/ 3200]\n",
            "loss: 0.830685  [   83/ 3200]\n",
            "loss: 0.929184  [   84/ 3200]\n",
            "loss: 0.923969  [   85/ 3200]\n",
            "loss: 0.980173  [   86/ 3200]\n",
            "loss: 1.077050  [   87/ 3200]\n",
            "loss: 0.825687  [   88/ 3200]\n",
            "loss: 1.021039  [   89/ 3200]\n",
            "loss: 0.866639  [   90/ 3200]\n",
            "loss: 0.926903  [   91/ 3200]\n",
            "loss: 0.802691  [   92/ 3200]\n",
            "loss: 0.891381  [   93/ 3200]\n",
            "loss: 0.966739  [   94/ 3200]\n",
            "loss: 1.023208  [   95/ 3200]\n",
            "loss: 0.829168  [   96/ 3200]\n",
            "loss: 0.919476  [   97/ 3200]\n",
            "loss: 1.146188  [   98/ 3200]\n",
            "loss: 0.809982  [   99/ 3200]\n",
            "loss: 1.042432  [  100/ 3200]\n",
            "loss: 0.836639  [  101/ 3200]\n",
            "loss: 0.564204  [  102/ 3200]\n",
            "loss: 0.792024  [  103/ 3200]\n",
            "loss: 0.874726  [  104/ 3200]\n",
            "loss: 1.006062  [  105/ 3200]\n",
            "loss: 1.190188  [  106/ 3200]\n",
            "loss: 0.848164  [  107/ 3200]\n",
            "loss: 0.879168  [  108/ 3200]\n",
            "loss: 0.828177  [  109/ 3200]\n",
            "loss: 0.944349  [  110/ 3200]\n",
            "loss: 0.974689  [  111/ 3200]\n",
            "loss: 0.873587  [  112/ 3200]\n",
            "loss: 1.193784  [  113/ 3200]\n",
            "loss: 1.424168  [  114/ 3200]\n",
            "loss: 1.047490  [  115/ 3200]\n",
            "loss: 1.022958  [  116/ 3200]\n",
            "loss: 1.048039  [  117/ 3200]\n",
            "loss: 0.776111  [  118/ 3200]\n",
            "loss: 0.951763  [  119/ 3200]\n",
            "loss: 0.932463  [  120/ 3200]\n",
            "loss: 1.127083  [  121/ 3200]\n",
            "loss: 0.923133  [  122/ 3200]\n",
            "loss: 1.103733  [  123/ 3200]\n",
            "loss: 0.918286  [  124/ 3200]\n",
            "loss: 1.072370  [  125/ 3200]\n",
            "loss: 0.977432  [  126/ 3200]\n",
            "loss: 1.133337  [  127/ 3200]\n",
            "loss: 0.959668  [  128/ 3200]\n",
            "loss: 1.029342  [  129/ 3200]\n",
            "loss: 0.860440  [  130/ 3200]\n",
            "loss: 0.856948  [  131/ 3200]\n",
            "loss: 0.776231  [  132/ 3200]\n",
            "loss: 1.022725  [  133/ 3200]\n",
            "loss: 0.924838  [  134/ 3200]\n",
            "loss: 1.026921  [  135/ 3200]\n",
            "loss: 1.227815  [  136/ 3200]\n",
            "loss: 1.126089  [  137/ 3200]\n",
            "loss: 0.846533  [  138/ 3200]\n",
            "loss: 0.912124  [  139/ 3200]\n",
            "loss: 0.943210  [  140/ 3200]\n",
            "loss: 1.162543  [  141/ 3200]\n",
            "loss: 1.101624  [  142/ 3200]\n",
            "loss: 0.886220  [  143/ 3200]\n",
            "loss: 0.854411  [  144/ 3200]\n",
            "loss: 1.003989  [  145/ 3200]\n",
            "loss: 1.159585  [  146/ 3200]\n",
            "loss: 0.945671  [  147/ 3200]\n",
            "loss: 1.164651  [  148/ 3200]\n",
            "loss: 0.827841  [  149/ 3200]\n",
            "loss: 0.926979  [  150/ 3200]\n",
            "loss: 1.014681  [  151/ 3200]\n",
            "loss: 1.139143  [  152/ 3200]\n",
            "loss: 1.057726  [  153/ 3200]\n",
            "loss: 0.963392  [  154/ 3200]\n",
            "loss: 1.077330  [  155/ 3200]\n",
            "loss: 0.873272  [  156/ 3200]\n",
            "loss: 0.928672  [  157/ 3200]\n",
            "loss: 0.984422  [  158/ 3200]\n",
            "loss: 1.081755  [  159/ 3200]\n",
            "loss: 0.841601  [  160/ 3200]\n",
            "loss: 1.177855  [  161/ 3200]\n",
            "loss: 0.868309  [  162/ 3200]\n",
            "loss: 0.906991  [  163/ 3200]\n",
            "loss: 0.913337  [  164/ 3200]\n",
            "loss: 1.140252  [  165/ 3200]\n",
            "loss: 0.741028  [  166/ 3200]\n",
            "loss: 1.088722  [  167/ 3200]\n",
            "loss: 0.887750  [  168/ 3200]\n",
            "loss: 0.777198  [  169/ 3200]\n",
            "loss: 0.920647  [  170/ 3200]\n",
            "loss: 0.895662  [  171/ 3200]\n",
            "loss: 1.118700  [  172/ 3200]\n",
            "loss: 0.835073  [  173/ 3200]\n",
            "loss: 0.775734  [  174/ 3200]\n",
            "loss: 1.194283  [  175/ 3200]\n",
            "loss: 0.755565  [  176/ 3200]\n",
            "loss: 0.775917  [  177/ 3200]\n",
            "loss: 1.017045  [  178/ 3200]\n",
            "loss: 0.829733  [  179/ 3200]\n",
            "loss: 1.087918  [  180/ 3200]\n",
            "loss: 0.738538  [  181/ 3200]\n",
            "loss: 1.198958  [  182/ 3200]\n",
            "loss: 1.230963  [  183/ 3200]\n",
            "loss: 0.968589  [  184/ 3200]\n",
            "loss: 0.632651  [  185/ 3200]\n",
            "loss: 0.716241  [  186/ 3200]\n",
            "loss: 1.158762  [  187/ 3200]\n",
            "loss: 0.837153  [  188/ 3200]\n",
            "loss: 1.328040  [  189/ 3200]\n",
            "loss: 0.987311  [  190/ 3200]\n",
            "loss: 0.968194  [  191/ 3200]\n",
            "loss: 0.928641  [  192/ 3200]\n",
            "loss: 0.927673  [  193/ 3200]\n",
            "loss: 1.040708  [  194/ 3200]\n",
            "loss: 0.786747  [  195/ 3200]\n",
            "loss: 1.125748  [  196/ 3200]\n",
            "loss: 0.647380  [  197/ 3200]\n",
            "loss: 0.992879  [  198/ 3200]\n",
            "loss: 0.859294  [  199/ 3200]\n",
            "Epoch:  29\n",
            "loss: 1.038486  [    0/ 3200]\n",
            "loss: 0.937759  [    1/ 3200]\n",
            "loss: 0.862925  [    2/ 3200]\n",
            "loss: 0.790223  [    3/ 3200]\n",
            "loss: 0.792366  [    4/ 3200]\n",
            "loss: 0.860918  [    5/ 3200]\n",
            "loss: 0.823049  [    6/ 3200]\n",
            "loss: 0.944951  [    7/ 3200]\n",
            "loss: 0.970176  [    8/ 3200]\n",
            "loss: 0.933100  [    9/ 3200]\n",
            "loss: 0.949828  [   10/ 3200]\n",
            "loss: 0.998376  [   11/ 3200]\n",
            "loss: 1.057402  [   12/ 3200]\n",
            "loss: 0.916710  [   13/ 3200]\n",
            "loss: 0.950214  [   14/ 3200]\n",
            "loss: 0.759151  [   15/ 3200]\n",
            "loss: 1.169374  [   16/ 3200]\n",
            "loss: 0.795865  [   17/ 3200]\n",
            "loss: 0.931350  [   18/ 3200]\n",
            "loss: 0.864066  [   19/ 3200]\n",
            "loss: 1.187386  [   20/ 3200]\n",
            "loss: 0.944107  [   21/ 3200]\n",
            "loss: 0.981389  [   22/ 3200]\n",
            "loss: 0.823327  [   23/ 3200]\n",
            "loss: 1.000986  [   24/ 3200]\n",
            "loss: 0.767634  [   25/ 3200]\n",
            "loss: 0.997849  [   26/ 3200]\n",
            "loss: 0.911463  [   27/ 3200]\n",
            "loss: 0.796955  [   28/ 3200]\n",
            "loss: 0.993966  [   29/ 3200]\n",
            "loss: 0.947446  [   30/ 3200]\n",
            "loss: 1.069886  [   31/ 3200]\n",
            "loss: 0.966185  [   32/ 3200]\n",
            "loss: 0.800506  [   33/ 3200]\n",
            "loss: 0.839955  [   34/ 3200]\n",
            "loss: 0.975077  [   35/ 3200]\n",
            "loss: 0.932060  [   36/ 3200]\n",
            "loss: 1.002905  [   37/ 3200]\n",
            "loss: 0.820180  [   38/ 3200]\n",
            "loss: 0.996286  [   39/ 3200]\n",
            "loss: 0.717227  [   40/ 3200]\n",
            "loss: 1.102578  [   41/ 3200]\n",
            "loss: 1.228797  [   42/ 3200]\n",
            "loss: 1.004781  [   43/ 3200]\n",
            "loss: 1.153761  [   44/ 3200]\n",
            "loss: 1.095768  [   45/ 3200]\n",
            "loss: 1.007413  [   46/ 3200]\n",
            "loss: 0.896621  [   47/ 3200]\n",
            "loss: 0.937491  [   48/ 3200]\n",
            "loss: 1.160722  [   49/ 3200]\n",
            "loss: 0.846299  [   50/ 3200]\n",
            "loss: 0.896540  [   51/ 3200]\n",
            "loss: 0.907717  [   52/ 3200]\n",
            "loss: 1.119044  [   53/ 3200]\n",
            "loss: 0.832364  [   54/ 3200]\n",
            "loss: 1.002490  [   55/ 3200]\n",
            "loss: 1.205088  [   56/ 3200]\n",
            "loss: 0.755921  [   57/ 3200]\n",
            "loss: 0.863201  [   58/ 3200]\n",
            "loss: 0.919337  [   59/ 3200]\n",
            "loss: 1.198781  [   60/ 3200]\n",
            "loss: 1.193843  [   61/ 3200]\n",
            "loss: 0.889333  [   62/ 3200]\n",
            "loss: 1.083830  [   63/ 3200]\n",
            "loss: 1.048556  [   64/ 3200]\n",
            "loss: 1.228879  [   65/ 3200]\n",
            "loss: 0.814130  [   66/ 3200]\n",
            "loss: 0.814792  [   67/ 3200]\n",
            "loss: 0.701451  [   68/ 3200]\n",
            "loss: 0.931663  [   69/ 3200]\n",
            "loss: 0.828394  [   70/ 3200]\n",
            "loss: 0.794235  [   71/ 3200]\n",
            "loss: 0.814337  [   72/ 3200]\n",
            "loss: 0.892149  [   73/ 3200]\n",
            "loss: 0.876005  [   74/ 3200]\n",
            "loss: 1.003536  [   75/ 3200]\n",
            "loss: 0.918591  [   76/ 3200]\n",
            "loss: 1.049817  [   77/ 3200]\n",
            "loss: 0.797611  [   78/ 3200]\n",
            "loss: 1.136117  [   79/ 3200]\n",
            "loss: 0.941570  [   80/ 3200]\n",
            "loss: 1.220180  [   81/ 3200]\n",
            "loss: 0.930075  [   82/ 3200]\n",
            "loss: 1.191054  [   83/ 3200]\n",
            "loss: 0.869513  [   84/ 3200]\n",
            "loss: 0.978847  [   85/ 3200]\n",
            "loss: 0.759257  [   86/ 3200]\n",
            "loss: 0.960645  [   87/ 3200]\n",
            "loss: 0.875845  [   88/ 3200]\n",
            "loss: 0.751329  [   89/ 3200]\n",
            "loss: 0.841033  [   90/ 3200]\n",
            "loss: 0.870061  [   91/ 3200]\n",
            "loss: 1.003431  [   92/ 3200]\n",
            "loss: 1.031259  [   93/ 3200]\n",
            "loss: 0.956230  [   94/ 3200]\n",
            "loss: 0.938420  [   95/ 3200]\n",
            "loss: 0.771885  [   96/ 3200]\n",
            "loss: 0.997048  [   97/ 3200]\n",
            "loss: 0.954330  [   98/ 3200]\n",
            "loss: 1.058443  [   99/ 3200]\n",
            "loss: 0.831891  [  100/ 3200]\n",
            "loss: 1.445149  [  101/ 3200]\n",
            "loss: 0.691406  [  102/ 3200]\n",
            "loss: 1.094285  [  103/ 3200]\n",
            "loss: 0.964142  [  104/ 3200]\n",
            "loss: 0.857605  [  105/ 3200]\n",
            "loss: 1.182676  [  106/ 3200]\n",
            "loss: 1.015500  [  107/ 3200]\n",
            "loss: 0.773222  [  108/ 3200]\n",
            "loss: 0.975661  [  109/ 3200]\n",
            "loss: 0.899091  [  110/ 3200]\n",
            "loss: 0.982923  [  111/ 3200]\n",
            "loss: 0.823031  [  112/ 3200]\n",
            "loss: 0.775799  [  113/ 3200]\n",
            "loss: 0.953981  [  114/ 3200]\n",
            "loss: 0.800388  [  115/ 3200]\n",
            "loss: 0.912665  [  116/ 3200]\n",
            "loss: 0.955418  [  117/ 3200]\n",
            "loss: 1.039037  [  118/ 3200]\n",
            "loss: 0.713185  [  119/ 3200]\n",
            "loss: 0.746403  [  120/ 3200]\n",
            "loss: 1.007930  [  121/ 3200]\n",
            "loss: 1.069074  [  122/ 3200]\n",
            "loss: 0.996668  [  123/ 3200]\n",
            "loss: 0.888500  [  124/ 3200]\n",
            "loss: 0.854349  [  125/ 3200]\n",
            "loss: 1.024871  [  126/ 3200]\n",
            "loss: 0.912017  [  127/ 3200]\n",
            "loss: 0.711830  [  128/ 3200]\n",
            "loss: 1.062423  [  129/ 3200]\n",
            "loss: 0.928621  [  130/ 3200]\n",
            "loss: 0.985061  [  131/ 3200]\n",
            "loss: 0.785556  [  132/ 3200]\n",
            "loss: 0.980894  [  133/ 3200]\n",
            "loss: 1.091570  [  134/ 3200]\n",
            "loss: 1.292857  [  135/ 3200]\n",
            "loss: 0.991375  [  136/ 3200]\n",
            "loss: 1.002632  [  137/ 3200]\n",
            "loss: 1.169540  [  138/ 3200]\n",
            "loss: 0.818583  [  139/ 3200]\n",
            "loss: 0.923691  [  140/ 3200]\n",
            "loss: 0.947792  [  141/ 3200]\n",
            "loss: 0.878012  [  142/ 3200]\n",
            "loss: 0.814619  [  143/ 3200]\n",
            "loss: 0.942076  [  144/ 3200]\n",
            "loss: 1.159930  [  145/ 3200]\n",
            "loss: 1.273403  [  146/ 3200]\n",
            "loss: 0.784394  [  147/ 3200]\n",
            "loss: 0.949564  [  148/ 3200]\n",
            "loss: 0.954719  [  149/ 3200]\n",
            "loss: 1.296911  [  150/ 3200]\n",
            "loss: 1.087129  [  151/ 3200]\n",
            "loss: 1.051842  [  152/ 3200]\n",
            "loss: 0.999397  [  153/ 3200]\n",
            "loss: 1.140348  [  154/ 3200]\n",
            "loss: 1.087053  [  155/ 3200]\n",
            "loss: 0.867766  [  156/ 3200]\n",
            "loss: 1.036996  [  157/ 3200]\n",
            "loss: 0.894798  [  158/ 3200]\n",
            "loss: 1.276234  [  159/ 3200]\n",
            "loss: 1.018833  [  160/ 3200]\n",
            "loss: 0.965128  [  161/ 3200]\n",
            "loss: 0.947073  [  162/ 3200]\n",
            "loss: 0.682964  [  163/ 3200]\n",
            "loss: 1.302530  [  164/ 3200]\n",
            "loss: 1.269451  [  165/ 3200]\n",
            "loss: 0.883727  [  166/ 3200]\n",
            "loss: 0.828592  [  167/ 3200]\n",
            "loss: 1.016095  [  168/ 3200]\n",
            "loss: 0.892888  [  169/ 3200]\n",
            "loss: 0.894042  [  170/ 3200]\n",
            "loss: 1.075542  [  171/ 3200]\n",
            "loss: 0.690488  [  172/ 3200]\n",
            "loss: 0.830167  [  173/ 3200]\n",
            "loss: 0.825269  [  174/ 3200]\n",
            "loss: 1.126986  [  175/ 3200]\n",
            "loss: 0.853271  [  176/ 3200]\n",
            "loss: 0.723303  [  177/ 3200]\n",
            "loss: 0.864308  [  178/ 3200]\n",
            "loss: 0.755571  [  179/ 3200]\n",
            "loss: 0.905103  [  180/ 3200]\n",
            "loss: 0.751004  [  181/ 3200]\n",
            "loss: 0.855710  [  182/ 3200]\n",
            "loss: 1.137645  [  183/ 3200]\n",
            "loss: 0.891357  [  184/ 3200]\n",
            "loss: 1.133864  [  185/ 3200]\n",
            "loss: 0.932168  [  186/ 3200]\n",
            "loss: 0.873302  [  187/ 3200]\n",
            "loss: 1.054538  [  188/ 3200]\n",
            "loss: 0.998658  [  189/ 3200]\n",
            "loss: 0.835287  [  190/ 3200]\n",
            "loss: 1.081620  [  191/ 3200]\n",
            "loss: 1.234180  [  192/ 3200]\n",
            "loss: 1.023369  [  193/ 3200]\n",
            "loss: 1.007514  [  194/ 3200]\n",
            "loss: 0.848186  [  195/ 3200]\n",
            "loss: 0.853869  [  196/ 3200]\n",
            "loss: 1.024510  [  197/ 3200]\n",
            "loss: 0.708113  [  198/ 3200]\n",
            "loss: 1.111594  [  199/ 3200]\n",
            "Epoch:  30\n",
            "loss: 0.977596  [    0/ 3200]\n",
            "loss: 0.783784  [    1/ 3200]\n",
            "loss: 0.818974  [    2/ 3200]\n",
            "loss: 1.165896  [    3/ 3200]\n",
            "loss: 1.224493  [    4/ 3200]\n",
            "loss: 0.973991  [    5/ 3200]\n",
            "loss: 0.771329  [    6/ 3200]\n",
            "loss: 1.007994  [    7/ 3200]\n",
            "loss: 0.924879  [    8/ 3200]\n",
            "loss: 0.842876  [    9/ 3200]\n",
            "loss: 1.005510  [   10/ 3200]\n",
            "loss: 0.868046  [   11/ 3200]\n",
            "loss: 0.700594  [   12/ 3200]\n",
            "loss: 0.763588  [   13/ 3200]\n",
            "loss: 0.983532  [   14/ 3200]\n",
            "loss: 0.892991  [   15/ 3200]\n",
            "loss: 1.219188  [   16/ 3200]\n",
            "loss: 1.065611  [   17/ 3200]\n",
            "loss: 0.955153  [   18/ 3200]\n",
            "loss: 0.879357  [   19/ 3200]\n",
            "loss: 0.751414  [   20/ 3200]\n",
            "loss: 0.897377  [   21/ 3200]\n",
            "loss: 0.909310  [   22/ 3200]\n",
            "loss: 0.647109  [   23/ 3200]\n",
            "loss: 1.091473  [   24/ 3200]\n",
            "loss: 1.051250  [   25/ 3200]\n",
            "loss: 1.002018  [   26/ 3200]\n",
            "loss: 1.161177  [   27/ 3200]\n",
            "loss: 0.764416  [   28/ 3200]\n",
            "loss: 0.884623  [   29/ 3200]\n",
            "loss: 0.762845  [   30/ 3200]\n",
            "loss: 1.093996  [   31/ 3200]\n",
            "loss: 0.992501  [   32/ 3200]\n",
            "loss: 1.311983  [   33/ 3200]\n",
            "loss: 0.932740  [   34/ 3200]\n",
            "loss: 1.019496  [   35/ 3200]\n",
            "loss: 1.050352  [   36/ 3200]\n",
            "loss: 0.980093  [   37/ 3200]\n",
            "loss: 1.135531  [   38/ 3200]\n",
            "loss: 0.739225  [   39/ 3200]\n",
            "loss: 0.880122  [   40/ 3200]\n",
            "loss: 0.685406  [   41/ 3200]\n",
            "loss: 1.053203  [   42/ 3200]\n",
            "loss: 1.131086  [   43/ 3200]\n",
            "loss: 1.231532  [   44/ 3200]\n",
            "loss: 1.103022  [   45/ 3200]\n",
            "loss: 0.967645  [   46/ 3200]\n",
            "loss: 0.901022  [   47/ 3200]\n",
            "loss: 1.007338  [   48/ 3200]\n",
            "loss: 0.880438  [   49/ 3200]\n",
            "loss: 0.797993  [   50/ 3200]\n",
            "loss: 1.253465  [   51/ 3200]\n",
            "loss: 1.086945  [   52/ 3200]\n",
            "loss: 0.981030  [   53/ 3200]\n",
            "loss: 0.868024  [   54/ 3200]\n",
            "loss: 0.874178  [   55/ 3200]\n",
            "loss: 1.119879  [   56/ 3200]\n",
            "loss: 0.910068  [   57/ 3200]\n",
            "loss: 0.649680  [   58/ 3200]\n",
            "loss: 0.991391  [   59/ 3200]\n",
            "loss: 0.783292  [   60/ 3200]\n",
            "loss: 0.892867  [   61/ 3200]\n",
            "loss: 1.024173  [   62/ 3200]\n",
            "loss: 1.051228  [   63/ 3200]\n",
            "loss: 1.013650  [   64/ 3200]\n",
            "loss: 1.019561  [   65/ 3200]\n",
            "loss: 1.238579  [   66/ 3200]\n",
            "loss: 0.751230  [   67/ 3200]\n",
            "loss: 1.056031  [   68/ 3200]\n",
            "loss: 0.940555  [   69/ 3200]\n",
            "loss: 0.869596  [   70/ 3200]\n",
            "loss: 1.247092  [   71/ 3200]\n",
            "loss: 1.055939  [   72/ 3200]\n",
            "loss: 0.685817  [   73/ 3200]\n",
            "loss: 0.981005  [   74/ 3200]\n",
            "loss: 0.757406  [   75/ 3200]\n",
            "loss: 0.865642  [   76/ 3200]\n",
            "loss: 1.234377  [   77/ 3200]\n",
            "loss: 0.703627  [   78/ 3200]\n",
            "loss: 0.952318  [   79/ 3200]\n",
            "loss: 0.948017  [   80/ 3200]\n",
            "loss: 0.941941  [   81/ 3200]\n",
            "loss: 0.898635  [   82/ 3200]\n",
            "loss: 0.897455  [   83/ 3200]\n",
            "loss: 0.981455  [   84/ 3200]\n",
            "loss: 1.149722  [   85/ 3200]\n",
            "loss: 0.917071  [   86/ 3200]\n",
            "loss: 1.113547  [   87/ 3200]\n",
            "loss: 0.925345  [   88/ 3200]\n",
            "loss: 1.024321  [   89/ 3200]\n",
            "loss: 1.205384  [   90/ 3200]\n",
            "loss: 0.844954  [   91/ 3200]\n",
            "loss: 0.853484  [   92/ 3200]\n",
            "loss: 1.100869  [   93/ 3200]\n",
            "loss: 0.856171  [   94/ 3200]\n",
            "loss: 0.967442  [   95/ 3200]\n",
            "loss: 0.865687  [   96/ 3200]\n",
            "loss: 0.779630  [   97/ 3200]\n",
            "loss: 0.713299  [   98/ 3200]\n",
            "loss: 0.893449  [   99/ 3200]\n",
            "loss: 0.942070  [  100/ 3200]\n",
            "loss: 1.116007  [  101/ 3200]\n",
            "loss: 1.224804  [  102/ 3200]\n",
            "loss: 1.093655  [  103/ 3200]\n",
            "loss: 1.130804  [  104/ 3200]\n",
            "loss: 1.072898  [  105/ 3200]\n",
            "loss: 0.887609  [  106/ 3200]\n",
            "loss: 0.991934  [  107/ 3200]\n",
            "loss: 0.898249  [  108/ 3200]\n",
            "loss: 1.032994  [  109/ 3200]\n",
            "loss: 0.952698  [  110/ 3200]\n",
            "loss: 0.990821  [  111/ 3200]\n",
            "loss: 0.936256  [  112/ 3200]\n",
            "loss: 0.761942  [  113/ 3200]\n",
            "loss: 0.781120  [  114/ 3200]\n",
            "loss: 0.937250  [  115/ 3200]\n",
            "loss: 0.902949  [  116/ 3200]\n",
            "loss: 1.147588  [  117/ 3200]\n",
            "loss: 0.622714  [  118/ 3200]\n",
            "loss: 1.106579  [  119/ 3200]\n",
            "loss: 0.937566  [  120/ 3200]\n",
            "loss: 1.255121  [  121/ 3200]\n",
            "loss: 1.249038  [  122/ 3200]\n",
            "loss: 1.015128  [  123/ 3200]\n",
            "loss: 0.801360  [  124/ 3200]\n",
            "loss: 0.641081  [  125/ 3200]\n",
            "loss: 0.730625  [  126/ 3200]\n",
            "loss: 1.087426  [  127/ 3200]\n",
            "loss: 0.711670  [  128/ 3200]\n",
            "loss: 0.826214  [  129/ 3200]\n",
            "loss: 0.964474  [  130/ 3200]\n",
            "loss: 0.835871  [  131/ 3200]\n",
            "loss: 0.659491  [  132/ 3200]\n",
            "loss: 0.751567  [  133/ 3200]\n",
            "loss: 0.931825  [  134/ 3200]\n",
            "loss: 1.092760  [  135/ 3200]\n",
            "loss: 1.094291  [  136/ 3200]\n",
            "loss: 1.001682  [  137/ 3200]\n",
            "loss: 0.878154  [  138/ 3200]\n",
            "loss: 0.957088  [  139/ 3200]\n",
            "loss: 1.129034  [  140/ 3200]\n",
            "loss: 1.091232  [  141/ 3200]\n",
            "loss: 1.152593  [  142/ 3200]\n",
            "loss: 1.048998  [  143/ 3200]\n",
            "loss: 1.095355  [  144/ 3200]\n",
            "loss: 0.805995  [  145/ 3200]\n",
            "loss: 1.066600  [  146/ 3200]\n",
            "loss: 0.787515  [  147/ 3200]\n",
            "loss: 1.000592  [  148/ 3200]\n",
            "loss: 0.800964  [  149/ 3200]\n",
            "loss: 1.105479  [  150/ 3200]\n",
            "loss: 0.860198  [  151/ 3200]\n",
            "loss: 0.794207  [  152/ 3200]\n",
            "loss: 1.199937  [  153/ 3200]\n",
            "loss: 0.808424  [  154/ 3200]\n",
            "loss: 0.699343  [  155/ 3200]\n",
            "loss: 1.032160  [  156/ 3200]\n",
            "loss: 0.675900  [  157/ 3200]\n",
            "loss: 0.838891  [  158/ 3200]\n",
            "loss: 0.872642  [  159/ 3200]\n",
            "loss: 1.081261  [  160/ 3200]\n",
            "loss: 1.033949  [  161/ 3200]\n",
            "loss: 0.796117  [  162/ 3200]\n",
            "loss: 0.978882  [  163/ 3200]\n",
            "loss: 1.071226  [  164/ 3200]\n",
            "loss: 0.969308  [  165/ 3200]\n",
            "loss: 1.021660  [  166/ 3200]\n",
            "loss: 0.707808  [  167/ 3200]\n",
            "loss: 0.885093  [  168/ 3200]\n",
            "loss: 0.837984  [  169/ 3200]\n",
            "loss: 1.304358  [  170/ 3200]\n",
            "loss: 1.001363  [  171/ 3200]\n",
            "loss: 0.969186  [  172/ 3200]\n",
            "loss: 1.076105  [  173/ 3200]\n",
            "loss: 0.739956  [  174/ 3200]\n",
            "loss: 0.889176  [  175/ 3200]\n",
            "loss: 1.026060  [  176/ 3200]\n",
            "loss: 1.101063  [  177/ 3200]\n",
            "loss: 0.943207  [  178/ 3200]\n",
            "loss: 1.018999  [  179/ 3200]\n",
            "loss: 0.973812  [  180/ 3200]\n",
            "loss: 0.785538  [  181/ 3200]\n",
            "loss: 1.017192  [  182/ 3200]\n",
            "loss: 1.032961  [  183/ 3200]\n",
            "loss: 0.761989  [  184/ 3200]\n",
            "loss: 0.904635  [  185/ 3200]\n",
            "loss: 0.984919  [  186/ 3200]\n",
            "loss: 0.912904  [  187/ 3200]\n",
            "loss: 0.741052  [  188/ 3200]\n",
            "loss: 0.801463  [  189/ 3200]\n",
            "loss: 0.886529  [  190/ 3200]\n",
            "loss: 0.869952  [  191/ 3200]\n",
            "loss: 0.960622  [  192/ 3200]\n",
            "loss: 1.068835  [  193/ 3200]\n",
            "loss: 0.765806  [  194/ 3200]\n",
            "loss: 0.818396  [  195/ 3200]\n",
            "loss: 0.843204  [  196/ 3200]\n",
            "loss: 0.736153  [  197/ 3200]\n",
            "loss: 0.732262  [  198/ 3200]\n",
            "loss: 0.835689  [  199/ 3200]\n",
            "Training finished.\n",
            "Average Loss: 0.9763737755805947\n",
            "F1 Score (Macro): 0.5973044425276356\n",
            "Accuracy: 0.6184593023255814\n",
            "Confusion Matrix:\n",
            "[[ 78  31 132  83]\n",
            " [ 23 246  20   8]\n",
            " [ 34  17 297   8]\n",
            " [ 56  44  69 230]]\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "# if we want to utilize the GPU (if available), we need to assign the model to the correct device\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device=\"cuda\"\n",
        "    print(\"GPU available\")\n",
        "else:\n",
        "    device=\"cpu\"\n",
        "\n",
        "print (\"device=\",device)\n",
        "model = FullyConnectedNeuralNetwork().to(device)\n",
        "\n",
        "# Define the optimizer, learning rate, and loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 30\n",
        "\n",
        "# Train the model\n",
        "train_network(model, optimizer, train_dataloader, loss_function, num_epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "avg_loss, f1_score_macro, accuracy, confusion_mat = evaluate_model(model, test_dataloader, loss_function)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Average Loss: {avg_loss}\")\n",
        "print(f\"F1 Score (Macro): {f1_score_macro}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Return the trained model\n",
        "trained_model = model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, optimizer, train_dataloader, val_dataloader, test_dataloader, loss_function, num_epochs):\n",
        "    best_model = None\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train the model\n",
        "        train_network(model, optimizer, train_dataloader, loss_function, 1)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        val_loss, val_f1, val_accuracy, val_confusion_mat = evaluate_model(model, val_dataloader, loss_function)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}\")\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "        print(f\"Validation F1: {val_f1:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        print(\"Validation Confusion Matrix:\")\n",
        "        print(val_confusion_mat)\n",
        "\n",
        "        # Save the model with the best F1 score on the validation set\n",
        "        if val_f1 > best_f1:\n",
        "            best_model = model\n",
        "            best_f1 = val_f1\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    test_loss, test_f1, test_accuracy, test_confusion_mat = evaluate_model(best_model, test_dataloader, loss_function)\n",
        "\n",
        "    print(\"Best Model Performance on Test Set:\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test F1: {test_f1:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"Test Confusion Matrix:\")\n",
        "    print(test_confusion_mat)\n",
        "\n",
        "    return best_model, test_loss, test_f1, test_accuracy, test_confusion_mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "loss: 0.652477  [    0/ 3200]\n",
            "loss: 1.010451  [    1/ 3200]\n",
            "loss: 0.926135  [    2/ 3200]\n",
            "loss: 0.895955  [    3/ 3200]\n",
            "loss: 0.997836  [    4/ 3200]\n",
            "loss: 0.903912  [    5/ 3200]\n",
            "loss: 1.161251  [    6/ 3200]\n",
            "loss: 1.120892  [    7/ 3200]\n",
            "loss: 1.030235  [    8/ 3200]\n",
            "loss: 0.936125  [    9/ 3200]\n",
            "loss: 0.920190  [   10/ 3200]\n",
            "loss: 1.148975  [   11/ 3200]\n",
            "loss: 0.677553  [   12/ 3200]\n",
            "loss: 0.850483  [   13/ 3200]\n",
            "loss: 0.969023  [   14/ 3200]\n",
            "loss: 0.803753  [   15/ 3200]\n",
            "loss: 0.750909  [   16/ 3200]\n",
            "loss: 0.803521  [   17/ 3200]\n",
            "loss: 0.966160  [   18/ 3200]\n",
            "loss: 0.557710  [   19/ 3200]\n",
            "loss: 0.944551  [   20/ 3200]\n",
            "loss: 0.827979  [   21/ 3200]\n",
            "loss: 0.922811  [   22/ 3200]\n",
            "loss: 0.922725  [   23/ 3200]\n",
            "loss: 0.849937  [   24/ 3200]\n",
            "loss: 0.853879  [   25/ 3200]\n",
            "loss: 0.887372  [   26/ 3200]\n",
            "loss: 1.131519  [   27/ 3200]\n",
            "loss: 1.039113  [   28/ 3200]\n",
            "loss: 0.700667  [   29/ 3200]\n",
            "loss: 0.831723  [   30/ 3200]\n",
            "loss: 0.831254  [   31/ 3200]\n",
            "loss: 1.251496  [   32/ 3200]\n",
            "loss: 0.901988  [   33/ 3200]\n",
            "loss: 1.093228  [   34/ 3200]\n",
            "loss: 1.177878  [   35/ 3200]\n",
            "loss: 1.032193  [   36/ 3200]\n",
            "loss: 1.041975  [   37/ 3200]\n",
            "loss: 0.821676  [   38/ 3200]\n",
            "loss: 0.968629  [   39/ 3200]\n",
            "loss: 0.931807  [   40/ 3200]\n",
            "loss: 0.823358  [   41/ 3200]\n",
            "loss: 1.102566  [   42/ 3200]\n",
            "loss: 1.057682  [   43/ 3200]\n",
            "loss: 0.721296  [   44/ 3200]\n",
            "loss: 0.928947  [   45/ 3200]\n",
            "loss: 1.179030  [   46/ 3200]\n",
            "loss: 0.676406  [   47/ 3200]\n",
            "loss: 0.968501  [   48/ 3200]\n",
            "loss: 1.002804  [   49/ 3200]\n",
            "loss: 0.897049  [   50/ 3200]\n",
            "loss: 0.877114  [   51/ 3200]\n",
            "loss: 0.655381  [   52/ 3200]\n",
            "loss: 0.769540  [   53/ 3200]\n",
            "loss: 0.710465  [   54/ 3200]\n",
            "loss: 0.871724  [   55/ 3200]\n",
            "loss: 1.180928  [   56/ 3200]\n",
            "loss: 0.951554  [   57/ 3200]\n",
            "loss: 0.726137  [   58/ 3200]\n",
            "loss: 0.695576  [   59/ 3200]\n",
            "loss: 1.202779  [   60/ 3200]\n",
            "loss: 0.926779  [   61/ 3200]\n",
            "loss: 0.993678  [   62/ 3200]\n",
            "loss: 0.946770  [   63/ 3200]\n",
            "loss: 1.474404  [   64/ 3200]\n",
            "loss: 1.180233  [   65/ 3200]\n",
            "loss: 1.196103  [   66/ 3200]\n",
            "loss: 0.891904  [   67/ 3200]\n",
            "loss: 1.044596  [   68/ 3200]\n",
            "loss: 0.760450  [   69/ 3200]\n",
            "loss: 0.862131  [   70/ 3200]\n",
            "loss: 0.843464  [   71/ 3200]\n",
            "loss: 1.029756  [   72/ 3200]\n",
            "loss: 1.210920  [   73/ 3200]\n",
            "loss: 0.962343  [   74/ 3200]\n",
            "loss: 0.995776  [   75/ 3200]\n",
            "loss: 0.943151  [   76/ 3200]\n",
            "loss: 0.842675  [   77/ 3200]\n",
            "loss: 1.256158  [   78/ 3200]\n",
            "loss: 1.116660  [   79/ 3200]\n",
            "loss: 0.926035  [   80/ 3200]\n",
            "loss: 0.771904  [   81/ 3200]\n",
            "loss: 0.909684  [   82/ 3200]\n",
            "loss: 0.986836  [   83/ 3200]\n",
            "loss: 0.819626  [   84/ 3200]\n",
            "loss: 1.162372  [   85/ 3200]\n",
            "loss: 0.910787  [   86/ 3200]\n",
            "loss: 1.017377  [   87/ 3200]\n",
            "loss: 0.893072  [   88/ 3200]\n",
            "loss: 0.855977  [   89/ 3200]\n",
            "loss: 0.925698  [   90/ 3200]\n",
            "loss: 1.362470  [   91/ 3200]\n",
            "loss: 0.750763  [   92/ 3200]\n",
            "loss: 0.911333  [   93/ 3200]\n",
            "loss: 0.690105  [   94/ 3200]\n",
            "loss: 0.937299  [   95/ 3200]\n",
            "loss: 1.041366  [   96/ 3200]\n",
            "loss: 0.756984  [   97/ 3200]\n",
            "loss: 0.880697  [   98/ 3200]\n",
            "loss: 1.069541  [   99/ 3200]\n",
            "loss: 0.757553  [  100/ 3200]\n",
            "loss: 0.898973  [  101/ 3200]\n",
            "loss: 1.081379  [  102/ 3200]\n",
            "loss: 0.983215  [  103/ 3200]\n",
            "loss: 0.931326  [  104/ 3200]\n",
            "loss: 1.050622  [  105/ 3200]\n",
            "loss: 0.790691  [  106/ 3200]\n",
            "loss: 0.824362  [  107/ 3200]\n",
            "loss: 0.956736  [  108/ 3200]\n",
            "loss: 0.992341  [  109/ 3200]\n",
            "loss: 0.877637  [  110/ 3200]\n",
            "loss: 0.930797  [  111/ 3200]\n",
            "loss: 0.764483  [  112/ 3200]\n",
            "loss: 0.827223  [  113/ 3200]\n",
            "loss: 1.075498  [  114/ 3200]\n",
            "loss: 0.990968  [  115/ 3200]\n",
            "loss: 0.697307  [  116/ 3200]\n",
            "loss: 1.298044  [  117/ 3200]\n",
            "loss: 1.049480  [  118/ 3200]\n",
            "loss: 1.309621  [  119/ 3200]\n",
            "loss: 1.168713  [  120/ 3200]\n",
            "loss: 0.915507  [  121/ 3200]\n",
            "loss: 0.898952  [  122/ 3200]\n",
            "loss: 1.312601  [  123/ 3200]\n",
            "loss: 0.991983  [  124/ 3200]\n",
            "loss: 0.923874  [  125/ 3200]\n",
            "loss: 0.882105  [  126/ 3200]\n",
            "loss: 1.113272  [  127/ 3200]\n",
            "loss: 0.920026  [  128/ 3200]\n",
            "loss: 1.024671  [  129/ 3200]\n",
            "loss: 1.174936  [  130/ 3200]\n",
            "loss: 1.003275  [  131/ 3200]\n",
            "loss: 0.904787  [  132/ 3200]\n",
            "loss: 1.071251  [  133/ 3200]\n",
            "loss: 1.041576  [  134/ 3200]\n",
            "loss: 0.843684  [  135/ 3200]\n",
            "loss: 1.128680  [  136/ 3200]\n",
            "loss: 0.804355  [  137/ 3200]\n",
            "loss: 0.765035  [  138/ 3200]\n",
            "loss: 0.952931  [  139/ 3200]\n",
            "loss: 0.814350  [  140/ 3200]\n",
            "loss: 0.832554  [  141/ 3200]\n",
            "loss: 0.970746  [  142/ 3200]\n",
            "loss: 0.860825  [  143/ 3200]\n",
            "loss: 0.910001  [  144/ 3200]\n",
            "loss: 0.950845  [  145/ 3200]\n",
            "loss: 0.968535  [  146/ 3200]\n",
            "loss: 0.752169  [  147/ 3200]\n",
            "loss: 0.887281  [  148/ 3200]\n",
            "loss: 0.836891  [  149/ 3200]\n",
            "loss: 0.936044  [  150/ 3200]\n",
            "loss: 0.937364  [  151/ 3200]\n",
            "loss: 0.761532  [  152/ 3200]\n",
            "loss: 0.899336  [  153/ 3200]\n",
            "loss: 0.886342  [  154/ 3200]\n",
            "loss: 0.837613  [  155/ 3200]\n",
            "loss: 0.797704  [  156/ 3200]\n",
            "loss: 1.068088  [  157/ 3200]\n",
            "loss: 0.886404  [  158/ 3200]\n",
            "loss: 1.049018  [  159/ 3200]\n",
            "loss: 0.974168  [  160/ 3200]\n",
            "loss: 0.682851  [  161/ 3200]\n",
            "loss: 0.871142  [  162/ 3200]\n",
            "loss: 1.241993  [  163/ 3200]\n",
            "loss: 0.904360  [  164/ 3200]\n",
            "loss: 1.198098  [  165/ 3200]\n",
            "loss: 0.833356  [  166/ 3200]\n",
            "loss: 0.759294  [  167/ 3200]\n",
            "loss: 0.910269  [  168/ 3200]\n",
            "loss: 0.896152  [  169/ 3200]\n",
            "loss: 1.126612  [  170/ 3200]\n",
            "loss: 0.932887  [  171/ 3200]\n",
            "loss: 0.774350  [  172/ 3200]\n",
            "loss: 0.720660  [  173/ 3200]\n",
            "loss: 0.965257  [  174/ 3200]\n",
            "loss: 0.705036  [  175/ 3200]\n",
            "loss: 0.735694  [  176/ 3200]\n",
            "loss: 1.104526  [  177/ 3200]\n",
            "loss: 1.217314  [  178/ 3200]\n",
            "loss: 0.921606  [  179/ 3200]\n",
            "loss: 0.762077  [  180/ 3200]\n",
            "loss: 0.758917  [  181/ 3200]\n",
            "loss: 0.899551  [  182/ 3200]\n",
            "loss: 1.226574  [  183/ 3200]\n",
            "loss: 0.813690  [  184/ 3200]\n",
            "loss: 0.674995  [  185/ 3200]\n",
            "loss: 1.183208  [  186/ 3200]\n",
            "loss: 1.007170  [  187/ 3200]\n",
            "loss: 0.872213  [  188/ 3200]\n",
            "loss: 1.017926  [  189/ 3200]\n",
            "loss: 1.263121  [  190/ 3200]\n",
            "loss: 0.820193  [  191/ 3200]\n",
            "loss: 0.703940  [  192/ 3200]\n",
            "loss: 0.917711  [  193/ 3200]\n",
            "loss: 1.245434  [  194/ 3200]\n",
            "loss: 0.870939  [  195/ 3200]\n",
            "loss: 0.751674  [  196/ 3200]\n",
            "loss: 1.033083  [  197/ 3200]\n",
            "loss: 0.919955  [  198/ 3200]\n",
            "loss: 1.220692  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 1\n",
            "Validation Loss: 0.9168\n",
            "Validation F1: 0.6074\n",
            "Validation Accuracy: 0.6312\n",
            "Validation Confusion Matrix:\n",
            "[[ 43  39 113   5]\n",
            " [ 15 178   7   0]\n",
            " [ 15   4 180   1]\n",
            " [ 48  17  31 104]]\n",
            "Epoch:  1\n",
            "loss: 0.699993  [    0/ 3200]\n",
            "loss: 0.938466  [    1/ 3200]\n",
            "loss: 0.992761  [    2/ 3200]\n",
            "loss: 0.999764  [    3/ 3200]\n",
            "loss: 0.978680  [    4/ 3200]\n",
            "loss: 1.285557  [    5/ 3200]\n",
            "loss: 1.061838  [    6/ 3200]\n",
            "loss: 1.233653  [    7/ 3200]\n",
            "loss: 0.937654  [    8/ 3200]\n",
            "loss: 0.997928  [    9/ 3200]\n",
            "loss: 0.832480  [   10/ 3200]\n",
            "loss: 0.718122  [   11/ 3200]\n",
            "loss: 1.163267  [   12/ 3200]\n",
            "loss: 0.876680  [   13/ 3200]\n",
            "loss: 1.049585  [   14/ 3200]\n",
            "loss: 0.994531  [   15/ 3200]\n",
            "loss: 0.842226  [   16/ 3200]\n",
            "loss: 1.258142  [   17/ 3200]\n",
            "loss: 1.157174  [   18/ 3200]\n",
            "loss: 1.356701  [   19/ 3200]\n",
            "loss: 0.881002  [   20/ 3200]\n",
            "loss: 0.942206  [   21/ 3200]\n",
            "loss: 1.113423  [   22/ 3200]\n",
            "loss: 0.827303  [   23/ 3200]\n",
            "loss: 0.886170  [   24/ 3200]\n",
            "loss: 0.914131  [   25/ 3200]\n",
            "loss: 1.036327  [   26/ 3200]\n",
            "loss: 0.742477  [   27/ 3200]\n",
            "loss: 1.024279  [   28/ 3200]\n",
            "loss: 0.802251  [   29/ 3200]\n",
            "loss: 0.705744  [   30/ 3200]\n",
            "loss: 0.887282  [   31/ 3200]\n",
            "loss: 0.913063  [   32/ 3200]\n",
            "loss: 0.829206  [   33/ 3200]\n",
            "loss: 1.131522  [   34/ 3200]\n",
            "loss: 1.244668  [   35/ 3200]\n",
            "loss: 0.788595  [   36/ 3200]\n",
            "loss: 1.059871  [   37/ 3200]\n",
            "loss: 0.851646  [   38/ 3200]\n",
            "loss: 0.977467  [   39/ 3200]\n",
            "loss: 0.899697  [   40/ 3200]\n",
            "loss: 0.874998  [   41/ 3200]\n",
            "loss: 0.954890  [   42/ 3200]\n",
            "loss: 1.128035  [   43/ 3200]\n",
            "loss: 1.274834  [   44/ 3200]\n",
            "loss: 1.220448  [   45/ 3200]\n",
            "loss: 0.913378  [   46/ 3200]\n",
            "loss: 1.036076  [   47/ 3200]\n",
            "loss: 0.840287  [   48/ 3200]\n",
            "loss: 0.827039  [   49/ 3200]\n",
            "loss: 0.908615  [   50/ 3200]\n",
            "loss: 0.757201  [   51/ 3200]\n",
            "loss: 1.160700  [   52/ 3200]\n",
            "loss: 0.841002  [   53/ 3200]\n",
            "loss: 0.720536  [   54/ 3200]\n",
            "loss: 0.567192  [   55/ 3200]\n",
            "loss: 1.149766  [   56/ 3200]\n",
            "loss: 0.706965  [   57/ 3200]\n",
            "loss: 0.848197  [   58/ 3200]\n",
            "loss: 0.833126  [   59/ 3200]\n",
            "loss: 0.783141  [   60/ 3200]\n",
            "loss: 1.085009  [   61/ 3200]\n",
            "loss: 0.675289  [   62/ 3200]\n",
            "loss: 0.910322  [   63/ 3200]\n",
            "loss: 0.920125  [   64/ 3200]\n",
            "loss: 1.162629  [   65/ 3200]\n",
            "loss: 1.189377  [   66/ 3200]\n",
            "loss: 0.765197  [   67/ 3200]\n",
            "loss: 0.790829  [   68/ 3200]\n",
            "loss: 0.952845  [   69/ 3200]\n",
            "loss: 0.886082  [   70/ 3200]\n",
            "loss: 1.165518  [   71/ 3200]\n",
            "loss: 0.812542  [   72/ 3200]\n",
            "loss: 1.073497  [   73/ 3200]\n",
            "loss: 1.076112  [   74/ 3200]\n",
            "loss: 0.785686  [   75/ 3200]\n",
            "loss: 0.999752  [   76/ 3200]\n",
            "loss: 0.872365  [   77/ 3200]\n",
            "loss: 0.959306  [   78/ 3200]\n",
            "loss: 0.861877  [   79/ 3200]\n",
            "loss: 0.929237  [   80/ 3200]\n",
            "loss: 1.394202  [   81/ 3200]\n",
            "loss: 0.772853  [   82/ 3200]\n",
            "loss: 0.776269  [   83/ 3200]\n",
            "loss: 1.061422  [   84/ 3200]\n",
            "loss: 0.911858  [   85/ 3200]\n",
            "loss: 1.491365  [   86/ 3200]\n",
            "loss: 1.142796  [   87/ 3200]\n",
            "loss: 0.741665  [   88/ 3200]\n",
            "loss: 0.899942  [   89/ 3200]\n",
            "loss: 0.918803  [   90/ 3200]\n",
            "loss: 0.804312  [   91/ 3200]\n",
            "loss: 1.009935  [   92/ 3200]\n",
            "loss: 0.883036  [   93/ 3200]\n",
            "loss: 0.756863  [   94/ 3200]\n",
            "loss: 0.903715  [   95/ 3200]\n",
            "loss: 0.989565  [   96/ 3200]\n",
            "loss: 1.048723  [   97/ 3200]\n",
            "loss: 0.838573  [   98/ 3200]\n",
            "loss: 0.823838  [   99/ 3200]\n",
            "loss: 0.804110  [  100/ 3200]\n",
            "loss: 1.019098  [  101/ 3200]\n",
            "loss: 0.762279  [  102/ 3200]\n",
            "loss: 0.995579  [  103/ 3200]\n",
            "loss: 1.043945  [  104/ 3200]\n",
            "loss: 0.663364  [  105/ 3200]\n",
            "loss: 0.693278  [  106/ 3200]\n",
            "loss: 0.984432  [  107/ 3200]\n",
            "loss: 1.121939  [  108/ 3200]\n",
            "loss: 0.777566  [  109/ 3200]\n",
            "loss: 0.841942  [  110/ 3200]\n",
            "loss: 0.983578  [  111/ 3200]\n",
            "loss: 1.058712  [  112/ 3200]\n",
            "loss: 0.733589  [  113/ 3200]\n",
            "loss: 0.845779  [  114/ 3200]\n",
            "loss: 1.020307  [  115/ 3200]\n",
            "loss: 0.980144  [  116/ 3200]\n",
            "loss: 0.762973  [  117/ 3200]\n",
            "loss: 0.990081  [  118/ 3200]\n",
            "loss: 0.670401  [  119/ 3200]\n",
            "loss: 0.768432  [  120/ 3200]\n",
            "loss: 1.216263  [  121/ 3200]\n",
            "loss: 0.793062  [  122/ 3200]\n",
            "loss: 1.038109  [  123/ 3200]\n",
            "loss: 0.742942  [  124/ 3200]\n",
            "loss: 0.933793  [  125/ 3200]\n",
            "loss: 1.069710  [  126/ 3200]\n",
            "loss: 0.917259  [  127/ 3200]\n",
            "loss: 0.677392  [  128/ 3200]\n",
            "loss: 0.835738  [  129/ 3200]\n",
            "loss: 0.926990  [  130/ 3200]\n",
            "loss: 1.056480  [  131/ 3200]\n",
            "loss: 0.882758  [  132/ 3200]\n",
            "loss: 0.973349  [  133/ 3200]\n",
            "loss: 1.007683  [  134/ 3200]\n",
            "loss: 1.085810  [  135/ 3200]\n",
            "loss: 0.801731  [  136/ 3200]\n",
            "loss: 0.858222  [  137/ 3200]\n",
            "loss: 1.042411  [  138/ 3200]\n",
            "loss: 0.987404  [  139/ 3200]\n",
            "loss: 1.021861  [  140/ 3200]\n",
            "loss: 0.745312  [  141/ 3200]\n",
            "loss: 0.905461  [  142/ 3200]\n",
            "loss: 1.052669  [  143/ 3200]\n",
            "loss: 1.207133  [  144/ 3200]\n",
            "loss: 1.265121  [  145/ 3200]\n",
            "loss: 0.815938  [  146/ 3200]\n",
            "loss: 0.673398  [  147/ 3200]\n",
            "loss: 0.991387  [  148/ 3200]\n",
            "loss: 0.733195  [  149/ 3200]\n",
            "loss: 0.908702  [  150/ 3200]\n",
            "loss: 1.041662  [  151/ 3200]\n",
            "loss: 1.075070  [  152/ 3200]\n",
            "loss: 0.825882  [  153/ 3200]\n",
            "loss: 1.050184  [  154/ 3200]\n",
            "loss: 0.794272  [  155/ 3200]\n",
            "loss: 0.982995  [  156/ 3200]\n",
            "loss: 1.042229  [  157/ 3200]\n",
            "loss: 0.700863  [  158/ 3200]\n",
            "loss: 0.964838  [  159/ 3200]\n",
            "loss: 0.878549  [  160/ 3200]\n",
            "loss: 0.685832  [  161/ 3200]\n",
            "loss: 0.831773  [  162/ 3200]\n",
            "loss: 1.299820  [  163/ 3200]\n",
            "loss: 0.629234  [  164/ 3200]\n",
            "loss: 1.174314  [  165/ 3200]\n",
            "loss: 0.882761  [  166/ 3200]\n",
            "loss: 1.061890  [  167/ 3200]\n",
            "loss: 0.877864  [  168/ 3200]\n",
            "loss: 1.059360  [  169/ 3200]\n",
            "loss: 0.705240  [  170/ 3200]\n",
            "loss: 0.819248  [  171/ 3200]\n",
            "loss: 1.148286  [  172/ 3200]\n",
            "loss: 0.970241  [  173/ 3200]\n",
            "loss: 1.402663  [  174/ 3200]\n",
            "loss: 0.753116  [  175/ 3200]\n",
            "loss: 1.074583  [  176/ 3200]\n",
            "loss: 1.213302  [  177/ 3200]\n",
            "loss: 0.616302  [  178/ 3200]\n",
            "loss: 0.956300  [  179/ 3200]\n",
            "loss: 0.831348  [  180/ 3200]\n",
            "loss: 0.985737  [  181/ 3200]\n",
            "loss: 1.103376  [  182/ 3200]\n",
            "loss: 1.141498  [  183/ 3200]\n",
            "loss: 1.064399  [  184/ 3200]\n",
            "loss: 0.841889  [  185/ 3200]\n",
            "loss: 0.969596  [  186/ 3200]\n",
            "loss: 0.877059  [  187/ 3200]\n",
            "loss: 0.883345  [  188/ 3200]\n",
            "loss: 0.823762  [  189/ 3200]\n",
            "loss: 0.648982  [  190/ 3200]\n",
            "loss: 0.819558  [  191/ 3200]\n",
            "loss: 1.309548  [  192/ 3200]\n",
            "loss: 0.882913  [  193/ 3200]\n",
            "loss: 0.928521  [  194/ 3200]\n",
            "loss: 0.887639  [  195/ 3200]\n",
            "loss: 1.155226  [  196/ 3200]\n",
            "loss: 0.768613  [  197/ 3200]\n",
            "loss: 0.887198  [  198/ 3200]\n",
            "loss: 0.812559  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 2\n",
            "Validation Loss: 0.8652\n",
            "Validation F1: 0.6512\n",
            "Validation Accuracy: 0.6700\n",
            "Validation Confusion Matrix:\n",
            "[[ 60  36  72  32]\n",
            " [ 14 165   4  17]\n",
            " [ 28   3 147  22]\n",
            " [ 11  12  13 164]]\n",
            "Epoch:  1\n",
            "loss: 0.877151  [    0/ 3200]\n",
            "loss: 0.835239  [    1/ 3200]\n",
            "loss: 1.170234  [    2/ 3200]\n",
            "loss: 1.011376  [    3/ 3200]\n",
            "loss: 0.869622  [    4/ 3200]\n",
            "loss: 0.719605  [    5/ 3200]\n",
            "loss: 0.991072  [    6/ 3200]\n",
            "loss: 0.956298  [    7/ 3200]\n",
            "loss: 0.668756  [    8/ 3200]\n",
            "loss: 0.977419  [    9/ 3200]\n",
            "loss: 0.719854  [   10/ 3200]\n",
            "loss: 0.884988  [   11/ 3200]\n",
            "loss: 0.898917  [   12/ 3200]\n",
            "loss: 0.841548  [   13/ 3200]\n",
            "loss: 0.964333  [   14/ 3200]\n",
            "loss: 0.879354  [   15/ 3200]\n",
            "loss: 0.844803  [   16/ 3200]\n",
            "loss: 1.261328  [   17/ 3200]\n",
            "loss: 1.127366  [   18/ 3200]\n",
            "loss: 0.982830  [   19/ 3200]\n",
            "loss: 1.046916  [   20/ 3200]\n",
            "loss: 0.846436  [   21/ 3200]\n",
            "loss: 1.053597  [   22/ 3200]\n",
            "loss: 0.699696  [   23/ 3200]\n",
            "loss: 0.883273  [   24/ 3200]\n",
            "loss: 0.966508  [   25/ 3200]\n",
            "loss: 0.803216  [   26/ 3200]\n",
            "loss: 0.976984  [   27/ 3200]\n",
            "loss: 0.756435  [   28/ 3200]\n",
            "loss: 0.864927  [   29/ 3200]\n",
            "loss: 0.942727  [   30/ 3200]\n",
            "loss: 0.981493  [   31/ 3200]\n",
            "loss: 0.999733  [   32/ 3200]\n",
            "loss: 0.811549  [   33/ 3200]\n",
            "loss: 1.036777  [   34/ 3200]\n",
            "loss: 1.024027  [   35/ 3200]\n",
            "loss: 0.854061  [   36/ 3200]\n",
            "loss: 1.094542  [   37/ 3200]\n",
            "loss: 0.594548  [   38/ 3200]\n",
            "loss: 1.104772  [   39/ 3200]\n",
            "loss: 0.926420  [   40/ 3200]\n",
            "loss: 0.975527  [   41/ 3200]\n",
            "loss: 0.903782  [   42/ 3200]\n",
            "loss: 0.871386  [   43/ 3200]\n",
            "loss: 0.862355  [   44/ 3200]\n",
            "loss: 1.312887  [   45/ 3200]\n",
            "loss: 0.670302  [   46/ 3200]\n",
            "loss: 0.929887  [   47/ 3200]\n",
            "loss: 0.789829  [   48/ 3200]\n",
            "loss: 1.058348  [   49/ 3200]\n",
            "loss: 0.978897  [   50/ 3200]\n",
            "loss: 0.548640  [   51/ 3200]\n",
            "loss: 0.758137  [   52/ 3200]\n",
            "loss: 1.074703  [   53/ 3200]\n",
            "loss: 1.307703  [   54/ 3200]\n",
            "loss: 0.876137  [   55/ 3200]\n",
            "loss: 1.042436  [   56/ 3200]\n",
            "loss: 0.811782  [   57/ 3200]\n",
            "loss: 0.994555  [   58/ 3200]\n",
            "loss: 0.976089  [   59/ 3200]\n",
            "loss: 1.011258  [   60/ 3200]\n",
            "loss: 0.818533  [   61/ 3200]\n",
            "loss: 1.254490  [   62/ 3200]\n",
            "loss: 0.607500  [   63/ 3200]\n",
            "loss: 0.935807  [   64/ 3200]\n",
            "loss: 0.852184  [   65/ 3200]\n",
            "loss: 1.320645  [   66/ 3200]\n",
            "loss: 0.804267  [   67/ 3200]\n",
            "loss: 0.890090  [   68/ 3200]\n",
            "loss: 0.858847  [   69/ 3200]\n",
            "loss: 1.070488  [   70/ 3200]\n",
            "loss: 1.053731  [   71/ 3200]\n",
            "loss: 0.884122  [   72/ 3200]\n",
            "loss: 0.995815  [   73/ 3200]\n",
            "loss: 0.950695  [   74/ 3200]\n",
            "loss: 0.806779  [   75/ 3200]\n",
            "loss: 0.862204  [   76/ 3200]\n",
            "loss: 1.034690  [   77/ 3200]\n",
            "loss: 0.779590  [   78/ 3200]\n",
            "loss: 0.888371  [   79/ 3200]\n",
            "loss: 0.589926  [   80/ 3200]\n",
            "loss: 0.823312  [   81/ 3200]\n",
            "loss: 1.010349  [   82/ 3200]\n",
            "loss: 0.918478  [   83/ 3200]\n",
            "loss: 1.010481  [   84/ 3200]\n",
            "loss: 0.888894  [   85/ 3200]\n",
            "loss: 0.956961  [   86/ 3200]\n",
            "loss: 0.822424  [   87/ 3200]\n",
            "loss: 0.741673  [   88/ 3200]\n",
            "loss: 0.845569  [   89/ 3200]\n",
            "loss: 1.173866  [   90/ 3200]\n",
            "loss: 0.803352  [   91/ 3200]\n",
            "loss: 0.950795  [   92/ 3200]\n",
            "loss: 1.171218  [   93/ 3200]\n",
            "loss: 0.954588  [   94/ 3200]\n",
            "loss: 0.994347  [   95/ 3200]\n",
            "loss: 0.720907  [   96/ 3200]\n",
            "loss: 0.803378  [   97/ 3200]\n",
            "loss: 0.636338  [   98/ 3200]\n",
            "loss: 0.905334  [   99/ 3200]\n",
            "loss: 1.077572  [  100/ 3200]\n",
            "loss: 0.904414  [  101/ 3200]\n",
            "loss: 1.126533  [  102/ 3200]\n",
            "loss: 1.229988  [  103/ 3200]\n",
            "loss: 0.820614  [  104/ 3200]\n",
            "loss: 0.759225  [  105/ 3200]\n",
            "loss: 1.143561  [  106/ 3200]\n",
            "loss: 0.873466  [  107/ 3200]\n",
            "loss: 0.902694  [  108/ 3200]\n",
            "loss: 1.056178  [  109/ 3200]\n",
            "loss: 0.747862  [  110/ 3200]\n",
            "loss: 0.951683  [  111/ 3200]\n",
            "loss: 0.966021  [  112/ 3200]\n",
            "loss: 1.157921  [  113/ 3200]\n",
            "loss: 0.979701  [  114/ 3200]\n",
            "loss: 1.181646  [  115/ 3200]\n",
            "loss: 0.952005  [  116/ 3200]\n",
            "loss: 1.041102  [  117/ 3200]\n",
            "loss: 0.643276  [  118/ 3200]\n",
            "loss: 0.716194  [  119/ 3200]\n",
            "loss: 1.089929  [  120/ 3200]\n",
            "loss: 0.871358  [  121/ 3200]\n",
            "loss: 1.089006  [  122/ 3200]\n",
            "loss: 0.962704  [  123/ 3200]\n",
            "loss: 1.007207  [  124/ 3200]\n",
            "loss: 1.032495  [  125/ 3200]\n",
            "loss: 0.962927  [  126/ 3200]\n",
            "loss: 0.806361  [  127/ 3200]\n",
            "loss: 1.231799  [  128/ 3200]\n",
            "loss: 0.996967  [  129/ 3200]\n",
            "loss: 1.021400  [  130/ 3200]\n",
            "loss: 0.746582  [  131/ 3200]\n",
            "loss: 0.937469  [  132/ 3200]\n",
            "loss: 1.015378  [  133/ 3200]\n",
            "loss: 1.019829  [  134/ 3200]\n",
            "loss: 0.639264  [  135/ 3200]\n",
            "loss: 1.021495  [  136/ 3200]\n",
            "loss: 0.866715  [  137/ 3200]\n",
            "loss: 1.226283  [  138/ 3200]\n",
            "loss: 0.974471  [  139/ 3200]\n",
            "loss: 1.055720  [  140/ 3200]\n",
            "loss: 0.667181  [  141/ 3200]\n",
            "loss: 0.834656  [  142/ 3200]\n",
            "loss: 1.073177  [  143/ 3200]\n",
            "loss: 0.859274  [  144/ 3200]\n",
            "loss: 0.773793  [  145/ 3200]\n",
            "loss: 0.828024  [  146/ 3200]\n",
            "loss: 1.006122  [  147/ 3200]\n",
            "loss: 0.866580  [  148/ 3200]\n",
            "loss: 0.899142  [  149/ 3200]\n",
            "loss: 1.068073  [  150/ 3200]\n",
            "loss: 0.801614  [  151/ 3200]\n",
            "loss: 0.921122  [  152/ 3200]\n",
            "loss: 0.782089  [  153/ 3200]\n",
            "loss: 0.915416  [  154/ 3200]\n",
            "loss: 0.757157  [  155/ 3200]\n",
            "loss: 1.113128  [  156/ 3200]\n",
            "loss: 0.997623  [  157/ 3200]\n",
            "loss: 0.738512  [  158/ 3200]\n",
            "loss: 0.748401  [  159/ 3200]\n",
            "loss: 0.860774  [  160/ 3200]\n",
            "loss: 0.917751  [  161/ 3200]\n",
            "loss: 0.934993  [  162/ 3200]\n",
            "loss: 0.755025  [  163/ 3200]\n",
            "loss: 1.253805  [  164/ 3200]\n",
            "loss: 0.961059  [  165/ 3200]\n",
            "loss: 1.006532  [  166/ 3200]\n",
            "loss: 0.975593  [  167/ 3200]\n",
            "loss: 0.722678  [  168/ 3200]\n",
            "loss: 0.737481  [  169/ 3200]\n",
            "loss: 0.836463  [  170/ 3200]\n",
            "loss: 1.058209  [  171/ 3200]\n",
            "loss: 1.150062  [  172/ 3200]\n",
            "loss: 0.843889  [  173/ 3200]\n",
            "loss: 1.329381  [  174/ 3200]\n",
            "loss: 0.762335  [  175/ 3200]\n",
            "loss: 0.829761  [  176/ 3200]\n",
            "loss: 0.664473  [  177/ 3200]\n",
            "loss: 0.811394  [  178/ 3200]\n",
            "loss: 1.042216  [  179/ 3200]\n",
            "loss: 0.873681  [  180/ 3200]\n",
            "loss: 0.717032  [  181/ 3200]\n",
            "loss: 1.039481  [  182/ 3200]\n",
            "loss: 1.074354  [  183/ 3200]\n",
            "loss: 1.018641  [  184/ 3200]\n",
            "loss: 1.081454  [  185/ 3200]\n",
            "loss: 0.963053  [  186/ 3200]\n",
            "loss: 0.943460  [  187/ 3200]\n",
            "loss: 0.942111  [  188/ 3200]\n",
            "loss: 0.901917  [  189/ 3200]\n",
            "loss: 1.250679  [  190/ 3200]\n",
            "loss: 0.988669  [  191/ 3200]\n",
            "loss: 1.034719  [  192/ 3200]\n",
            "loss: 0.984041  [  193/ 3200]\n",
            "loss: 0.827018  [  194/ 3200]\n",
            "loss: 0.936385  [  195/ 3200]\n",
            "loss: 1.350903  [  196/ 3200]\n",
            "loss: 0.944794  [  197/ 3200]\n",
            "loss: 0.867636  [  198/ 3200]\n",
            "loss: 0.955299  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 3\n",
            "Validation Loss: 0.8943\n",
            "Validation F1: 0.6475\n",
            "Validation Accuracy: 0.6525\n",
            "Validation Confusion Matrix:\n",
            "[[ 87  53  50  10]\n",
            " [  9 187   4   0]\n",
            " [ 49   7 141   3]\n",
            " [ 59  25   9 107]]\n",
            "Epoch:  1\n",
            "loss: 0.983942  [    0/ 3200]\n",
            "loss: 1.166323  [    1/ 3200]\n",
            "loss: 0.820939  [    2/ 3200]\n",
            "loss: 0.886967  [    3/ 3200]\n",
            "loss: 0.970230  [    4/ 3200]\n",
            "loss: 0.690866  [    5/ 3200]\n",
            "loss: 0.844549  [    6/ 3200]\n",
            "loss: 0.799177  [    7/ 3200]\n",
            "loss: 0.912965  [    8/ 3200]\n",
            "loss: 1.198105  [    9/ 3200]\n",
            "loss: 0.895208  [   10/ 3200]\n",
            "loss: 0.880662  [   11/ 3200]\n",
            "loss: 1.272694  [   12/ 3200]\n",
            "loss: 0.729739  [   13/ 3200]\n",
            "loss: 0.968582  [   14/ 3200]\n",
            "loss: 1.030914  [   15/ 3200]\n",
            "loss: 1.091128  [   16/ 3200]\n",
            "loss: 0.765898  [   17/ 3200]\n",
            "loss: 0.987563  [   18/ 3200]\n",
            "loss: 1.063223  [   19/ 3200]\n",
            "loss: 0.902205  [   20/ 3200]\n",
            "loss: 0.954354  [   21/ 3200]\n",
            "loss: 0.998710  [   22/ 3200]\n",
            "loss: 0.694567  [   23/ 3200]\n",
            "loss: 0.733107  [   24/ 3200]\n",
            "loss: 0.764133  [   25/ 3200]\n",
            "loss: 0.873671  [   26/ 3200]\n",
            "loss: 0.846542  [   27/ 3200]\n",
            "loss: 0.965195  [   28/ 3200]\n",
            "loss: 1.030864  [   29/ 3200]\n",
            "loss: 0.826631  [   30/ 3200]\n",
            "loss: 0.727028  [   31/ 3200]\n",
            "loss: 0.896067  [   32/ 3200]\n",
            "loss: 0.742544  [   33/ 3200]\n",
            "loss: 0.761058  [   34/ 3200]\n",
            "loss: 0.641086  [   35/ 3200]\n",
            "loss: 1.013037  [   36/ 3200]\n",
            "loss: 0.840284  [   37/ 3200]\n",
            "loss: 0.758809  [   38/ 3200]\n",
            "loss: 0.933682  [   39/ 3200]\n",
            "loss: 0.858586  [   40/ 3200]\n",
            "loss: 0.746994  [   41/ 3200]\n",
            "loss: 0.916279  [   42/ 3200]\n",
            "loss: 1.039609  [   43/ 3200]\n",
            "loss: 1.212118  [   44/ 3200]\n",
            "loss: 0.837469  [   45/ 3200]\n",
            "loss: 0.732932  [   46/ 3200]\n",
            "loss: 0.985388  [   47/ 3200]\n",
            "loss: 0.989645  [   48/ 3200]\n",
            "loss: 1.072038  [   49/ 3200]\n",
            "loss: 1.155935  [   50/ 3200]\n",
            "loss: 0.582330  [   51/ 3200]\n",
            "loss: 1.097371  [   52/ 3200]\n",
            "loss: 1.024543  [   53/ 3200]\n",
            "loss: 1.025700  [   54/ 3200]\n",
            "loss: 0.966766  [   55/ 3200]\n",
            "loss: 0.828841  [   56/ 3200]\n",
            "loss: 0.885692  [   57/ 3200]\n",
            "loss: 1.035518  [   58/ 3200]\n",
            "loss: 0.994256  [   59/ 3200]\n",
            "loss: 0.971697  [   60/ 3200]\n",
            "loss: 0.974638  [   61/ 3200]\n",
            "loss: 0.709941  [   62/ 3200]\n",
            "loss: 0.743945  [   63/ 3200]\n",
            "loss: 0.745506  [   64/ 3200]\n",
            "loss: 1.243968  [   65/ 3200]\n",
            "loss: 0.886880  [   66/ 3200]\n",
            "loss: 0.826885  [   67/ 3200]\n",
            "loss: 0.802897  [   68/ 3200]\n",
            "loss: 0.800805  [   69/ 3200]\n",
            "loss: 1.045340  [   70/ 3200]\n",
            "loss: 0.801862  [   71/ 3200]\n",
            "loss: 0.811679  [   72/ 3200]\n",
            "loss: 0.855167  [   73/ 3200]\n",
            "loss: 1.004319  [   74/ 3200]\n",
            "loss: 1.076345  [   75/ 3200]\n",
            "loss: 0.969413  [   76/ 3200]\n",
            "loss: 0.818749  [   77/ 3200]\n",
            "loss: 0.804898  [   78/ 3200]\n",
            "loss: 0.682764  [   79/ 3200]\n",
            "loss: 1.029381  [   80/ 3200]\n",
            "loss: 0.791991  [   81/ 3200]\n",
            "loss: 0.832402  [   82/ 3200]\n",
            "loss: 0.900450  [   83/ 3200]\n",
            "loss: 0.911333  [   84/ 3200]\n",
            "loss: 1.145619  [   85/ 3200]\n",
            "loss: 1.179446  [   86/ 3200]\n",
            "loss: 0.680230  [   87/ 3200]\n",
            "loss: 0.885094  [   88/ 3200]\n",
            "loss: 0.917699  [   89/ 3200]\n",
            "loss: 1.187556  [   90/ 3200]\n",
            "loss: 0.947187  [   91/ 3200]\n",
            "loss: 1.022060  [   92/ 3200]\n",
            "loss: 0.836198  [   93/ 3200]\n",
            "loss: 0.640731  [   94/ 3200]\n",
            "loss: 0.916382  [   95/ 3200]\n",
            "loss: 0.947750  [   96/ 3200]\n",
            "loss: 1.023688  [   97/ 3200]\n",
            "loss: 0.990177  [   98/ 3200]\n",
            "loss: 1.017148  [   99/ 3200]\n",
            "loss: 1.080546  [  100/ 3200]\n",
            "loss: 1.081554  [  101/ 3200]\n",
            "loss: 0.811464  [  102/ 3200]\n",
            "loss: 0.993854  [  103/ 3200]\n",
            "loss: 0.940296  [  104/ 3200]\n",
            "loss: 0.994881  [  105/ 3200]\n",
            "loss: 0.793023  [  106/ 3200]\n",
            "loss: 0.770691  [  107/ 3200]\n",
            "loss: 1.008270  [  108/ 3200]\n",
            "loss: 0.546529  [  109/ 3200]\n",
            "loss: 0.921821  [  110/ 3200]\n",
            "loss: 0.824948  [  111/ 3200]\n",
            "loss: 1.144566  [  112/ 3200]\n",
            "loss: 0.937080  [  113/ 3200]\n",
            "loss: 0.650264  [  114/ 3200]\n",
            "loss: 0.897091  [  115/ 3200]\n",
            "loss: 0.831045  [  116/ 3200]\n",
            "loss: 1.263779  [  117/ 3200]\n",
            "loss: 0.995280  [  118/ 3200]\n",
            "loss: 0.981950  [  119/ 3200]\n",
            "loss: 0.871669  [  120/ 3200]\n",
            "loss: 0.755783  [  121/ 3200]\n",
            "loss: 0.957747  [  122/ 3200]\n",
            "loss: 0.744973  [  123/ 3200]\n",
            "loss: 0.887703  [  124/ 3200]\n",
            "loss: 0.724772  [  125/ 3200]\n",
            "loss: 1.063087  [  126/ 3200]\n",
            "loss: 1.115261  [  127/ 3200]\n",
            "loss: 1.077743  [  128/ 3200]\n",
            "loss: 0.930498  [  129/ 3200]\n",
            "loss: 0.826011  [  130/ 3200]\n",
            "loss: 0.868074  [  131/ 3200]\n",
            "loss: 0.600686  [  132/ 3200]\n",
            "loss: 0.695388  [  133/ 3200]\n",
            "loss: 0.820752  [  134/ 3200]\n",
            "loss: 1.401967  [  135/ 3200]\n",
            "loss: 0.668226  [  136/ 3200]\n",
            "loss: 0.651193  [  137/ 3200]\n",
            "loss: 0.788349  [  138/ 3200]\n",
            "loss: 1.009107  [  139/ 3200]\n",
            "loss: 1.154934  [  140/ 3200]\n",
            "loss: 1.255163  [  141/ 3200]\n",
            "loss: 0.791496  [  142/ 3200]\n",
            "loss: 0.896860  [  143/ 3200]\n",
            "loss: 0.903268  [  144/ 3200]\n",
            "loss: 0.934931  [  145/ 3200]\n",
            "loss: 1.217639  [  146/ 3200]\n",
            "loss: 0.732501  [  147/ 3200]\n",
            "loss: 0.808295  [  148/ 3200]\n",
            "loss: 1.327601  [  149/ 3200]\n",
            "loss: 0.914130  [  150/ 3200]\n",
            "loss: 0.977329  [  151/ 3200]\n",
            "loss: 1.190421  [  152/ 3200]\n",
            "loss: 0.870972  [  153/ 3200]\n",
            "loss: 1.132163  [  154/ 3200]\n",
            "loss: 0.898852  [  155/ 3200]\n",
            "loss: 0.816877  [  156/ 3200]\n",
            "loss: 1.140664  [  157/ 3200]\n",
            "loss: 0.874435  [  158/ 3200]\n",
            "loss: 0.912001  [  159/ 3200]\n",
            "loss: 1.097830  [  160/ 3200]\n",
            "loss: 0.901593  [  161/ 3200]\n",
            "loss: 0.974642  [  162/ 3200]\n",
            "loss: 0.998502  [  163/ 3200]\n",
            "loss: 0.824666  [  164/ 3200]\n",
            "loss: 0.972379  [  165/ 3200]\n",
            "loss: 0.720049  [  166/ 3200]\n",
            "loss: 1.043708  [  167/ 3200]\n",
            "loss: 0.961795  [  168/ 3200]\n",
            "loss: 1.168361  [  169/ 3200]\n",
            "loss: 0.853858  [  170/ 3200]\n",
            "loss: 0.970378  [  171/ 3200]\n",
            "loss: 0.945818  [  172/ 3200]\n",
            "loss: 0.821849  [  173/ 3200]\n",
            "loss: 1.047460  [  174/ 3200]\n",
            "loss: 0.728490  [  175/ 3200]\n",
            "loss: 1.130480  [  176/ 3200]\n",
            "loss: 0.921823  [  177/ 3200]\n",
            "loss: 0.990791  [  178/ 3200]\n",
            "loss: 1.239252  [  179/ 3200]\n",
            "loss: 1.034645  [  180/ 3200]\n",
            "loss: 0.935583  [  181/ 3200]\n",
            "loss: 1.114401  [  182/ 3200]\n",
            "loss: 0.968717  [  183/ 3200]\n",
            "loss: 0.686032  [  184/ 3200]\n",
            "loss: 1.201042  [  185/ 3200]\n",
            "loss: 1.272972  [  186/ 3200]\n",
            "loss: 1.177078  [  187/ 3200]\n",
            "loss: 0.902066  [  188/ 3200]\n",
            "loss: 0.973896  [  189/ 3200]\n",
            "loss: 0.934089  [  190/ 3200]\n",
            "loss: 0.853457  [  191/ 3200]\n",
            "loss: 0.804876  [  192/ 3200]\n",
            "loss: 0.821032  [  193/ 3200]\n",
            "loss: 0.992887  [  194/ 3200]\n",
            "loss: 0.816341  [  195/ 3200]\n",
            "loss: 0.848252  [  196/ 3200]\n",
            "loss: 0.787832  [  197/ 3200]\n",
            "loss: 0.821623  [  198/ 3200]\n",
            "loss: 0.888790  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 4\n",
            "Validation Loss: 0.8574\n",
            "Validation F1: 0.6560\n",
            "Validation Accuracy: 0.6675\n",
            "Validation Confusion Matrix:\n",
            "[[ 72  32  59  37]\n",
            " [ 19 151   4  26]\n",
            " [ 33   1 143  23]\n",
            " [ 15   9   8 168]]\n",
            "Epoch:  1\n",
            "loss: 0.781186  [    0/ 3200]\n",
            "loss: 0.981494  [    1/ 3200]\n",
            "loss: 0.725861  [    2/ 3200]\n",
            "loss: 0.584831  [    3/ 3200]\n",
            "loss: 1.019240  [    4/ 3200]\n",
            "loss: 0.811491  [    5/ 3200]\n",
            "loss: 0.748773  [    6/ 3200]\n",
            "loss: 0.594274  [    7/ 3200]\n",
            "loss: 0.920276  [    8/ 3200]\n",
            "loss: 1.067590  [    9/ 3200]\n",
            "loss: 0.948623  [   10/ 3200]\n",
            "loss: 1.043777  [   11/ 3200]\n",
            "loss: 0.897097  [   12/ 3200]\n",
            "loss: 1.175944  [   13/ 3200]\n",
            "loss: 1.242320  [   14/ 3200]\n",
            "loss: 1.260740  [   15/ 3200]\n",
            "loss: 0.792428  [   16/ 3200]\n",
            "loss: 0.642062  [   17/ 3200]\n",
            "loss: 0.705765  [   18/ 3200]\n",
            "loss: 0.947326  [   19/ 3200]\n",
            "loss: 0.900275  [   20/ 3200]\n",
            "loss: 0.677731  [   21/ 3200]\n",
            "loss: 1.048326  [   22/ 3200]\n",
            "loss: 1.121861  [   23/ 3200]\n",
            "loss: 0.653697  [   24/ 3200]\n",
            "loss: 0.684890  [   25/ 3200]\n",
            "loss: 1.018254  [   26/ 3200]\n",
            "loss: 1.107819  [   27/ 3200]\n",
            "loss: 0.987256  [   28/ 3200]\n",
            "loss: 0.808802  [   29/ 3200]\n",
            "loss: 1.041837  [   30/ 3200]\n",
            "loss: 0.932221  [   31/ 3200]\n",
            "loss: 0.984857  [   32/ 3200]\n",
            "loss: 0.651946  [   33/ 3200]\n",
            "loss: 0.832319  [   34/ 3200]\n",
            "loss: 0.763054  [   35/ 3200]\n",
            "loss: 0.999285  [   36/ 3200]\n",
            "loss: 0.774047  [   37/ 3200]\n",
            "loss: 0.890497  [   38/ 3200]\n",
            "loss: 1.021592  [   39/ 3200]\n",
            "loss: 1.151062  [   40/ 3200]\n",
            "loss: 0.903526  [   41/ 3200]\n",
            "loss: 0.674435  [   42/ 3200]\n",
            "loss: 0.857042  [   43/ 3200]\n",
            "loss: 0.927027  [   44/ 3200]\n",
            "loss: 0.944384  [   45/ 3200]\n",
            "loss: 1.031263  [   46/ 3200]\n",
            "loss: 0.913178  [   47/ 3200]\n",
            "loss: 1.052785  [   48/ 3200]\n",
            "loss: 0.747073  [   49/ 3200]\n",
            "loss: 0.966451  [   50/ 3200]\n",
            "loss: 0.961608  [   51/ 3200]\n",
            "loss: 0.961849  [   52/ 3200]\n",
            "loss: 0.982446  [   53/ 3200]\n",
            "loss: 0.778629  [   54/ 3200]\n",
            "loss: 0.693868  [   55/ 3200]\n",
            "loss: 0.821678  [   56/ 3200]\n",
            "loss: 1.327212  [   57/ 3200]\n",
            "loss: 0.875325  [   58/ 3200]\n",
            "loss: 0.782067  [   59/ 3200]\n",
            "loss: 0.900525  [   60/ 3200]\n",
            "loss: 1.055802  [   61/ 3200]\n",
            "loss: 1.093815  [   62/ 3200]\n",
            "loss: 0.770104  [   63/ 3200]\n",
            "loss: 0.998853  [   64/ 3200]\n",
            "loss: 1.074883  [   65/ 3200]\n",
            "loss: 0.751106  [   66/ 3200]\n",
            "loss: 0.743208  [   67/ 3200]\n",
            "loss: 0.738693  [   68/ 3200]\n",
            "loss: 0.935716  [   69/ 3200]\n",
            "loss: 0.877809  [   70/ 3200]\n",
            "loss: 0.904090  [   71/ 3200]\n",
            "loss: 1.221524  [   72/ 3200]\n",
            "loss: 0.965670  [   73/ 3200]\n",
            "loss: 0.847150  [   74/ 3200]\n",
            "loss: 0.815183  [   75/ 3200]\n",
            "loss: 0.892191  [   76/ 3200]\n",
            "loss: 1.187410  [   77/ 3200]\n",
            "loss: 0.794532  [   78/ 3200]\n",
            "loss: 1.065167  [   79/ 3200]\n",
            "loss: 0.860636  [   80/ 3200]\n",
            "loss: 1.003418  [   81/ 3200]\n",
            "loss: 0.828405  [   82/ 3200]\n",
            "loss: 0.997411  [   83/ 3200]\n",
            "loss: 0.689275  [   84/ 3200]\n",
            "loss: 0.989963  [   85/ 3200]\n",
            "loss: 1.099571  [   86/ 3200]\n",
            "loss: 0.881047  [   87/ 3200]\n",
            "loss: 1.027966  [   88/ 3200]\n",
            "loss: 0.860767  [   89/ 3200]\n",
            "loss: 1.158394  [   90/ 3200]\n",
            "loss: 0.704537  [   91/ 3200]\n",
            "loss: 0.833572  [   92/ 3200]\n",
            "loss: 1.063542  [   93/ 3200]\n",
            "loss: 0.826823  [   94/ 3200]\n",
            "loss: 0.889651  [   95/ 3200]\n",
            "loss: 1.038866  [   96/ 3200]\n",
            "loss: 0.948674  [   97/ 3200]\n",
            "loss: 0.922287  [   98/ 3200]\n",
            "loss: 0.990904  [   99/ 3200]\n",
            "loss: 0.864350  [  100/ 3200]\n",
            "loss: 0.647239  [  101/ 3200]\n",
            "loss: 0.613329  [  102/ 3200]\n",
            "loss: 0.828530  [  103/ 3200]\n",
            "loss: 1.079465  [  104/ 3200]\n",
            "loss: 0.801295  [  105/ 3200]\n",
            "loss: 0.889462  [  106/ 3200]\n",
            "loss: 0.893508  [  107/ 3200]\n",
            "loss: 0.881488  [  108/ 3200]\n",
            "loss: 0.987926  [  109/ 3200]\n",
            "loss: 0.778754  [  110/ 3200]\n",
            "loss: 0.691241  [  111/ 3200]\n",
            "loss: 0.900985  [  112/ 3200]\n",
            "loss: 0.747209  [  113/ 3200]\n",
            "loss: 0.653063  [  114/ 3200]\n",
            "loss: 0.730360  [  115/ 3200]\n",
            "loss: 0.806142  [  116/ 3200]\n",
            "loss: 0.990998  [  117/ 3200]\n",
            "loss: 1.063678  [  118/ 3200]\n",
            "loss: 0.986550  [  119/ 3200]\n",
            "loss: 0.845000  [  120/ 3200]\n",
            "loss: 0.848435  [  121/ 3200]\n",
            "loss: 0.788124  [  122/ 3200]\n",
            "loss: 0.954703  [  123/ 3200]\n",
            "loss: 1.035010  [  124/ 3200]\n",
            "loss: 1.009963  [  125/ 3200]\n",
            "loss: 1.063580  [  126/ 3200]\n",
            "loss: 0.973917  [  127/ 3200]\n",
            "loss: 0.985535  [  128/ 3200]\n",
            "loss: 0.791313  [  129/ 3200]\n",
            "loss: 1.099361  [  130/ 3200]\n",
            "loss: 0.956615  [  131/ 3200]\n",
            "loss: 0.870924  [  132/ 3200]\n",
            "loss: 1.023562  [  133/ 3200]\n",
            "loss: 1.123220  [  134/ 3200]\n",
            "loss: 0.802361  [  135/ 3200]\n",
            "loss: 0.897781  [  136/ 3200]\n",
            "loss: 1.138947  [  137/ 3200]\n",
            "loss: 1.089949  [  138/ 3200]\n",
            "loss: 0.997762  [  139/ 3200]\n",
            "loss: 0.936646  [  140/ 3200]\n",
            "loss: 1.041279  [  141/ 3200]\n",
            "loss: 1.049881  [  142/ 3200]\n",
            "loss: 0.932799  [  143/ 3200]\n",
            "loss: 0.802483  [  144/ 3200]\n",
            "loss: 1.103989  [  145/ 3200]\n",
            "loss: 0.974211  [  146/ 3200]\n",
            "loss: 1.044769  [  147/ 3200]\n",
            "loss: 1.207158  [  148/ 3200]\n",
            "loss: 0.854497  [  149/ 3200]\n",
            "loss: 0.725013  [  150/ 3200]\n",
            "loss: 1.001700  [  151/ 3200]\n",
            "loss: 1.083537  [  152/ 3200]\n",
            "loss: 1.197491  [  153/ 3200]\n",
            "loss: 1.255349  [  154/ 3200]\n",
            "loss: 1.211738  [  155/ 3200]\n",
            "loss: 1.183865  [  156/ 3200]\n",
            "loss: 1.012965  [  157/ 3200]\n",
            "loss: 1.082430  [  158/ 3200]\n",
            "loss: 0.697194  [  159/ 3200]\n",
            "loss: 0.861039  [  160/ 3200]\n",
            "loss: 1.123247  [  161/ 3200]\n",
            "loss: 1.205884  [  162/ 3200]\n",
            "loss: 0.946670  [  163/ 3200]\n",
            "loss: 0.640921  [  164/ 3200]\n",
            "loss: 0.893795  [  165/ 3200]\n",
            "loss: 0.913857  [  166/ 3200]\n",
            "loss: 0.844773  [  167/ 3200]\n",
            "loss: 0.988618  [  168/ 3200]\n",
            "loss: 0.872341  [  169/ 3200]\n",
            "loss: 0.952816  [  170/ 3200]\n",
            "loss: 1.035442  [  171/ 3200]\n",
            "loss: 1.017782  [  172/ 3200]\n",
            "loss: 1.200962  [  173/ 3200]\n",
            "loss: 0.927595  [  174/ 3200]\n",
            "loss: 0.858193  [  175/ 3200]\n",
            "loss: 1.079967  [  176/ 3200]\n",
            "loss: 0.979482  [  177/ 3200]\n",
            "loss: 0.762451  [  178/ 3200]\n",
            "loss: 0.651445  [  179/ 3200]\n",
            "loss: 0.984867  [  180/ 3200]\n",
            "loss: 1.011944  [  181/ 3200]\n",
            "loss: 0.902092  [  182/ 3200]\n",
            "loss: 0.894919  [  183/ 3200]\n",
            "loss: 0.732080  [  184/ 3200]\n",
            "loss: 0.998716  [  185/ 3200]\n",
            "loss: 0.879025  [  186/ 3200]\n",
            "loss: 0.988863  [  187/ 3200]\n",
            "loss: 0.742337  [  188/ 3200]\n",
            "loss: 1.040335  [  189/ 3200]\n",
            "loss: 0.935368  [  190/ 3200]\n",
            "loss: 0.734415  [  191/ 3200]\n",
            "loss: 0.766797  [  192/ 3200]\n",
            "loss: 0.825150  [  193/ 3200]\n",
            "loss: 0.942580  [  194/ 3200]\n",
            "loss: 0.726329  [  195/ 3200]\n",
            "loss: 0.959255  [  196/ 3200]\n",
            "loss: 0.890760  [  197/ 3200]\n",
            "loss: 0.809059  [  198/ 3200]\n",
            "loss: 1.186044  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 5\n",
            "Validation Loss: 0.8506\n",
            "Validation F1: 0.6683\n",
            "Validation Accuracy: 0.6800\n",
            "Validation Confusion Matrix:\n",
            "[[ 69  37  70  24]\n",
            " [ 19 170   3   8]\n",
            " [ 35   3 152  10]\n",
            " [ 20  13  14 153]]\n",
            "Epoch:  1\n",
            "loss: 0.853068  [    0/ 3200]\n",
            "loss: 0.944860  [    1/ 3200]\n",
            "loss: 0.698643  [    2/ 3200]\n",
            "loss: 0.972616  [    3/ 3200]\n",
            "loss: 0.818611  [    4/ 3200]\n",
            "loss: 0.868688  [    5/ 3200]\n",
            "loss: 0.808726  [    6/ 3200]\n",
            "loss: 0.694724  [    7/ 3200]\n",
            "loss: 0.878261  [    8/ 3200]\n",
            "loss: 0.949304  [    9/ 3200]\n",
            "loss: 0.920671  [   10/ 3200]\n",
            "loss: 0.984381  [   11/ 3200]\n",
            "loss: 1.035625  [   12/ 3200]\n",
            "loss: 0.749133  [   13/ 3200]\n",
            "loss: 1.180064  [   14/ 3200]\n",
            "loss: 0.668666  [   15/ 3200]\n",
            "loss: 0.686584  [   16/ 3200]\n",
            "loss: 0.957497  [   17/ 3200]\n",
            "loss: 1.261612  [   18/ 3200]\n",
            "loss: 0.861735  [   19/ 3200]\n",
            "loss: 1.019941  [   20/ 3200]\n",
            "loss: 0.793773  [   21/ 3200]\n",
            "loss: 1.014179  [   22/ 3200]\n",
            "loss: 1.069252  [   23/ 3200]\n",
            "loss: 1.014651  [   24/ 3200]\n",
            "loss: 0.866153  [   25/ 3200]\n",
            "loss: 0.748225  [   26/ 3200]\n",
            "loss: 1.168549  [   27/ 3200]\n",
            "loss: 0.585451  [   28/ 3200]\n",
            "loss: 0.981907  [   29/ 3200]\n",
            "loss: 0.939990  [   30/ 3200]\n",
            "loss: 1.104571  [   31/ 3200]\n",
            "loss: 0.906004  [   32/ 3200]\n",
            "loss: 0.920918  [   33/ 3200]\n",
            "loss: 1.002763  [   34/ 3200]\n",
            "loss: 1.109772  [   35/ 3200]\n",
            "loss: 0.808789  [   36/ 3200]\n",
            "loss: 0.708031  [   37/ 3200]\n",
            "loss: 0.841692  [   38/ 3200]\n",
            "loss: 1.035916  [   39/ 3200]\n",
            "loss: 0.961935  [   40/ 3200]\n",
            "loss: 1.203644  [   41/ 3200]\n",
            "loss: 0.795339  [   42/ 3200]\n",
            "loss: 0.895212  [   43/ 3200]\n",
            "loss: 0.904848  [   44/ 3200]\n",
            "loss: 1.253971  [   45/ 3200]\n",
            "loss: 0.901285  [   46/ 3200]\n",
            "loss: 0.633693  [   47/ 3200]\n",
            "loss: 0.863749  [   48/ 3200]\n",
            "loss: 0.710938  [   49/ 3200]\n",
            "loss: 0.823817  [   50/ 3200]\n",
            "loss: 0.799378  [   51/ 3200]\n",
            "loss: 1.081997  [   52/ 3200]\n",
            "loss: 0.896604  [   53/ 3200]\n",
            "loss: 1.206811  [   54/ 3200]\n",
            "loss: 1.196114  [   55/ 3200]\n",
            "loss: 0.827855  [   56/ 3200]\n",
            "loss: 1.240279  [   57/ 3200]\n",
            "loss: 0.688167  [   58/ 3200]\n",
            "loss: 0.941707  [   59/ 3200]\n",
            "loss: 1.107911  [   60/ 3200]\n",
            "loss: 1.072841  [   61/ 3200]\n",
            "loss: 0.960927  [   62/ 3200]\n",
            "loss: 0.839121  [   63/ 3200]\n",
            "loss: 0.763927  [   64/ 3200]\n",
            "loss: 1.071150  [   65/ 3200]\n",
            "loss: 1.056827  [   66/ 3200]\n",
            "loss: 0.968373  [   67/ 3200]\n",
            "loss: 0.937115  [   68/ 3200]\n",
            "loss: 0.809233  [   69/ 3200]\n",
            "loss: 0.838212  [   70/ 3200]\n",
            "loss: 0.900921  [   71/ 3200]\n",
            "loss: 0.916981  [   72/ 3200]\n",
            "loss: 1.111421  [   73/ 3200]\n",
            "loss: 0.841161  [   74/ 3200]\n",
            "loss: 1.188805  [   75/ 3200]\n",
            "loss: 0.884358  [   76/ 3200]\n",
            "loss: 1.058339  [   77/ 3200]\n",
            "loss: 0.678330  [   78/ 3200]\n",
            "loss: 0.934087  [   79/ 3200]\n",
            "loss: 0.951057  [   80/ 3200]\n",
            "loss: 0.778864  [   81/ 3200]\n",
            "loss: 0.665624  [   82/ 3200]\n",
            "loss: 0.916572  [   83/ 3200]\n",
            "loss: 1.067924  [   84/ 3200]\n",
            "loss: 0.927316  [   85/ 3200]\n",
            "loss: 0.917891  [   86/ 3200]\n",
            "loss: 0.992870  [   87/ 3200]\n",
            "loss: 1.066236  [   88/ 3200]\n",
            "loss: 0.906586  [   89/ 3200]\n",
            "loss: 0.952176  [   90/ 3200]\n",
            "loss: 0.847314  [   91/ 3200]\n",
            "loss: 0.788611  [   92/ 3200]\n",
            "loss: 0.783501  [   93/ 3200]\n",
            "loss: 1.150139  [   94/ 3200]\n",
            "loss: 1.028344  [   95/ 3200]\n",
            "loss: 1.126438  [   96/ 3200]\n",
            "loss: 0.715842  [   97/ 3200]\n",
            "loss: 0.964267  [   98/ 3200]\n",
            "loss: 1.089085  [   99/ 3200]\n",
            "loss: 0.775299  [  100/ 3200]\n",
            "loss: 0.834914  [  101/ 3200]\n",
            "loss: 0.926584  [  102/ 3200]\n",
            "loss: 1.042131  [  103/ 3200]\n",
            "loss: 0.884124  [  104/ 3200]\n",
            "loss: 0.945936  [  105/ 3200]\n",
            "loss: 0.767382  [  106/ 3200]\n",
            "loss: 0.630506  [  107/ 3200]\n",
            "loss: 0.866722  [  108/ 3200]\n",
            "loss: 0.865155  [  109/ 3200]\n",
            "loss: 1.029940  [  110/ 3200]\n",
            "loss: 0.745284  [  111/ 3200]\n",
            "loss: 1.119484  [  112/ 3200]\n",
            "loss: 1.019539  [  113/ 3200]\n",
            "loss: 0.878262  [  114/ 3200]\n",
            "loss: 0.879959  [  115/ 3200]\n",
            "loss: 0.917244  [  116/ 3200]\n",
            "loss: 0.845828  [  117/ 3200]\n",
            "loss: 0.962800  [  118/ 3200]\n",
            "loss: 0.963799  [  119/ 3200]\n",
            "loss: 0.857869  [  120/ 3200]\n",
            "loss: 0.603154  [  121/ 3200]\n",
            "loss: 0.810072  [  122/ 3200]\n",
            "loss: 0.911964  [  123/ 3200]\n",
            "loss: 1.105054  [  124/ 3200]\n",
            "loss: 0.945386  [  125/ 3200]\n",
            "loss: 0.595776  [  126/ 3200]\n",
            "loss: 1.061348  [  127/ 3200]\n",
            "loss: 1.176466  [  128/ 3200]\n",
            "loss: 0.937508  [  129/ 3200]\n",
            "loss: 0.815579  [  130/ 3200]\n",
            "loss: 1.229141  [  131/ 3200]\n",
            "loss: 1.079586  [  132/ 3200]\n",
            "loss: 0.739241  [  133/ 3200]\n",
            "loss: 1.126501  [  134/ 3200]\n",
            "loss: 1.074262  [  135/ 3200]\n",
            "loss: 1.075662  [  136/ 3200]\n",
            "loss: 1.265114  [  137/ 3200]\n",
            "loss: 0.988149  [  138/ 3200]\n",
            "loss: 0.854078  [  139/ 3200]\n",
            "loss: 1.235781  [  140/ 3200]\n",
            "loss: 1.058326  [  141/ 3200]\n",
            "loss: 0.980702  [  142/ 3200]\n",
            "loss: 0.729831  [  143/ 3200]\n",
            "loss: 0.662382  [  144/ 3200]\n",
            "loss: 0.786738  [  145/ 3200]\n",
            "loss: 0.805456  [  146/ 3200]\n",
            "loss: 0.819391  [  147/ 3200]\n",
            "loss: 1.081117  [  148/ 3200]\n",
            "loss: 0.882533  [  149/ 3200]\n",
            "loss: 1.148587  [  150/ 3200]\n",
            "loss: 0.778892  [  151/ 3200]\n",
            "loss: 1.015850  [  152/ 3200]\n",
            "loss: 1.020419  [  153/ 3200]\n",
            "loss: 1.076964  [  154/ 3200]\n",
            "loss: 0.936338  [  155/ 3200]\n",
            "loss: 0.859838  [  156/ 3200]\n",
            "loss: 0.669595  [  157/ 3200]\n",
            "loss: 0.933114  [  158/ 3200]\n",
            "loss: 1.189857  [  159/ 3200]\n",
            "loss: 0.939067  [  160/ 3200]\n",
            "loss: 1.202380  [  161/ 3200]\n",
            "loss: 1.251338  [  162/ 3200]\n",
            "loss: 1.177220  [  163/ 3200]\n",
            "loss: 0.901543  [  164/ 3200]\n",
            "loss: 0.929752  [  165/ 3200]\n",
            "loss: 1.089523  [  166/ 3200]\n",
            "loss: 0.789563  [  167/ 3200]\n",
            "loss: 0.623981  [  168/ 3200]\n",
            "loss: 0.737919  [  169/ 3200]\n",
            "loss: 0.826111  [  170/ 3200]\n",
            "loss: 0.688137  [  171/ 3200]\n",
            "loss: 0.977145  [  172/ 3200]\n",
            "loss: 1.163405  [  173/ 3200]\n",
            "loss: 1.003315  [  174/ 3200]\n",
            "loss: 0.823337  [  175/ 3200]\n",
            "loss: 0.894567  [  176/ 3200]\n",
            "loss: 1.344261  [  177/ 3200]\n",
            "loss: 0.814893  [  178/ 3200]\n",
            "loss: 0.826275  [  179/ 3200]\n",
            "loss: 0.907227  [  180/ 3200]\n",
            "loss: 0.721369  [  181/ 3200]\n",
            "loss: 0.972895  [  182/ 3200]\n",
            "loss: 0.952335  [  183/ 3200]\n",
            "loss: 0.848365  [  184/ 3200]\n",
            "loss: 0.985298  [  185/ 3200]\n",
            "loss: 0.909867  [  186/ 3200]\n",
            "loss: 0.771904  [  187/ 3200]\n",
            "loss: 1.052032  [  188/ 3200]\n",
            "loss: 0.810800  [  189/ 3200]\n",
            "loss: 0.825253  [  190/ 3200]\n",
            "loss: 0.455822  [  191/ 3200]\n",
            "loss: 0.654436  [  192/ 3200]\n",
            "loss: 0.629367  [  193/ 3200]\n",
            "loss: 0.735531  [  194/ 3200]\n",
            "loss: 0.978678  [  195/ 3200]\n",
            "loss: 0.884556  [  196/ 3200]\n",
            "loss: 0.780698  [  197/ 3200]\n",
            "loss: 0.652481  [  198/ 3200]\n",
            "loss: 0.848439  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 6\n",
            "Validation Loss: 0.8465\n",
            "Validation F1: 0.6304\n",
            "Validation Accuracy: 0.6700\n",
            "Validation Confusion Matrix:\n",
            "[[ 38  59  63  40]\n",
            " [  2 184   4  10]\n",
            " [ 22   7 146  25]\n",
            " [  5  17  10 168]]\n",
            "Epoch:  1\n",
            "loss: 1.194197  [    0/ 3200]\n",
            "loss: 1.074297  [    1/ 3200]\n",
            "loss: 0.690323  [    2/ 3200]\n",
            "loss: 0.937740  [    3/ 3200]\n",
            "loss: 0.886091  [    4/ 3200]\n",
            "loss: 0.708802  [    5/ 3200]\n",
            "loss: 0.809606  [    6/ 3200]\n",
            "loss: 0.709960  [    7/ 3200]\n",
            "loss: 0.841703  [    8/ 3200]\n",
            "loss: 1.180303  [    9/ 3200]\n",
            "loss: 1.064406  [   10/ 3200]\n",
            "loss: 0.897457  [   11/ 3200]\n",
            "loss: 0.921287  [   12/ 3200]\n",
            "loss: 1.143017  [   13/ 3200]\n",
            "loss: 1.076438  [   14/ 3200]\n",
            "loss: 0.904892  [   15/ 3200]\n",
            "loss: 0.890382  [   16/ 3200]\n",
            "loss: 1.064327  [   17/ 3200]\n",
            "loss: 0.869340  [   18/ 3200]\n",
            "loss: 0.923817  [   19/ 3200]\n",
            "loss: 1.158227  [   20/ 3200]\n",
            "loss: 1.166515  [   21/ 3200]\n",
            "loss: 0.569920  [   22/ 3200]\n",
            "loss: 1.006236  [   23/ 3200]\n",
            "loss: 0.687566  [   24/ 3200]\n",
            "loss: 0.640681  [   25/ 3200]\n",
            "loss: 0.943135  [   26/ 3200]\n",
            "loss: 0.929291  [   27/ 3200]\n",
            "loss: 0.985625  [   28/ 3200]\n",
            "loss: 1.021052  [   29/ 3200]\n",
            "loss: 1.015900  [   30/ 3200]\n",
            "loss: 0.872197  [   31/ 3200]\n",
            "loss: 1.191407  [   32/ 3200]\n",
            "loss: 1.112503  [   33/ 3200]\n",
            "loss: 0.968499  [   34/ 3200]\n",
            "loss: 0.748978  [   35/ 3200]\n",
            "loss: 1.020427  [   36/ 3200]\n",
            "loss: 0.908505  [   37/ 3200]\n",
            "loss: 0.751648  [   38/ 3200]\n",
            "loss: 0.409794  [   39/ 3200]\n",
            "loss: 0.914052  [   40/ 3200]\n",
            "loss: 1.049354  [   41/ 3200]\n",
            "loss: 0.897168  [   42/ 3200]\n",
            "loss: 1.031387  [   43/ 3200]\n",
            "loss: 0.529762  [   44/ 3200]\n",
            "loss: 1.008084  [   45/ 3200]\n",
            "loss: 0.867928  [   46/ 3200]\n",
            "loss: 1.147459  [   47/ 3200]\n",
            "loss: 1.082633  [   48/ 3200]\n",
            "loss: 0.910769  [   49/ 3200]\n",
            "loss: 0.789117  [   50/ 3200]\n",
            "loss: 1.061560  [   51/ 3200]\n",
            "loss: 0.824179  [   52/ 3200]\n",
            "loss: 1.619007  [   53/ 3200]\n",
            "loss: 0.787803  [   54/ 3200]\n",
            "loss: 0.872541  [   55/ 3200]\n",
            "loss: 1.426504  [   56/ 3200]\n",
            "loss: 0.907512  [   57/ 3200]\n",
            "loss: 0.881022  [   58/ 3200]\n",
            "loss: 1.180356  [   59/ 3200]\n",
            "loss: 0.646922  [   60/ 3200]\n",
            "loss: 1.230598  [   61/ 3200]\n",
            "loss: 1.001731  [   62/ 3200]\n",
            "loss: 1.128940  [   63/ 3200]\n",
            "loss: 0.716905  [   64/ 3200]\n",
            "loss: 0.824537  [   65/ 3200]\n",
            "loss: 0.730100  [   66/ 3200]\n",
            "loss: 0.914219  [   67/ 3200]\n",
            "loss: 0.841380  [   68/ 3200]\n",
            "loss: 1.144316  [   69/ 3200]\n",
            "loss: 1.015516  [   70/ 3200]\n",
            "loss: 0.830741  [   71/ 3200]\n",
            "loss: 1.118415  [   72/ 3200]\n",
            "loss: 1.261963  [   73/ 3200]\n",
            "loss: 0.882816  [   74/ 3200]\n",
            "loss: 0.943066  [   75/ 3200]\n",
            "loss: 1.129334  [   76/ 3200]\n",
            "loss: 0.604486  [   77/ 3200]\n",
            "loss: 1.220846  [   78/ 3200]\n",
            "loss: 0.656155  [   79/ 3200]\n",
            "loss: 1.041727  [   80/ 3200]\n",
            "loss: 0.889677  [   81/ 3200]\n",
            "loss: 0.900098  [   82/ 3200]\n",
            "loss: 0.769218  [   83/ 3200]\n",
            "loss: 1.060645  [   84/ 3200]\n",
            "loss: 0.813749  [   85/ 3200]\n",
            "loss: 0.841510  [   86/ 3200]\n",
            "loss: 1.000829  [   87/ 3200]\n",
            "loss: 0.937948  [   88/ 3200]\n",
            "loss: 0.994046  [   89/ 3200]\n",
            "loss: 0.671125  [   90/ 3200]\n",
            "loss: 0.716363  [   91/ 3200]\n",
            "loss: 0.782941  [   92/ 3200]\n",
            "loss: 0.811694  [   93/ 3200]\n",
            "loss: 0.825375  [   94/ 3200]\n",
            "loss: 0.707318  [   95/ 3200]\n",
            "loss: 1.189568  [   96/ 3200]\n",
            "loss: 0.944998  [   97/ 3200]\n",
            "loss: 0.738374  [   98/ 3200]\n",
            "loss: 0.697600  [   99/ 3200]\n",
            "loss: 0.880947  [  100/ 3200]\n",
            "loss: 0.926787  [  101/ 3200]\n",
            "loss: 1.078250  [  102/ 3200]\n",
            "loss: 0.927963  [  103/ 3200]\n",
            "loss: 0.749884  [  104/ 3200]\n",
            "loss: 0.691296  [  105/ 3200]\n",
            "loss: 1.031402  [  106/ 3200]\n",
            "loss: 0.883671  [  107/ 3200]\n",
            "loss: 1.185238  [  108/ 3200]\n",
            "loss: 1.028901  [  109/ 3200]\n",
            "loss: 0.675512  [  110/ 3200]\n",
            "loss: 0.715718  [  111/ 3200]\n",
            "loss: 0.643575  [  112/ 3200]\n",
            "loss: 1.032480  [  113/ 3200]\n",
            "loss: 0.709855  [  114/ 3200]\n",
            "loss: 0.869279  [  115/ 3200]\n",
            "loss: 0.651619  [  116/ 3200]\n",
            "loss: 0.818451  [  117/ 3200]\n",
            "loss: 1.025763  [  118/ 3200]\n",
            "loss: 0.971961  [  119/ 3200]\n",
            "loss: 1.133121  [  120/ 3200]\n",
            "loss: 0.955093  [  121/ 3200]\n",
            "loss: 0.963482  [  122/ 3200]\n",
            "loss: 0.794430  [  123/ 3200]\n",
            "loss: 0.618937  [  124/ 3200]\n",
            "loss: 0.741169  [  125/ 3200]\n",
            "loss: 0.764368  [  126/ 3200]\n",
            "loss: 1.072951  [  127/ 3200]\n",
            "loss: 0.919941  [  128/ 3200]\n",
            "loss: 1.027541  [  129/ 3200]\n",
            "loss: 0.825449  [  130/ 3200]\n",
            "loss: 0.710676  [  131/ 3200]\n",
            "loss: 1.004540  [  132/ 3200]\n",
            "loss: 1.018839  [  133/ 3200]\n",
            "loss: 0.757450  [  134/ 3200]\n",
            "loss: 1.106515  [  135/ 3200]\n",
            "loss: 0.842094  [  136/ 3200]\n",
            "loss: 0.918427  [  137/ 3200]\n",
            "loss: 0.985258  [  138/ 3200]\n",
            "loss: 0.785823  [  139/ 3200]\n",
            "loss: 0.768052  [  140/ 3200]\n",
            "loss: 0.907843  [  141/ 3200]\n",
            "loss: 0.822950  [  142/ 3200]\n",
            "loss: 1.008376  [  143/ 3200]\n",
            "loss: 0.832786  [  144/ 3200]\n",
            "loss: 0.885133  [  145/ 3200]\n",
            "loss: 1.100483  [  146/ 3200]\n",
            "loss: 1.321489  [  147/ 3200]\n",
            "loss: 0.859454  [  148/ 3200]\n",
            "loss: 0.984276  [  149/ 3200]\n",
            "loss: 0.912506  [  150/ 3200]\n",
            "loss: 1.051521  [  151/ 3200]\n",
            "loss: 1.043781  [  152/ 3200]\n",
            "loss: 1.180901  [  153/ 3200]\n",
            "loss: 0.969056  [  154/ 3200]\n",
            "loss: 0.990539  [  155/ 3200]\n",
            "loss: 1.274758  [  156/ 3200]\n",
            "loss: 0.937104  [  157/ 3200]\n",
            "loss: 0.966723  [  158/ 3200]\n",
            "loss: 1.072273  [  159/ 3200]\n",
            "loss: 0.979021  [  160/ 3200]\n",
            "loss: 0.722841  [  161/ 3200]\n",
            "loss: 0.833811  [  162/ 3200]\n",
            "loss: 0.682667  [  163/ 3200]\n",
            "loss: 1.067049  [  164/ 3200]\n",
            "loss: 1.011638  [  165/ 3200]\n",
            "loss: 1.115855  [  166/ 3200]\n",
            "loss: 0.841536  [  167/ 3200]\n",
            "loss: 0.755205  [  168/ 3200]\n",
            "loss: 1.208014  [  169/ 3200]\n",
            "loss: 1.178023  [  170/ 3200]\n",
            "loss: 1.115288  [  171/ 3200]\n",
            "loss: 0.954830  [  172/ 3200]\n",
            "loss: 0.854888  [  173/ 3200]\n",
            "loss: 0.779463  [  174/ 3200]\n",
            "loss: 1.005124  [  175/ 3200]\n",
            "loss: 1.036054  [  176/ 3200]\n",
            "loss: 0.983452  [  177/ 3200]\n",
            "loss: 0.934150  [  178/ 3200]\n",
            "loss: 1.064826  [  179/ 3200]\n",
            "loss: 0.917177  [  180/ 3200]\n",
            "loss: 0.562458  [  181/ 3200]\n",
            "loss: 0.935140  [  182/ 3200]\n",
            "loss: 0.747269  [  183/ 3200]\n",
            "loss: 0.827087  [  184/ 3200]\n",
            "loss: 0.591106  [  185/ 3200]\n",
            "loss: 0.731407  [  186/ 3200]\n",
            "loss: 0.877650  [  187/ 3200]\n",
            "loss: 0.713227  [  188/ 3200]\n",
            "loss: 0.966610  [  189/ 3200]\n",
            "loss: 0.642758  [  190/ 3200]\n",
            "loss: 0.668008  [  191/ 3200]\n",
            "loss: 0.770908  [  192/ 3200]\n",
            "loss: 0.701915  [  193/ 3200]\n",
            "loss: 0.699430  [  194/ 3200]\n",
            "loss: 0.819330  [  195/ 3200]\n",
            "loss: 0.918245  [  196/ 3200]\n",
            "loss: 0.873738  [  197/ 3200]\n",
            "loss: 0.860686  [  198/ 3200]\n",
            "loss: 1.103600  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 7\n",
            "Validation Loss: 0.8446\n",
            "Validation F1: 0.6667\n",
            "Validation Accuracy: 0.6837\n",
            "Validation Confusion Matrix:\n",
            "[[ 70  57  44  29]\n",
            " [  4 185   3   8]\n",
            " [ 39   7 132  22]\n",
            " [ 15  17   8 160]]\n",
            "Epoch:  1\n",
            "loss: 0.735204  [    0/ 3200]\n",
            "loss: 0.729732  [    1/ 3200]\n",
            "loss: 0.968601  [    2/ 3200]\n",
            "loss: 0.832759  [    3/ 3200]\n",
            "loss: 0.857800  [    4/ 3200]\n",
            "loss: 0.929616  [    5/ 3200]\n",
            "loss: 0.764846  [    6/ 3200]\n",
            "loss: 0.848784  [    7/ 3200]\n",
            "loss: 1.192739  [    8/ 3200]\n",
            "loss: 0.663530  [    9/ 3200]\n",
            "loss: 0.870888  [   10/ 3200]\n",
            "loss: 0.907593  [   11/ 3200]\n",
            "loss: 1.071307  [   12/ 3200]\n",
            "loss: 0.866596  [   13/ 3200]\n",
            "loss: 0.826006  [   14/ 3200]\n",
            "loss: 0.733643  [   15/ 3200]\n",
            "loss: 1.442109  [   16/ 3200]\n",
            "loss: 1.358065  [   17/ 3200]\n",
            "loss: 0.970352  [   18/ 3200]\n",
            "loss: 0.926187  [   19/ 3200]\n",
            "loss: 1.153667  [   20/ 3200]\n",
            "loss: 0.805146  [   21/ 3200]\n",
            "loss: 1.242473  [   22/ 3200]\n",
            "loss: 0.946936  [   23/ 3200]\n",
            "loss: 1.040137  [   24/ 3200]\n",
            "loss: 1.283589  [   25/ 3200]\n",
            "loss: 0.947638  [   26/ 3200]\n",
            "loss: 1.303299  [   27/ 3200]\n",
            "loss: 0.953739  [   28/ 3200]\n",
            "loss: 1.120691  [   29/ 3200]\n",
            "loss: 0.897466  [   30/ 3200]\n",
            "loss: 1.099225  [   31/ 3200]\n",
            "loss: 1.075940  [   32/ 3200]\n",
            "loss: 0.879400  [   33/ 3200]\n",
            "loss: 0.967298  [   34/ 3200]\n",
            "loss: 0.866893  [   35/ 3200]\n",
            "loss: 0.993170  [   36/ 3200]\n",
            "loss: 0.827822  [   37/ 3200]\n",
            "loss: 1.236589  [   38/ 3200]\n",
            "loss: 1.166178  [   39/ 3200]\n",
            "loss: 0.828317  [   40/ 3200]\n",
            "loss: 0.791661  [   41/ 3200]\n",
            "loss: 0.827679  [   42/ 3200]\n",
            "loss: 0.697303  [   43/ 3200]\n",
            "loss: 0.859729  [   44/ 3200]\n",
            "loss: 0.634095  [   45/ 3200]\n",
            "loss: 0.791154  [   46/ 3200]\n",
            "loss: 1.011332  [   47/ 3200]\n",
            "loss: 1.106404  [   48/ 3200]\n",
            "loss: 1.029793  [   49/ 3200]\n",
            "loss: 1.191058  [   50/ 3200]\n",
            "loss: 0.812373  [   51/ 3200]\n",
            "loss: 0.771304  [   52/ 3200]\n",
            "loss: 0.721602  [   53/ 3200]\n",
            "loss: 0.853593  [   54/ 3200]\n",
            "loss: 0.902381  [   55/ 3200]\n",
            "loss: 0.862760  [   56/ 3200]\n",
            "loss: 0.677889  [   57/ 3200]\n",
            "loss: 1.016342  [   58/ 3200]\n",
            "loss: 0.833438  [   59/ 3200]\n",
            "loss: 1.128950  [   60/ 3200]\n",
            "loss: 1.003404  [   61/ 3200]\n",
            "loss: 1.087980  [   62/ 3200]\n",
            "loss: 1.095171  [   63/ 3200]\n",
            "loss: 0.889272  [   64/ 3200]\n",
            "loss: 0.890092  [   65/ 3200]\n",
            "loss: 0.834408  [   66/ 3200]\n",
            "loss: 0.724409  [   67/ 3200]\n",
            "loss: 0.760469  [   68/ 3200]\n",
            "loss: 0.912868  [   69/ 3200]\n",
            "loss: 0.640283  [   70/ 3200]\n",
            "loss: 0.858519  [   71/ 3200]\n",
            "loss: 1.034088  [   72/ 3200]\n",
            "loss: 0.922530  [   73/ 3200]\n",
            "loss: 0.954385  [   74/ 3200]\n",
            "loss: 0.782503  [   75/ 3200]\n",
            "loss: 0.836038  [   76/ 3200]\n",
            "loss: 0.701312  [   77/ 3200]\n",
            "loss: 0.921170  [   78/ 3200]\n",
            "loss: 1.367807  [   79/ 3200]\n",
            "loss: 0.944314  [   80/ 3200]\n",
            "loss: 0.903222  [   81/ 3200]\n",
            "loss: 1.162815  [   82/ 3200]\n",
            "loss: 0.687890  [   83/ 3200]\n",
            "loss: 0.764296  [   84/ 3200]\n",
            "loss: 1.055149  [   85/ 3200]\n",
            "loss: 0.852044  [   86/ 3200]\n",
            "loss: 1.207736  [   87/ 3200]\n",
            "loss: 1.009392  [   88/ 3200]\n",
            "loss: 0.636664  [   89/ 3200]\n",
            "loss: 0.937126  [   90/ 3200]\n",
            "loss: 1.065769  [   91/ 3200]\n",
            "loss: 0.717179  [   92/ 3200]\n",
            "loss: 0.743438  [   93/ 3200]\n",
            "loss: 0.786754  [   94/ 3200]\n",
            "loss: 0.889718  [   95/ 3200]\n",
            "loss: 0.959218  [   96/ 3200]\n",
            "loss: 0.776865  [   97/ 3200]\n",
            "loss: 0.855050  [   98/ 3200]\n",
            "loss: 0.959176  [   99/ 3200]\n",
            "loss: 0.943401  [  100/ 3200]\n",
            "loss: 0.934390  [  101/ 3200]\n",
            "loss: 1.156417  [  102/ 3200]\n",
            "loss: 1.100029  [  103/ 3200]\n",
            "loss: 0.830212  [  104/ 3200]\n",
            "loss: 0.548720  [  105/ 3200]\n",
            "loss: 1.391018  [  106/ 3200]\n",
            "loss: 1.169463  [  107/ 3200]\n",
            "loss: 0.941516  [  108/ 3200]\n",
            "loss: 0.932712  [  109/ 3200]\n",
            "loss: 1.227991  [  110/ 3200]\n",
            "loss: 0.878797  [  111/ 3200]\n",
            "loss: 0.709414  [  112/ 3200]\n",
            "loss: 0.878524  [  113/ 3200]\n",
            "loss: 1.188772  [  114/ 3200]\n",
            "loss: 0.855081  [  115/ 3200]\n",
            "loss: 0.929530  [  116/ 3200]\n",
            "loss: 0.786128  [  117/ 3200]\n",
            "loss: 0.716519  [  118/ 3200]\n",
            "loss: 0.823802  [  119/ 3200]\n",
            "loss: 1.034417  [  120/ 3200]\n",
            "loss: 0.822821  [  121/ 3200]\n",
            "loss: 0.731246  [  122/ 3200]\n",
            "loss: 1.140800  [  123/ 3200]\n",
            "loss: 0.779333  [  124/ 3200]\n",
            "loss: 1.235011  [  125/ 3200]\n",
            "loss: 0.931661  [  126/ 3200]\n",
            "loss: 0.785789  [  127/ 3200]\n",
            "loss: 0.758280  [  128/ 3200]\n",
            "loss: 1.011163  [  129/ 3200]\n",
            "loss: 0.915569  [  130/ 3200]\n",
            "loss: 1.178039  [  131/ 3200]\n",
            "loss: 0.845139  [  132/ 3200]\n",
            "loss: 0.663638  [  133/ 3200]\n",
            "loss: 0.873113  [  134/ 3200]\n",
            "loss: 0.823977  [  135/ 3200]\n",
            "loss: 0.851395  [  136/ 3200]\n",
            "loss: 0.806423  [  137/ 3200]\n",
            "loss: 1.022902  [  138/ 3200]\n",
            "loss: 0.976447  [  139/ 3200]\n",
            "loss: 0.990984  [  140/ 3200]\n",
            "loss: 0.946174  [  141/ 3200]\n",
            "loss: 1.024150  [  142/ 3200]\n",
            "loss: 0.861506  [  143/ 3200]\n",
            "loss: 0.870558  [  144/ 3200]\n",
            "loss: 1.152973  [  145/ 3200]\n",
            "loss: 0.840153  [  146/ 3200]\n",
            "loss: 1.036811  [  147/ 3200]\n",
            "loss: 1.022722  [  148/ 3200]\n",
            "loss: 0.883836  [  149/ 3200]\n",
            "loss: 1.199378  [  150/ 3200]\n",
            "loss: 1.002285  [  151/ 3200]\n",
            "loss: 1.040244  [  152/ 3200]\n",
            "loss: 0.660897  [  153/ 3200]\n",
            "loss: 1.057961  [  154/ 3200]\n",
            "loss: 0.776750  [  155/ 3200]\n",
            "loss: 0.683558  [  156/ 3200]\n",
            "loss: 0.826676  [  157/ 3200]\n",
            "loss: 0.755074  [  158/ 3200]\n",
            "loss: 1.152612  [  159/ 3200]\n",
            "loss: 1.142049  [  160/ 3200]\n",
            "loss: 0.850521  [  161/ 3200]\n",
            "loss: 0.770013  [  162/ 3200]\n",
            "loss: 0.826851  [  163/ 3200]\n",
            "loss: 0.720939  [  164/ 3200]\n",
            "loss: 0.948611  [  165/ 3200]\n",
            "loss: 0.728440  [  166/ 3200]\n",
            "loss: 0.755430  [  167/ 3200]\n",
            "loss: 0.854785  [  168/ 3200]\n",
            "loss: 1.128690  [  169/ 3200]\n",
            "loss: 0.614364  [  170/ 3200]\n",
            "loss: 0.638091  [  171/ 3200]\n",
            "loss: 0.641176  [  172/ 3200]\n",
            "loss: 0.754185  [  173/ 3200]\n",
            "loss: 1.005958  [  174/ 3200]\n",
            "loss: 0.851196  [  175/ 3200]\n",
            "loss: 0.938896  [  176/ 3200]\n",
            "loss: 0.998826  [  177/ 3200]\n",
            "loss: 1.019677  [  178/ 3200]\n",
            "loss: 0.749749  [  179/ 3200]\n",
            "loss: 0.857939  [  180/ 3200]\n",
            "loss: 1.112608  [  181/ 3200]\n",
            "loss: 0.875039  [  182/ 3200]\n",
            "loss: 1.133199  [  183/ 3200]\n",
            "loss: 1.024487  [  184/ 3200]\n",
            "loss: 0.715022  [  185/ 3200]\n",
            "loss: 0.739215  [  186/ 3200]\n",
            "loss: 1.066956  [  187/ 3200]\n",
            "loss: 0.841667  [  188/ 3200]\n",
            "loss: 0.945592  [  189/ 3200]\n",
            "loss: 0.909904  [  190/ 3200]\n",
            "loss: 0.703080  [  191/ 3200]\n",
            "loss: 0.945267  [  192/ 3200]\n",
            "loss: 1.005086  [  193/ 3200]\n",
            "loss: 0.836248  [  194/ 3200]\n",
            "loss: 0.645584  [  195/ 3200]\n",
            "loss: 0.921358  [  196/ 3200]\n",
            "loss: 0.794320  [  197/ 3200]\n",
            "loss: 0.992769  [  198/ 3200]\n",
            "loss: 0.879568  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 8\n",
            "Validation Loss: 0.8385\n",
            "Validation F1: 0.6361\n",
            "Validation Accuracy: 0.6525\n",
            "Validation Confusion Matrix:\n",
            "[[ 60  29  77  34]\n",
            " [ 27 140   4  29]\n",
            " [ 22   1 156  21]\n",
            " [ 12   7  15 166]]\n",
            "Epoch:  1\n",
            "loss: 0.915105  [    0/ 3200]\n",
            "loss: 0.864211  [    1/ 3200]\n",
            "loss: 1.142870  [    2/ 3200]\n",
            "loss: 0.908223  [    3/ 3200]\n",
            "loss: 0.934789  [    4/ 3200]\n",
            "loss: 1.077300  [    5/ 3200]\n",
            "loss: 0.754371  [    6/ 3200]\n",
            "loss: 1.053101  [    7/ 3200]\n",
            "loss: 0.921005  [    8/ 3200]\n",
            "loss: 0.850163  [    9/ 3200]\n",
            "loss: 1.111508  [   10/ 3200]\n",
            "loss: 0.625053  [   11/ 3200]\n",
            "loss: 0.910970  [   12/ 3200]\n",
            "loss: 0.854922  [   13/ 3200]\n",
            "loss: 1.012539  [   14/ 3200]\n",
            "loss: 0.681471  [   15/ 3200]\n",
            "loss: 1.173885  [   16/ 3200]\n",
            "loss: 0.967831  [   17/ 3200]\n",
            "loss: 0.756762  [   18/ 3200]\n",
            "loss: 1.099674  [   19/ 3200]\n",
            "loss: 1.166068  [   20/ 3200]\n",
            "loss: 1.034644  [   21/ 3200]\n",
            "loss: 0.812423  [   22/ 3200]\n",
            "loss: 0.753324  [   23/ 3200]\n",
            "loss: 0.904205  [   24/ 3200]\n",
            "loss: 0.925776  [   25/ 3200]\n",
            "loss: 0.833209  [   26/ 3200]\n",
            "loss: 0.902396  [   27/ 3200]\n",
            "loss: 0.863812  [   28/ 3200]\n",
            "loss: 1.119754  [   29/ 3200]\n",
            "loss: 0.884767  [   30/ 3200]\n",
            "loss: 0.924438  [   31/ 3200]\n",
            "loss: 0.788187  [   32/ 3200]\n",
            "loss: 0.742594  [   33/ 3200]\n",
            "loss: 0.756776  [   34/ 3200]\n",
            "loss: 1.001555  [   35/ 3200]\n",
            "loss: 0.999352  [   36/ 3200]\n",
            "loss: 0.869160  [   37/ 3200]\n",
            "loss: 1.112786  [   38/ 3200]\n",
            "loss: 0.757953  [   39/ 3200]\n",
            "loss: 0.667452  [   40/ 3200]\n",
            "loss: 1.129237  [   41/ 3200]\n",
            "loss: 0.940171  [   42/ 3200]\n",
            "loss: 1.330343  [   43/ 3200]\n",
            "loss: 1.358472  [   44/ 3200]\n",
            "loss: 0.783288  [   45/ 3200]\n",
            "loss: 0.971715  [   46/ 3200]\n",
            "loss: 1.150096  [   47/ 3200]\n",
            "loss: 1.031044  [   48/ 3200]\n",
            "loss: 1.058606  [   49/ 3200]\n",
            "loss: 0.935611  [   50/ 3200]\n",
            "loss: 0.917766  [   51/ 3200]\n",
            "loss: 1.158622  [   52/ 3200]\n",
            "loss: 0.735311  [   53/ 3200]\n",
            "loss: 0.979691  [   54/ 3200]\n",
            "loss: 1.073461  [   55/ 3200]\n",
            "loss: 1.056377  [   56/ 3200]\n",
            "loss: 0.774707  [   57/ 3200]\n",
            "loss: 0.759540  [   58/ 3200]\n",
            "loss: 0.663495  [   59/ 3200]\n",
            "loss: 0.665519  [   60/ 3200]\n",
            "loss: 0.937959  [   61/ 3200]\n",
            "loss: 0.970634  [   62/ 3200]\n",
            "loss: 0.840485  [   63/ 3200]\n",
            "loss: 0.943682  [   64/ 3200]\n",
            "loss: 1.053419  [   65/ 3200]\n",
            "loss: 1.052438  [   66/ 3200]\n",
            "loss: 0.965614  [   67/ 3200]\n",
            "loss: 1.165859  [   68/ 3200]\n",
            "loss: 1.023193  [   69/ 3200]\n",
            "loss: 0.798253  [   70/ 3200]\n",
            "loss: 0.815359  [   71/ 3200]\n",
            "loss: 1.172651  [   72/ 3200]\n",
            "loss: 0.953490  [   73/ 3200]\n",
            "loss: 0.858626  [   74/ 3200]\n",
            "loss: 0.870691  [   75/ 3200]\n",
            "loss: 0.731465  [   76/ 3200]\n",
            "loss: 0.795601  [   77/ 3200]\n",
            "loss: 0.977178  [   78/ 3200]\n",
            "loss: 1.381459  [   79/ 3200]\n",
            "loss: 1.205301  [   80/ 3200]\n",
            "loss: 0.963627  [   81/ 3200]\n",
            "loss: 0.876157  [   82/ 3200]\n",
            "loss: 0.928833  [   83/ 3200]\n",
            "loss: 1.098713  [   84/ 3200]\n",
            "loss: 1.066349  [   85/ 3200]\n",
            "loss: 0.672616  [   86/ 3200]\n",
            "loss: 0.884969  [   87/ 3200]\n",
            "loss: 0.810063  [   88/ 3200]\n",
            "loss: 0.875143  [   89/ 3200]\n",
            "loss: 1.030372  [   90/ 3200]\n",
            "loss: 0.814694  [   91/ 3200]\n",
            "loss: 0.684722  [   92/ 3200]\n",
            "loss: 0.949909  [   93/ 3200]\n",
            "loss: 0.742407  [   94/ 3200]\n",
            "loss: 0.736990  [   95/ 3200]\n",
            "loss: 0.689297  [   96/ 3200]\n",
            "loss: 0.637609  [   97/ 3200]\n",
            "loss: 0.835243  [   98/ 3200]\n",
            "loss: 0.642843  [   99/ 3200]\n",
            "loss: 1.001870  [  100/ 3200]\n",
            "loss: 0.989852  [  101/ 3200]\n",
            "loss: 0.744052  [  102/ 3200]\n",
            "loss: 1.259162  [  103/ 3200]\n",
            "loss: 0.854321  [  104/ 3200]\n",
            "loss: 1.001978  [  105/ 3200]\n",
            "loss: 0.777445  [  106/ 3200]\n",
            "loss: 0.846221  [  107/ 3200]\n",
            "loss: 1.065194  [  108/ 3200]\n",
            "loss: 0.855044  [  109/ 3200]\n",
            "loss: 1.102236  [  110/ 3200]\n",
            "loss: 1.181004  [  111/ 3200]\n",
            "loss: 0.695709  [  112/ 3200]\n",
            "loss: 0.762431  [  113/ 3200]\n",
            "loss: 1.066742  [  114/ 3200]\n",
            "loss: 0.653996  [  115/ 3200]\n",
            "loss: 1.170898  [  116/ 3200]\n",
            "loss: 0.871672  [  117/ 3200]\n",
            "loss: 1.008579  [  118/ 3200]\n",
            "loss: 0.910178  [  119/ 3200]\n",
            "loss: 0.493562  [  120/ 3200]\n",
            "loss: 0.953086  [  121/ 3200]\n",
            "loss: 1.062303  [  122/ 3200]\n",
            "loss: 1.006349  [  123/ 3200]\n",
            "loss: 0.854793  [  124/ 3200]\n",
            "loss: 0.912441  [  125/ 3200]\n",
            "loss: 1.049335  [  126/ 3200]\n",
            "loss: 0.805988  [  127/ 3200]\n",
            "loss: 1.051304  [  128/ 3200]\n",
            "loss: 0.812519  [  129/ 3200]\n",
            "loss: 0.771508  [  130/ 3200]\n",
            "loss: 1.039024  [  131/ 3200]\n",
            "loss: 0.814130  [  132/ 3200]\n",
            "loss: 1.386013  [  133/ 3200]\n",
            "loss: 1.085682  [  134/ 3200]\n",
            "loss: 0.814137  [  135/ 3200]\n",
            "loss: 1.185072  [  136/ 3200]\n",
            "loss: 0.970151  [  137/ 3200]\n",
            "loss: 0.684922  [  138/ 3200]\n",
            "loss: 0.807476  [  139/ 3200]\n",
            "loss: 0.974198  [  140/ 3200]\n",
            "loss: 0.706272  [  141/ 3200]\n",
            "loss: 0.750490  [  142/ 3200]\n",
            "loss: 1.264805  [  143/ 3200]\n",
            "loss: 1.093009  [  144/ 3200]\n",
            "loss: 0.823418  [  145/ 3200]\n",
            "loss: 0.803791  [  146/ 3200]\n",
            "loss: 0.838096  [  147/ 3200]\n",
            "loss: 0.847268  [  148/ 3200]\n",
            "loss: 0.841215  [  149/ 3200]\n",
            "loss: 0.877850  [  150/ 3200]\n",
            "loss: 1.008697  [  151/ 3200]\n",
            "loss: 0.741950  [  152/ 3200]\n",
            "loss: 0.893583  [  153/ 3200]\n",
            "loss: 1.054908  [  154/ 3200]\n",
            "loss: 0.827563  [  155/ 3200]\n",
            "loss: 0.656819  [  156/ 3200]\n",
            "loss: 0.833075  [  157/ 3200]\n",
            "loss: 1.239161  [  158/ 3200]\n",
            "loss: 1.116879  [  159/ 3200]\n",
            "loss: 1.066371  [  160/ 3200]\n",
            "loss: 0.864891  [  161/ 3200]\n",
            "loss: 0.865824  [  162/ 3200]\n",
            "loss: 0.786713  [  163/ 3200]\n",
            "loss: 1.010238  [  164/ 3200]\n",
            "loss: 0.880321  [  165/ 3200]\n",
            "loss: 0.683179  [  166/ 3200]\n",
            "loss: 0.954273  [  167/ 3200]\n",
            "loss: 0.823781  [  168/ 3200]\n",
            "loss: 1.027968  [  169/ 3200]\n",
            "loss: 1.042849  [  170/ 3200]\n",
            "loss: 0.751717  [  171/ 3200]\n",
            "loss: 0.673912  [  172/ 3200]\n",
            "loss: 0.584208  [  173/ 3200]\n",
            "loss: 0.630023  [  174/ 3200]\n",
            "loss: 0.746274  [  175/ 3200]\n",
            "loss: 0.820440  [  176/ 3200]\n",
            "loss: 0.891229  [  177/ 3200]\n",
            "loss: 0.754804  [  178/ 3200]\n",
            "loss: 1.088575  [  179/ 3200]\n",
            "loss: 1.097738  [  180/ 3200]\n",
            "loss: 1.212260  [  181/ 3200]\n",
            "loss: 0.917374  [  182/ 3200]\n",
            "loss: 0.712065  [  183/ 3200]\n",
            "loss: 0.984253  [  184/ 3200]\n",
            "loss: 0.726457  [  185/ 3200]\n",
            "loss: 0.776575  [  186/ 3200]\n",
            "loss: 0.657563  [  187/ 3200]\n",
            "loss: 0.853424  [  188/ 3200]\n",
            "loss: 0.970318  [  189/ 3200]\n",
            "loss: 0.601779  [  190/ 3200]\n",
            "loss: 0.852903  [  191/ 3200]\n",
            "loss: 0.882854  [  192/ 3200]\n",
            "loss: 0.848046  [  193/ 3200]\n",
            "loss: 1.338471  [  194/ 3200]\n",
            "loss: 0.802161  [  195/ 3200]\n",
            "loss: 1.201092  [  196/ 3200]\n",
            "loss: 0.660975  [  197/ 3200]\n",
            "loss: 0.794566  [  198/ 3200]\n",
            "loss: 1.022008  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 9\n",
            "Validation Loss: 0.8337\n",
            "Validation F1: 0.6520\n",
            "Validation Accuracy: 0.6775\n",
            "Validation Confusion Matrix:\n",
            "[[ 53  52  69  26]\n",
            " [  5 184   3   8]\n",
            " [ 28   6 148  18]\n",
            " [ 14  17  12 157]]\n",
            "Epoch:  1\n",
            "loss: 0.958008  [    0/ 3200]\n",
            "loss: 1.177469  [    1/ 3200]\n",
            "loss: 1.066617  [    2/ 3200]\n",
            "loss: 0.906023  [    3/ 3200]\n",
            "loss: 0.930214  [    4/ 3200]\n",
            "loss: 0.680098  [    5/ 3200]\n",
            "loss: 0.870789  [    6/ 3200]\n",
            "loss: 1.126251  [    7/ 3200]\n",
            "loss: 1.032717  [    8/ 3200]\n",
            "loss: 1.209792  [    9/ 3200]\n",
            "loss: 0.893815  [   10/ 3200]\n",
            "loss: 0.964782  [   11/ 3200]\n",
            "loss: 0.855132  [   12/ 3200]\n",
            "loss: 1.446708  [   13/ 3200]\n",
            "loss: 0.626180  [   14/ 3200]\n",
            "loss: 0.824294  [   15/ 3200]\n",
            "loss: 0.919096  [   16/ 3200]\n",
            "loss: 1.231550  [   17/ 3200]\n",
            "loss: 0.955175  [   18/ 3200]\n",
            "loss: 0.724195  [   19/ 3200]\n",
            "loss: 1.027709  [   20/ 3200]\n",
            "loss: 0.741818  [   21/ 3200]\n",
            "loss: 0.701368  [   22/ 3200]\n",
            "loss: 1.010087  [   23/ 3200]\n",
            "loss: 0.961588  [   24/ 3200]\n",
            "loss: 0.938000  [   25/ 3200]\n",
            "loss: 0.844187  [   26/ 3200]\n",
            "loss: 1.035548  [   27/ 3200]\n",
            "loss: 0.880103  [   28/ 3200]\n",
            "loss: 0.704367  [   29/ 3200]\n",
            "loss: 0.865097  [   30/ 3200]\n",
            "loss: 0.737189  [   31/ 3200]\n",
            "loss: 1.238956  [   32/ 3200]\n",
            "loss: 0.883411  [   33/ 3200]\n",
            "loss: 1.239794  [   34/ 3200]\n",
            "loss: 0.815468  [   35/ 3200]\n",
            "loss: 0.830999  [   36/ 3200]\n",
            "loss: 0.764697  [   37/ 3200]\n",
            "loss: 0.835052  [   38/ 3200]\n",
            "loss: 0.850040  [   39/ 3200]\n",
            "loss: 0.874692  [   40/ 3200]\n",
            "loss: 0.693476  [   41/ 3200]\n",
            "loss: 1.173845  [   42/ 3200]\n",
            "loss: 0.917567  [   43/ 3200]\n",
            "loss: 0.700141  [   44/ 3200]\n",
            "loss: 1.047855  [   45/ 3200]\n",
            "loss: 0.981396  [   46/ 3200]\n",
            "loss: 1.130680  [   47/ 3200]\n",
            "loss: 0.696054  [   48/ 3200]\n",
            "loss: 1.045717  [   49/ 3200]\n",
            "loss: 0.941178  [   50/ 3200]\n",
            "loss: 0.634895  [   51/ 3200]\n",
            "loss: 0.788373  [   52/ 3200]\n",
            "loss: 0.726494  [   53/ 3200]\n",
            "loss: 0.601230  [   54/ 3200]\n",
            "loss: 0.851618  [   55/ 3200]\n",
            "loss: 1.216528  [   56/ 3200]\n",
            "loss: 1.006700  [   57/ 3200]\n",
            "loss: 1.002288  [   58/ 3200]\n",
            "loss: 0.944106  [   59/ 3200]\n",
            "loss: 1.196305  [   60/ 3200]\n",
            "loss: 1.095543  [   61/ 3200]\n",
            "loss: 0.886588  [   62/ 3200]\n",
            "loss: 1.164036  [   63/ 3200]\n",
            "loss: 0.559489  [   64/ 3200]\n",
            "loss: 0.723332  [   65/ 3200]\n",
            "loss: 0.612064  [   66/ 3200]\n",
            "loss: 0.854974  [   67/ 3200]\n",
            "loss: 0.860716  [   68/ 3200]\n",
            "loss: 0.850248  [   69/ 3200]\n",
            "loss: 1.048807  [   70/ 3200]\n",
            "loss: 0.990124  [   71/ 3200]\n",
            "loss: 1.051564  [   72/ 3200]\n",
            "loss: 0.638471  [   73/ 3200]\n",
            "loss: 0.957691  [   74/ 3200]\n",
            "loss: 0.899997  [   75/ 3200]\n",
            "loss: 1.042989  [   76/ 3200]\n",
            "loss: 1.022998  [   77/ 3200]\n",
            "loss: 0.965196  [   78/ 3200]\n",
            "loss: 0.752882  [   79/ 3200]\n",
            "loss: 0.813579  [   80/ 3200]\n",
            "loss: 0.830624  [   81/ 3200]\n",
            "loss: 1.090441  [   82/ 3200]\n",
            "loss: 1.199980  [   83/ 3200]\n",
            "loss: 0.584637  [   84/ 3200]\n",
            "loss: 0.882266  [   85/ 3200]\n",
            "loss: 1.059236  [   86/ 3200]\n",
            "loss: 1.021379  [   87/ 3200]\n",
            "loss: 1.114802  [   88/ 3200]\n",
            "loss: 0.770841  [   89/ 3200]\n",
            "loss: 0.853779  [   90/ 3200]\n",
            "loss: 0.884265  [   91/ 3200]\n",
            "loss: 0.905068  [   92/ 3200]\n",
            "loss: 0.648325  [   93/ 3200]\n",
            "loss: 0.789993  [   94/ 3200]\n",
            "loss: 1.064701  [   95/ 3200]\n",
            "loss: 1.385509  [   96/ 3200]\n",
            "loss: 0.853599  [   97/ 3200]\n",
            "loss: 0.746510  [   98/ 3200]\n",
            "loss: 1.042923  [   99/ 3200]\n",
            "loss: 1.247356  [  100/ 3200]\n",
            "loss: 0.872169  [  101/ 3200]\n",
            "loss: 0.798730  [  102/ 3200]\n",
            "loss: 0.670068  [  103/ 3200]\n",
            "loss: 0.784461  [  104/ 3200]\n",
            "loss: 0.795733  [  105/ 3200]\n",
            "loss: 0.787719  [  106/ 3200]\n",
            "loss: 0.719021  [  107/ 3200]\n",
            "loss: 0.842567  [  108/ 3200]\n",
            "loss: 1.108086  [  109/ 3200]\n",
            "loss: 0.996920  [  110/ 3200]\n",
            "loss: 0.963659  [  111/ 3200]\n",
            "loss: 1.464143  [  112/ 3200]\n",
            "loss: 1.161942  [  113/ 3200]\n",
            "loss: 0.829434  [  114/ 3200]\n",
            "loss: 0.920968  [  115/ 3200]\n",
            "loss: 0.779963  [  116/ 3200]\n",
            "loss: 1.012823  [  117/ 3200]\n",
            "loss: 1.067223  [  118/ 3200]\n",
            "loss: 0.865099  [  119/ 3200]\n",
            "loss: 0.881479  [  120/ 3200]\n",
            "loss: 0.774665  [  121/ 3200]\n",
            "loss: 0.614285  [  122/ 3200]\n",
            "loss: 1.210487  [  123/ 3200]\n",
            "loss: 1.120361  [  124/ 3200]\n",
            "loss: 1.014546  [  125/ 3200]\n",
            "loss: 1.008827  [  126/ 3200]\n",
            "loss: 1.022169  [  127/ 3200]\n",
            "loss: 0.880031  [  128/ 3200]\n",
            "loss: 0.721318  [  129/ 3200]\n",
            "loss: 1.099213  [  130/ 3200]\n",
            "loss: 0.807561  [  131/ 3200]\n",
            "loss: 0.774219  [  132/ 3200]\n",
            "loss: 0.869652  [  133/ 3200]\n",
            "loss: 0.865568  [  134/ 3200]\n",
            "loss: 0.821721  [  135/ 3200]\n",
            "loss: 0.902961  [  136/ 3200]\n",
            "loss: 1.030249  [  137/ 3200]\n",
            "loss: 0.749671  [  138/ 3200]\n",
            "loss: 0.931356  [  139/ 3200]\n",
            "loss: 0.809306  [  140/ 3200]\n",
            "loss: 0.804269  [  141/ 3200]\n",
            "loss: 0.554168  [  142/ 3200]\n",
            "loss: 1.092644  [  143/ 3200]\n",
            "loss: 0.876159  [  144/ 3200]\n",
            "loss: 0.929487  [  145/ 3200]\n",
            "loss: 0.727980  [  146/ 3200]\n",
            "loss: 1.104728  [  147/ 3200]\n",
            "loss: 0.827550  [  148/ 3200]\n",
            "loss: 0.743325  [  149/ 3200]\n",
            "loss: 0.733967  [  150/ 3200]\n",
            "loss: 0.852648  [  151/ 3200]\n",
            "loss: 1.305173  [  152/ 3200]\n",
            "loss: 1.144322  [  153/ 3200]\n",
            "loss: 0.669394  [  154/ 3200]\n",
            "loss: 1.305695  [  155/ 3200]\n",
            "loss: 1.208784  [  156/ 3200]\n",
            "loss: 0.758159  [  157/ 3200]\n",
            "loss: 0.995853  [  158/ 3200]\n",
            "loss: 0.979906  [  159/ 3200]\n",
            "loss: 1.099929  [  160/ 3200]\n",
            "loss: 0.807970  [  161/ 3200]\n",
            "loss: 1.112506  [  162/ 3200]\n",
            "loss: 1.199080  [  163/ 3200]\n",
            "loss: 0.971608  [  164/ 3200]\n",
            "loss: 0.614263  [  165/ 3200]\n",
            "loss: 0.898716  [  166/ 3200]\n",
            "loss: 0.983139  [  167/ 3200]\n",
            "loss: 0.617753  [  168/ 3200]\n",
            "loss: 0.650957  [  169/ 3200]\n",
            "loss: 0.847412  [  170/ 3200]\n",
            "loss: 0.524723  [  171/ 3200]\n",
            "loss: 0.835346  [  172/ 3200]\n",
            "loss: 0.908009  [  173/ 3200]\n",
            "loss: 1.026711  [  174/ 3200]\n",
            "loss: 1.027524  [  175/ 3200]\n",
            "loss: 1.135140  [  176/ 3200]\n",
            "loss: 0.938263  [  177/ 3200]\n",
            "loss: 0.719124  [  178/ 3200]\n",
            "loss: 0.512323  [  179/ 3200]\n",
            "loss: 0.776655  [  180/ 3200]\n",
            "loss: 0.815300  [  181/ 3200]\n",
            "loss: 1.042485  [  182/ 3200]\n",
            "loss: 0.787866  [  183/ 3200]\n",
            "loss: 0.604771  [  184/ 3200]\n",
            "loss: 0.947996  [  185/ 3200]\n",
            "loss: 1.225890  [  186/ 3200]\n",
            "loss: 0.761069  [  187/ 3200]\n",
            "loss: 1.001456  [  188/ 3200]\n",
            "loss: 1.123211  [  189/ 3200]\n",
            "loss: 0.895624  [  190/ 3200]\n",
            "loss: 0.600773  [  191/ 3200]\n",
            "loss: 0.769950  [  192/ 3200]\n",
            "loss: 0.882822  [  193/ 3200]\n",
            "loss: 1.056009  [  194/ 3200]\n",
            "loss: 0.913289  [  195/ 3200]\n",
            "loss: 0.632107  [  196/ 3200]\n",
            "loss: 1.101415  [  197/ 3200]\n",
            "loss: 1.021063  [  198/ 3200]\n",
            "loss: 0.823811  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 10\n",
            "Validation Loss: 0.8659\n",
            "Validation F1: 0.5696\n",
            "Validation Accuracy: 0.6075\n",
            "Validation Confusion Matrix:\n",
            "[[ 30  24 101  45]\n",
            " [ 31 115   8  46]\n",
            " [ 10   1 165  24]\n",
            " [  4   2  18 176]]\n",
            "Epoch:  1\n",
            "loss: 0.859521  [    0/ 3200]\n",
            "loss: 0.945165  [    1/ 3200]\n",
            "loss: 0.924837  [    2/ 3200]\n",
            "loss: 1.064947  [    3/ 3200]\n",
            "loss: 0.818463  [    4/ 3200]\n",
            "loss: 1.258332  [    5/ 3200]\n",
            "loss: 0.810577  [    6/ 3200]\n",
            "loss: 0.880001  [    7/ 3200]\n",
            "loss: 0.885866  [    8/ 3200]\n",
            "loss: 0.731796  [    9/ 3200]\n",
            "loss: 0.978811  [   10/ 3200]\n",
            "loss: 0.957459  [   11/ 3200]\n",
            "loss: 0.871106  [   12/ 3200]\n",
            "loss: 0.978339  [   13/ 3200]\n",
            "loss: 1.240709  [   14/ 3200]\n",
            "loss: 0.878638  [   15/ 3200]\n",
            "loss: 0.835596  [   16/ 3200]\n",
            "loss: 1.152863  [   17/ 3200]\n",
            "loss: 0.901391  [   18/ 3200]\n",
            "loss: 0.906494  [   19/ 3200]\n",
            "loss: 1.034217  [   20/ 3200]\n",
            "loss: 1.075592  [   21/ 3200]\n",
            "loss: 0.600767  [   22/ 3200]\n",
            "loss: 0.878773  [   23/ 3200]\n",
            "loss: 1.006176  [   24/ 3200]\n",
            "loss: 1.021080  [   25/ 3200]\n",
            "loss: 0.743603  [   26/ 3200]\n",
            "loss: 1.153898  [   27/ 3200]\n",
            "loss: 0.694011  [   28/ 3200]\n",
            "loss: 0.717709  [   29/ 3200]\n",
            "loss: 1.369878  [   30/ 3200]\n",
            "loss: 1.304646  [   31/ 3200]\n",
            "loss: 0.680186  [   32/ 3200]\n",
            "loss: 0.832902  [   33/ 3200]\n",
            "loss: 0.991989  [   34/ 3200]\n",
            "loss: 1.316025  [   35/ 3200]\n",
            "loss: 0.813709  [   36/ 3200]\n",
            "loss: 0.796632  [   37/ 3200]\n",
            "loss: 0.813696  [   38/ 3200]\n",
            "loss: 0.500083  [   39/ 3200]\n",
            "loss: 0.880650  [   40/ 3200]\n",
            "loss: 0.809143  [   41/ 3200]\n",
            "loss: 0.713083  [   42/ 3200]\n",
            "loss: 0.792126  [   43/ 3200]\n",
            "loss: 0.740597  [   44/ 3200]\n",
            "loss: 1.044663  [   45/ 3200]\n",
            "loss: 0.734183  [   46/ 3200]\n",
            "loss: 1.236363  [   47/ 3200]\n",
            "loss: 1.063753  [   48/ 3200]\n",
            "loss: 0.843057  [   49/ 3200]\n",
            "loss: 1.019215  [   50/ 3200]\n",
            "loss: 0.793041  [   51/ 3200]\n",
            "loss: 0.760691  [   52/ 3200]\n",
            "loss: 0.803061  [   53/ 3200]\n",
            "loss: 0.538659  [   54/ 3200]\n",
            "loss: 0.782682  [   55/ 3200]\n",
            "loss: 0.900517  [   56/ 3200]\n",
            "loss: 0.589989  [   57/ 3200]\n",
            "loss: 0.808686  [   58/ 3200]\n",
            "loss: 0.966294  [   59/ 3200]\n",
            "loss: 0.731435  [   60/ 3200]\n",
            "loss: 0.986214  [   61/ 3200]\n",
            "loss: 0.790324  [   62/ 3200]\n",
            "loss: 1.009222  [   63/ 3200]\n",
            "loss: 0.767496  [   64/ 3200]\n",
            "loss: 1.026885  [   65/ 3200]\n",
            "loss: 0.712638  [   66/ 3200]\n",
            "loss: 1.088434  [   67/ 3200]\n",
            "loss: 0.966416  [   68/ 3200]\n",
            "loss: 0.857272  [   69/ 3200]\n",
            "loss: 0.909886  [   70/ 3200]\n",
            "loss: 1.111847  [   71/ 3200]\n",
            "loss: 1.045877  [   72/ 3200]\n",
            "loss: 0.702179  [   73/ 3200]\n",
            "loss: 1.404922  [   74/ 3200]\n",
            "loss: 0.839407  [   75/ 3200]\n",
            "loss: 0.988143  [   76/ 3200]\n",
            "loss: 0.849309  [   77/ 3200]\n",
            "loss: 1.008393  [   78/ 3200]\n",
            "loss: 1.240389  [   79/ 3200]\n",
            "loss: 0.717973  [   80/ 3200]\n",
            "loss: 1.246267  [   81/ 3200]\n",
            "loss: 0.941523  [   82/ 3200]\n",
            "loss: 0.958051  [   83/ 3200]\n",
            "loss: 0.680905  [   84/ 3200]\n",
            "loss: 1.169134  [   85/ 3200]\n",
            "loss: 1.200846  [   86/ 3200]\n",
            "loss: 0.903360  [   87/ 3200]\n",
            "loss: 1.114312  [   88/ 3200]\n",
            "loss: 0.807246  [   89/ 3200]\n",
            "loss: 0.899793  [   90/ 3200]\n",
            "loss: 0.657282  [   91/ 3200]\n",
            "loss: 1.131822  [   92/ 3200]\n",
            "loss: 1.278729  [   93/ 3200]\n",
            "loss: 1.020666  [   94/ 3200]\n",
            "loss: 0.951427  [   95/ 3200]\n",
            "loss: 0.784868  [   96/ 3200]\n",
            "loss: 0.883342  [   97/ 3200]\n",
            "loss: 0.862988  [   98/ 3200]\n",
            "loss: 0.961013  [   99/ 3200]\n",
            "loss: 0.898789  [  100/ 3200]\n",
            "loss: 0.814368  [  101/ 3200]\n",
            "loss: 1.098313  [  102/ 3200]\n",
            "loss: 0.687298  [  103/ 3200]\n",
            "loss: 1.014222  [  104/ 3200]\n",
            "loss: 0.877802  [  105/ 3200]\n",
            "loss: 0.708542  [  106/ 3200]\n",
            "loss: 1.309284  [  107/ 3200]\n",
            "loss: 0.939971  [  108/ 3200]\n",
            "loss: 0.871148  [  109/ 3200]\n",
            "loss: 1.052625  [  110/ 3200]\n",
            "loss: 0.551963  [  111/ 3200]\n",
            "loss: 0.957200  [  112/ 3200]\n",
            "loss: 0.641755  [  113/ 3200]\n",
            "loss: 0.813456  [  114/ 3200]\n",
            "loss: 0.804190  [  115/ 3200]\n",
            "loss: 0.842316  [  116/ 3200]\n",
            "loss: 0.694230  [  117/ 3200]\n",
            "loss: 1.043041  [  118/ 3200]\n",
            "loss: 1.028495  [  119/ 3200]\n",
            "loss: 0.846494  [  120/ 3200]\n",
            "loss: 0.686301  [  121/ 3200]\n",
            "loss: 0.820539  [  122/ 3200]\n",
            "loss: 0.934274  [  123/ 3200]\n",
            "loss: 0.782920  [  124/ 3200]\n",
            "loss: 1.019840  [  125/ 3200]\n",
            "loss: 0.707785  [  126/ 3200]\n",
            "loss: 0.856983  [  127/ 3200]\n",
            "loss: 1.310631  [  128/ 3200]\n",
            "loss: 0.889823  [  129/ 3200]\n",
            "loss: 1.067988  [  130/ 3200]\n",
            "loss: 1.101926  [  131/ 3200]\n",
            "loss: 0.959501  [  132/ 3200]\n",
            "loss: 0.786194  [  133/ 3200]\n",
            "loss: 0.595232  [  134/ 3200]\n",
            "loss: 1.086252  [  135/ 3200]\n",
            "loss: 0.824374  [  136/ 3200]\n",
            "loss: 0.761186  [  137/ 3200]\n",
            "loss: 0.967966  [  138/ 3200]\n",
            "loss: 0.816212  [  139/ 3200]\n",
            "loss: 1.238080  [  140/ 3200]\n",
            "loss: 0.995028  [  141/ 3200]\n",
            "loss: 0.790984  [  142/ 3200]\n",
            "loss: 0.748351  [  143/ 3200]\n",
            "loss: 0.783620  [  144/ 3200]\n",
            "loss: 1.167028  [  145/ 3200]\n",
            "loss: 0.794288  [  146/ 3200]\n",
            "loss: 0.868061  [  147/ 3200]\n",
            "loss: 0.830878  [  148/ 3200]\n",
            "loss: 0.727508  [  149/ 3200]\n",
            "loss: 0.723534  [  150/ 3200]\n",
            "loss: 1.272438  [  151/ 3200]\n",
            "loss: 1.033327  [  152/ 3200]\n",
            "loss: 0.890325  [  153/ 3200]\n",
            "loss: 1.011926  [  154/ 3200]\n",
            "loss: 1.200963  [  155/ 3200]\n",
            "loss: 1.023503  [  156/ 3200]\n",
            "loss: 0.984736  [  157/ 3200]\n",
            "loss: 0.950884  [  158/ 3200]\n",
            "loss: 1.345401  [  159/ 3200]\n",
            "loss: 0.944898  [  160/ 3200]\n",
            "loss: 0.970538  [  161/ 3200]\n",
            "loss: 0.566472  [  162/ 3200]\n",
            "loss: 0.893388  [  163/ 3200]\n",
            "loss: 0.780488  [  164/ 3200]\n",
            "loss: 0.809002  [  165/ 3200]\n",
            "loss: 0.666022  [  166/ 3200]\n",
            "loss: 0.689833  [  167/ 3200]\n",
            "loss: 0.971864  [  168/ 3200]\n",
            "loss: 0.797875  [  169/ 3200]\n",
            "loss: 1.028970  [  170/ 3200]\n",
            "loss: 0.974750  [  171/ 3200]\n",
            "loss: 0.963160  [  172/ 3200]\n",
            "loss: 0.883599  [  173/ 3200]\n",
            "loss: 0.945993  [  174/ 3200]\n",
            "loss: 0.810275  [  175/ 3200]\n",
            "loss: 0.868773  [  176/ 3200]\n",
            "loss: 1.199120  [  177/ 3200]\n",
            "loss: 0.836204  [  178/ 3200]\n",
            "loss: 0.836191  [  179/ 3200]\n",
            "loss: 0.933892  [  180/ 3200]\n",
            "loss: 0.894165  [  181/ 3200]\n",
            "loss: 0.776523  [  182/ 3200]\n",
            "loss: 1.107774  [  183/ 3200]\n",
            "loss: 0.950997  [  184/ 3200]\n",
            "loss: 0.691891  [  185/ 3200]\n",
            "loss: 0.804671  [  186/ 3200]\n",
            "loss: 0.965802  [  187/ 3200]\n",
            "loss: 0.919383  [  188/ 3200]\n",
            "loss: 0.763690  [  189/ 3200]\n",
            "loss: 0.838925  [  190/ 3200]\n",
            "loss: 0.570256  [  191/ 3200]\n",
            "loss: 0.670798  [  192/ 3200]\n",
            "loss: 0.973602  [  193/ 3200]\n",
            "loss: 0.735512  [  194/ 3200]\n",
            "loss: 0.756072  [  195/ 3200]\n",
            "loss: 0.779956  [  196/ 3200]\n",
            "loss: 0.863336  [  197/ 3200]\n",
            "loss: 1.023746  [  198/ 3200]\n",
            "loss: 0.585937  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 11\n",
            "Validation Loss: 0.8367\n",
            "Validation F1: 0.6631\n",
            "Validation Accuracy: 0.6800\n",
            "Validation Confusion Matrix:\n",
            "[[ 72  50  31  47]\n",
            " [  4 177   3  16]\n",
            " [ 39   6 123  32]\n",
            " [  9  13   6 172]]\n",
            "Epoch:  1\n",
            "loss: 1.277179  [    0/ 3200]\n",
            "loss: 0.828320  [    1/ 3200]\n",
            "loss: 0.730129  [    2/ 3200]\n",
            "loss: 0.800676  [    3/ 3200]\n",
            "loss: 0.676414  [    4/ 3200]\n",
            "loss: 0.846577  [    5/ 3200]\n",
            "loss: 0.993084  [    6/ 3200]\n",
            "loss: 0.917163  [    7/ 3200]\n",
            "loss: 0.939569  [    8/ 3200]\n",
            "loss: 0.684145  [    9/ 3200]\n",
            "loss: 1.041134  [   10/ 3200]\n",
            "loss: 0.816836  [   11/ 3200]\n",
            "loss: 0.752421  [   12/ 3200]\n",
            "loss: 1.253373  [   13/ 3200]\n",
            "loss: 0.878894  [   14/ 3200]\n",
            "loss: 0.720474  [   15/ 3200]\n",
            "loss: 0.871904  [   16/ 3200]\n",
            "loss: 0.880107  [   17/ 3200]\n",
            "loss: 0.950977  [   18/ 3200]\n",
            "loss: 0.951563  [   19/ 3200]\n",
            "loss: 1.259295  [   20/ 3200]\n",
            "loss: 1.013919  [   21/ 3200]\n",
            "loss: 0.696245  [   22/ 3200]\n",
            "loss: 0.936506  [   23/ 3200]\n",
            "loss: 0.906354  [   24/ 3200]\n",
            "loss: 1.063377  [   25/ 3200]\n",
            "loss: 0.949537  [   26/ 3200]\n",
            "loss: 0.876353  [   27/ 3200]\n",
            "loss: 1.311069  [   28/ 3200]\n",
            "loss: 1.097383  [   29/ 3200]\n",
            "loss: 0.607810  [   30/ 3200]\n",
            "loss: 1.087100  [   31/ 3200]\n",
            "loss: 0.818685  [   32/ 3200]\n",
            "loss: 0.867878  [   33/ 3200]\n",
            "loss: 1.248739  [   34/ 3200]\n",
            "loss: 1.039268  [   35/ 3200]\n",
            "loss: 0.936082  [   36/ 3200]\n",
            "loss: 0.704003  [   37/ 3200]\n",
            "loss: 0.922544  [   38/ 3200]\n",
            "loss: 0.940381  [   39/ 3200]\n",
            "loss: 0.813056  [   40/ 3200]\n",
            "loss: 0.929093  [   41/ 3200]\n",
            "loss: 0.896111  [   42/ 3200]\n",
            "loss: 0.835629  [   43/ 3200]\n",
            "loss: 1.359291  [   44/ 3200]\n",
            "loss: 0.739901  [   45/ 3200]\n",
            "loss: 0.399826  [   46/ 3200]\n",
            "loss: 1.037583  [   47/ 3200]\n",
            "loss: 0.577641  [   48/ 3200]\n",
            "loss: 1.037527  [   49/ 3200]\n",
            "loss: 0.920805  [   50/ 3200]\n",
            "loss: 0.729991  [   51/ 3200]\n",
            "loss: 1.522986  [   52/ 3200]\n",
            "loss: 0.989087  [   53/ 3200]\n",
            "loss: 0.929305  [   54/ 3200]\n",
            "loss: 0.825181  [   55/ 3200]\n",
            "loss: 1.114996  [   56/ 3200]\n",
            "loss: 1.087141  [   57/ 3200]\n",
            "loss: 0.852109  [   58/ 3200]\n",
            "loss: 1.070590  [   59/ 3200]\n",
            "loss: 0.601982  [   60/ 3200]\n",
            "loss: 1.161240  [   61/ 3200]\n",
            "loss: 1.341682  [   62/ 3200]\n",
            "loss: 1.012676  [   63/ 3200]\n",
            "loss: 0.758730  [   64/ 3200]\n",
            "loss: 0.912309  [   65/ 3200]\n",
            "loss: 1.043086  [   66/ 3200]\n",
            "loss: 0.866789  [   67/ 3200]\n",
            "loss: 0.689940  [   68/ 3200]\n",
            "loss: 0.838693  [   69/ 3200]\n",
            "loss: 0.959597  [   70/ 3200]\n",
            "loss: 0.672131  [   71/ 3200]\n",
            "loss: 1.011288  [   72/ 3200]\n",
            "loss: 0.862667  [   73/ 3200]\n",
            "loss: 1.085475  [   74/ 3200]\n",
            "loss: 0.864135  [   75/ 3200]\n",
            "loss: 0.888252  [   76/ 3200]\n",
            "loss: 0.799889  [   77/ 3200]\n",
            "loss: 0.730937  [   78/ 3200]\n",
            "loss: 0.690456  [   79/ 3200]\n",
            "loss: 0.637190  [   80/ 3200]\n",
            "loss: 0.813939  [   81/ 3200]\n",
            "loss: 1.034915  [   82/ 3200]\n",
            "loss: 0.870949  [   83/ 3200]\n",
            "loss: 0.715161  [   84/ 3200]\n",
            "loss: 0.743430  [   85/ 3200]\n",
            "loss: 0.982705  [   86/ 3200]\n",
            "loss: 0.951695  [   87/ 3200]\n",
            "loss: 1.035928  [   88/ 3200]\n",
            "loss: 0.863898  [   89/ 3200]\n",
            "loss: 0.603196  [   90/ 3200]\n",
            "loss: 0.925314  [   91/ 3200]\n",
            "loss: 0.866673  [   92/ 3200]\n",
            "loss: 1.073049  [   93/ 3200]\n",
            "loss: 0.876131  [   94/ 3200]\n",
            "loss: 0.913723  [   95/ 3200]\n",
            "loss: 0.790612  [   96/ 3200]\n",
            "loss: 0.698367  [   97/ 3200]\n",
            "loss: 0.990907  [   98/ 3200]\n",
            "loss: 0.814555  [   99/ 3200]\n",
            "loss: 1.073027  [  100/ 3200]\n",
            "loss: 0.988537  [  101/ 3200]\n",
            "loss: 1.096978  [  102/ 3200]\n",
            "loss: 0.836205  [  103/ 3200]\n",
            "loss: 0.834870  [  104/ 3200]\n",
            "loss: 0.791155  [  105/ 3200]\n",
            "loss: 0.801543  [  106/ 3200]\n",
            "loss: 1.012426  [  107/ 3200]\n",
            "loss: 1.313207  [  108/ 3200]\n",
            "loss: 0.986120  [  109/ 3200]\n",
            "loss: 0.932732  [  110/ 3200]\n",
            "loss: 0.807851  [  111/ 3200]\n",
            "loss: 0.743174  [  112/ 3200]\n",
            "loss: 0.875071  [  113/ 3200]\n",
            "loss: 1.139218  [  114/ 3200]\n",
            "loss: 0.873450  [  115/ 3200]\n",
            "loss: 1.006008  [  116/ 3200]\n",
            "loss: 0.712997  [  117/ 3200]\n",
            "loss: 0.673090  [  118/ 3200]\n",
            "loss: 0.668064  [  119/ 3200]\n",
            "loss: 0.666686  [  120/ 3200]\n",
            "loss: 0.966550  [  121/ 3200]\n",
            "loss: 1.108437  [  122/ 3200]\n",
            "loss: 0.896914  [  123/ 3200]\n",
            "loss: 0.934776  [  124/ 3200]\n",
            "loss: 0.579241  [  125/ 3200]\n",
            "loss: 0.940198  [  126/ 3200]\n",
            "loss: 1.006645  [  127/ 3200]\n",
            "loss: 0.826752  [  128/ 3200]\n",
            "loss: 0.772334  [  129/ 3200]\n",
            "loss: 0.587614  [  130/ 3200]\n",
            "loss: 0.910662  [  131/ 3200]\n",
            "loss: 0.931046  [  132/ 3200]\n",
            "loss: 0.863953  [  133/ 3200]\n",
            "loss: 0.873391  [  134/ 3200]\n",
            "loss: 0.727633  [  135/ 3200]\n",
            "loss: 0.750722  [  136/ 3200]\n",
            "loss: 1.103235  [  137/ 3200]\n",
            "loss: 0.651466  [  138/ 3200]\n",
            "loss: 0.857339  [  139/ 3200]\n",
            "loss: 1.071056  [  140/ 3200]\n",
            "loss: 1.175398  [  141/ 3200]\n",
            "loss: 0.864838  [  142/ 3200]\n",
            "loss: 0.842269  [  143/ 3200]\n",
            "loss: 1.129469  [  144/ 3200]\n",
            "loss: 0.914791  [  145/ 3200]\n",
            "loss: 0.870331  [  146/ 3200]\n",
            "loss: 0.794357  [  147/ 3200]\n",
            "loss: 0.823689  [  148/ 3200]\n",
            "loss: 0.678678  [  149/ 3200]\n",
            "loss: 1.063674  [  150/ 3200]\n",
            "loss: 1.255721  [  151/ 3200]\n",
            "loss: 1.148664  [  152/ 3200]\n",
            "loss: 1.110175  [  153/ 3200]\n",
            "loss: 0.697288  [  154/ 3200]\n",
            "loss: 0.835120  [  155/ 3200]\n",
            "loss: 0.844886  [  156/ 3200]\n",
            "loss: 0.838292  [  157/ 3200]\n",
            "loss: 0.867297  [  158/ 3200]\n",
            "loss: 0.974936  [  159/ 3200]\n",
            "loss: 0.922069  [  160/ 3200]\n",
            "loss: 1.076504  [  161/ 3200]\n",
            "loss: 0.850407  [  162/ 3200]\n",
            "loss: 1.052980  [  163/ 3200]\n",
            "loss: 1.104823  [  164/ 3200]\n",
            "loss: 1.060191  [  165/ 3200]\n",
            "loss: 0.912568  [  166/ 3200]\n",
            "loss: 0.787976  [  167/ 3200]\n",
            "loss: 0.796197  [  168/ 3200]\n",
            "loss: 0.873717  [  169/ 3200]\n",
            "loss: 1.205614  [  170/ 3200]\n",
            "loss: 0.867499  [  171/ 3200]\n",
            "loss: 1.270710  [  172/ 3200]\n",
            "loss: 0.841013  [  173/ 3200]\n",
            "loss: 1.302123  [  174/ 3200]\n",
            "loss: 1.051967  [  175/ 3200]\n",
            "loss: 1.057681  [  176/ 3200]\n",
            "loss: 0.864615  [  177/ 3200]\n",
            "loss: 0.532559  [  178/ 3200]\n",
            "loss: 1.144570  [  179/ 3200]\n",
            "loss: 0.817725  [  180/ 3200]\n",
            "loss: 0.761835  [  181/ 3200]\n",
            "loss: 0.646829  [  182/ 3200]\n",
            "loss: 0.902956  [  183/ 3200]\n",
            "loss: 0.862258  [  184/ 3200]\n",
            "loss: 0.792798  [  185/ 3200]\n",
            "loss: 0.815298  [  186/ 3200]\n",
            "loss: 0.996681  [  187/ 3200]\n",
            "loss: 0.650853  [  188/ 3200]\n",
            "loss: 0.972571  [  189/ 3200]\n",
            "loss: 0.838872  [  190/ 3200]\n",
            "loss: 0.775323  [  191/ 3200]\n",
            "loss: 0.834754  [  192/ 3200]\n",
            "loss: 0.712711  [  193/ 3200]\n",
            "loss: 0.687216  [  194/ 3200]\n",
            "loss: 0.758080  [  195/ 3200]\n",
            "loss: 0.678611  [  196/ 3200]\n",
            "loss: 1.297907  [  197/ 3200]\n",
            "loss: 1.029325  [  198/ 3200]\n",
            "loss: 1.114039  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 12\n",
            "Validation Loss: 0.8372\n",
            "Validation F1: 0.6650\n",
            "Validation Accuracy: 0.6800\n",
            "Validation Confusion Matrix:\n",
            "[[ 76  58  27  39]\n",
            " [  3 184   3  10]\n",
            " [ 48   7 120  25]\n",
            " [ 14  16   6 164]]\n",
            "Epoch:  1\n",
            "loss: 0.514414  [    0/ 3200]\n",
            "loss: 0.542288  [    1/ 3200]\n",
            "loss: 1.233221  [    2/ 3200]\n",
            "loss: 0.651521  [    3/ 3200]\n",
            "loss: 0.901634  [    4/ 3200]\n",
            "loss: 1.008143  [    5/ 3200]\n",
            "loss: 0.670808  [    6/ 3200]\n",
            "loss: 1.064447  [    7/ 3200]\n",
            "loss: 1.056454  [    8/ 3200]\n",
            "loss: 0.657977  [    9/ 3200]\n",
            "loss: 0.554170  [   10/ 3200]\n",
            "loss: 0.982623  [   11/ 3200]\n",
            "loss: 0.795172  [   12/ 3200]\n",
            "loss: 1.238432  [   13/ 3200]\n",
            "loss: 0.851769  [   14/ 3200]\n",
            "loss: 1.035150  [   15/ 3200]\n",
            "loss: 0.812874  [   16/ 3200]\n",
            "loss: 1.408514  [   17/ 3200]\n",
            "loss: 1.131905  [   18/ 3200]\n",
            "loss: 0.840952  [   19/ 3200]\n",
            "loss: 0.838092  [   20/ 3200]\n",
            "loss: 1.166355  [   21/ 3200]\n",
            "loss: 0.917930  [   22/ 3200]\n",
            "loss: 0.861708  [   23/ 3200]\n",
            "loss: 0.866733  [   24/ 3200]\n",
            "loss: 1.216597  [   25/ 3200]\n",
            "loss: 0.751014  [   26/ 3200]\n",
            "loss: 1.077117  [   27/ 3200]\n",
            "loss: 0.815255  [   28/ 3200]\n",
            "loss: 1.389614  [   29/ 3200]\n",
            "loss: 0.917634  [   30/ 3200]\n",
            "loss: 0.996188  [   31/ 3200]\n",
            "loss: 1.409360  [   32/ 3200]\n",
            "loss: 0.611332  [   33/ 3200]\n",
            "loss: 0.866166  [   34/ 3200]\n",
            "loss: 0.999140  [   35/ 3200]\n",
            "loss: 0.717167  [   36/ 3200]\n",
            "loss: 1.252823  [   37/ 3200]\n",
            "loss: 0.937956  [   38/ 3200]\n",
            "loss: 0.865557  [   39/ 3200]\n",
            "loss: 0.890249  [   40/ 3200]\n",
            "loss: 0.854963  [   41/ 3200]\n",
            "loss: 1.079860  [   42/ 3200]\n",
            "loss: 0.768291  [   43/ 3200]\n",
            "loss: 0.802898  [   44/ 3200]\n",
            "loss: 0.701638  [   45/ 3200]\n",
            "loss: 1.283202  [   46/ 3200]\n",
            "loss: 1.234262  [   47/ 3200]\n",
            "loss: 0.864940  [   48/ 3200]\n",
            "loss: 0.846576  [   49/ 3200]\n",
            "loss: 1.241943  [   50/ 3200]\n",
            "loss: 1.021063  [   51/ 3200]\n",
            "loss: 0.713544  [   52/ 3200]\n",
            "loss: 0.831129  [   53/ 3200]\n",
            "loss: 0.784205  [   54/ 3200]\n",
            "loss: 1.371407  [   55/ 3200]\n",
            "loss: 1.004240  [   56/ 3200]\n",
            "loss: 0.879987  [   57/ 3200]\n",
            "loss: 1.073642  [   58/ 3200]\n",
            "loss: 1.083389  [   59/ 3200]\n",
            "loss: 1.173460  [   60/ 3200]\n",
            "loss: 0.644045  [   61/ 3200]\n",
            "loss: 0.987596  [   62/ 3200]\n",
            "loss: 0.731786  [   63/ 3200]\n",
            "loss: 0.712392  [   64/ 3200]\n",
            "loss: 0.935652  [   65/ 3200]\n",
            "loss: 0.651786  [   66/ 3200]\n",
            "loss: 0.785519  [   67/ 3200]\n",
            "loss: 0.887506  [   68/ 3200]\n",
            "loss: 0.763674  [   69/ 3200]\n",
            "loss: 0.820193  [   70/ 3200]\n",
            "loss: 0.923451  [   71/ 3200]\n",
            "loss: 1.072163  [   72/ 3200]\n",
            "loss: 0.833795  [   73/ 3200]\n",
            "loss: 0.716562  [   74/ 3200]\n",
            "loss: 0.705879  [   75/ 3200]\n",
            "loss: 0.919912  [   76/ 3200]\n",
            "loss: 0.771930  [   77/ 3200]\n",
            "loss: 0.841253  [   78/ 3200]\n",
            "loss: 0.928400  [   79/ 3200]\n",
            "loss: 1.005633  [   80/ 3200]\n",
            "loss: 0.932126  [   81/ 3200]\n",
            "loss: 0.733033  [   82/ 3200]\n",
            "loss: 0.956258  [   83/ 3200]\n",
            "loss: 0.809265  [   84/ 3200]\n",
            "loss: 0.986476  [   85/ 3200]\n",
            "loss: 0.925984  [   86/ 3200]\n",
            "loss: 0.835971  [   87/ 3200]\n",
            "loss: 1.186057  [   88/ 3200]\n",
            "loss: 0.978246  [   89/ 3200]\n",
            "loss: 0.989655  [   90/ 3200]\n",
            "loss: 1.000806  [   91/ 3200]\n",
            "loss: 0.965512  [   92/ 3200]\n",
            "loss: 0.731848  [   93/ 3200]\n",
            "loss: 0.850017  [   94/ 3200]\n",
            "loss: 0.806957  [   95/ 3200]\n",
            "loss: 0.977138  [   96/ 3200]\n",
            "loss: 0.870978  [   97/ 3200]\n",
            "loss: 1.043084  [   98/ 3200]\n",
            "loss: 1.065439  [   99/ 3200]\n",
            "loss: 0.854392  [  100/ 3200]\n",
            "loss: 0.854437  [  101/ 3200]\n",
            "loss: 0.559072  [  102/ 3200]\n",
            "loss: 0.789835  [  103/ 3200]\n",
            "loss: 0.752112  [  104/ 3200]\n",
            "loss: 0.917845  [  105/ 3200]\n",
            "loss: 0.696310  [  106/ 3200]\n",
            "loss: 0.896573  [  107/ 3200]\n",
            "loss: 0.595982  [  108/ 3200]\n",
            "loss: 0.772972  [  109/ 3200]\n",
            "loss: 0.719246  [  110/ 3200]\n",
            "loss: 0.697772  [  111/ 3200]\n",
            "loss: 1.109715  [  112/ 3200]\n",
            "loss: 0.652953  [  113/ 3200]\n",
            "loss: 1.115190  [  114/ 3200]\n",
            "loss: 1.351496  [  115/ 3200]\n",
            "loss: 0.933211  [  116/ 3200]\n",
            "loss: 1.132768  [  117/ 3200]\n",
            "loss: 1.149953  [  118/ 3200]\n",
            "loss: 1.104903  [  119/ 3200]\n",
            "loss: 0.905525  [  120/ 3200]\n",
            "loss: 0.893383  [  121/ 3200]\n",
            "loss: 0.670899  [  122/ 3200]\n",
            "loss: 1.400341  [  123/ 3200]\n",
            "loss: 1.160894  [  124/ 3200]\n",
            "loss: 1.248740  [  125/ 3200]\n",
            "loss: 0.991434  [  126/ 3200]\n",
            "loss: 0.674719  [  127/ 3200]\n",
            "loss: 0.995337  [  128/ 3200]\n",
            "loss: 0.794062  [  129/ 3200]\n",
            "loss: 0.628350  [  130/ 3200]\n",
            "loss: 1.053104  [  131/ 3200]\n",
            "loss: 0.925735  [  132/ 3200]\n",
            "loss: 0.858422  [  133/ 3200]\n",
            "loss: 0.912744  [  134/ 3200]\n",
            "loss: 0.774317  [  135/ 3200]\n",
            "loss: 0.776805  [  136/ 3200]\n",
            "loss: 0.829726  [  137/ 3200]\n",
            "loss: 0.806275  [  138/ 3200]\n",
            "loss: 0.787396  [  139/ 3200]\n",
            "loss: 0.690237  [  140/ 3200]\n",
            "loss: 0.942283  [  141/ 3200]\n",
            "loss: 1.130116  [  142/ 3200]\n",
            "loss: 0.716028  [  143/ 3200]\n",
            "loss: 0.780963  [  144/ 3200]\n",
            "loss: 1.190799  [  145/ 3200]\n",
            "loss: 0.732550  [  146/ 3200]\n",
            "loss: 0.941678  [  147/ 3200]\n",
            "loss: 0.929312  [  148/ 3200]\n",
            "loss: 0.962433  [  149/ 3200]\n",
            "loss: 0.845328  [  150/ 3200]\n",
            "loss: 0.867866  [  151/ 3200]\n",
            "loss: 1.070798  [  152/ 3200]\n",
            "loss: 1.045727  [  153/ 3200]\n",
            "loss: 1.188241  [  154/ 3200]\n",
            "loss: 1.150503  [  155/ 3200]\n",
            "loss: 0.889374  [  156/ 3200]\n",
            "loss: 1.136093  [  157/ 3200]\n",
            "loss: 0.719169  [  158/ 3200]\n",
            "loss: 0.748924  [  159/ 3200]\n",
            "loss: 0.839460  [  160/ 3200]\n",
            "loss: 0.787965  [  161/ 3200]\n",
            "loss: 1.091737  [  162/ 3200]\n",
            "loss: 0.854206  [  163/ 3200]\n",
            "loss: 0.732685  [  164/ 3200]\n",
            "loss: 0.932522  [  165/ 3200]\n",
            "loss: 0.872968  [  166/ 3200]\n",
            "loss: 1.053616  [  167/ 3200]\n",
            "loss: 0.708580  [  168/ 3200]\n",
            "loss: 0.771305  [  169/ 3200]\n",
            "loss: 1.065880  [  170/ 3200]\n",
            "loss: 1.007548  [  171/ 3200]\n",
            "loss: 0.886153  [  172/ 3200]\n",
            "loss: 0.958834  [  173/ 3200]\n",
            "loss: 0.936824  [  174/ 3200]\n",
            "loss: 0.621850  [  175/ 3200]\n",
            "loss: 0.946296  [  176/ 3200]\n",
            "loss: 0.687672  [  177/ 3200]\n",
            "loss: 0.872505  [  178/ 3200]\n",
            "loss: 0.898635  [  179/ 3200]\n",
            "loss: 0.619762  [  180/ 3200]\n",
            "loss: 0.945291  [  181/ 3200]\n",
            "loss: 0.561054  [  182/ 3200]\n",
            "loss: 0.913914  [  183/ 3200]\n",
            "loss: 0.741230  [  184/ 3200]\n",
            "loss: 0.673329  [  185/ 3200]\n",
            "loss: 0.982971  [  186/ 3200]\n",
            "loss: 0.692881  [  187/ 3200]\n",
            "loss: 0.785489  [  188/ 3200]\n",
            "loss: 0.991050  [  189/ 3200]\n",
            "loss: 1.172759  [  190/ 3200]\n",
            "loss: 0.867528  [  191/ 3200]\n",
            "loss: 0.723935  [  192/ 3200]\n",
            "loss: 1.013783  [  193/ 3200]\n",
            "loss: 0.907834  [  194/ 3200]\n",
            "loss: 0.996150  [  195/ 3200]\n",
            "loss: 0.906705  [  196/ 3200]\n",
            "loss: 0.770278  [  197/ 3200]\n",
            "loss: 0.772684  [  198/ 3200]\n",
            "loss: 1.107684  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 13\n",
            "Validation Loss: 0.8298\n",
            "Validation F1: 0.6680\n",
            "Validation Accuracy: 0.6837\n",
            "Validation Confusion Matrix:\n",
            "[[ 74  54  33  39]\n",
            " [  3 184   3  10]\n",
            " [ 45   6 124  25]\n",
            " [ 15  14   6 165]]\n",
            "Epoch:  1\n",
            "loss: 1.020135  [    0/ 3200]\n",
            "loss: 0.951037  [    1/ 3200]\n",
            "loss: 0.840215  [    2/ 3200]\n",
            "loss: 0.835183  [    3/ 3200]\n",
            "loss: 0.939259  [    4/ 3200]\n",
            "loss: 1.186288  [    5/ 3200]\n",
            "loss: 0.850398  [    6/ 3200]\n",
            "loss: 0.634932  [    7/ 3200]\n",
            "loss: 1.036418  [    8/ 3200]\n",
            "loss: 1.037332  [    9/ 3200]\n",
            "loss: 1.042743  [   10/ 3200]\n",
            "loss: 1.119976  [   11/ 3200]\n",
            "loss: 0.978136  [   12/ 3200]\n",
            "loss: 0.959411  [   13/ 3200]\n",
            "loss: 0.826589  [   14/ 3200]\n",
            "loss: 1.022621  [   15/ 3200]\n",
            "loss: 1.126354  [   16/ 3200]\n",
            "loss: 1.093865  [   17/ 3200]\n",
            "loss: 0.854198  [   18/ 3200]\n",
            "loss: 1.039723  [   19/ 3200]\n",
            "loss: 0.865600  [   20/ 3200]\n",
            "loss: 0.960757  [   21/ 3200]\n",
            "loss: 1.000957  [   22/ 3200]\n",
            "loss: 0.833530  [   23/ 3200]\n",
            "loss: 0.707195  [   24/ 3200]\n",
            "loss: 0.706282  [   25/ 3200]\n",
            "loss: 0.925655  [   26/ 3200]\n",
            "loss: 1.309632  [   27/ 3200]\n",
            "loss: 0.585105  [   28/ 3200]\n",
            "loss: 1.150148  [   29/ 3200]\n",
            "loss: 0.754291  [   30/ 3200]\n",
            "loss: 0.903820  [   31/ 3200]\n",
            "loss: 1.044349  [   32/ 3200]\n",
            "loss: 0.672506  [   33/ 3200]\n",
            "loss: 0.868179  [   34/ 3200]\n",
            "loss: 0.678585  [   35/ 3200]\n",
            "loss: 0.979532  [   36/ 3200]\n",
            "loss: 0.734984  [   37/ 3200]\n",
            "loss: 0.678315  [   38/ 3200]\n",
            "loss: 0.683601  [   39/ 3200]\n",
            "loss: 0.909930  [   40/ 3200]\n",
            "loss: 1.376304  [   41/ 3200]\n",
            "loss: 0.693558  [   42/ 3200]\n",
            "loss: 0.799344  [   43/ 3200]\n",
            "loss: 0.969883  [   44/ 3200]\n",
            "loss: 0.692973  [   45/ 3200]\n",
            "loss: 0.989530  [   46/ 3200]\n",
            "loss: 0.710988  [   47/ 3200]\n",
            "loss: 0.791887  [   48/ 3200]\n",
            "loss: 0.966839  [   49/ 3200]\n",
            "loss: 1.330387  [   50/ 3200]\n",
            "loss: 0.934844  [   51/ 3200]\n",
            "loss: 0.799260  [   52/ 3200]\n",
            "loss: 1.231028  [   53/ 3200]\n",
            "loss: 0.731471  [   54/ 3200]\n",
            "loss: 1.036242  [   55/ 3200]\n",
            "loss: 1.373544  [   56/ 3200]\n",
            "loss: 1.011146  [   57/ 3200]\n",
            "loss: 0.795387  [   58/ 3200]\n",
            "loss: 0.884207  [   59/ 3200]\n",
            "loss: 0.776395  [   60/ 3200]\n",
            "loss: 0.801477  [   61/ 3200]\n",
            "loss: 1.494168  [   62/ 3200]\n",
            "loss: 0.835142  [   63/ 3200]\n",
            "loss: 1.111423  [   64/ 3200]\n",
            "loss: 1.024075  [   65/ 3200]\n",
            "loss: 0.785634  [   66/ 3200]\n",
            "loss: 0.949257  [   67/ 3200]\n",
            "loss: 0.940380  [   68/ 3200]\n",
            "loss: 0.700944  [   69/ 3200]\n",
            "loss: 0.782643  [   70/ 3200]\n",
            "loss: 0.852925  [   71/ 3200]\n",
            "loss: 1.097545  [   72/ 3200]\n",
            "loss: 1.013719  [   73/ 3200]\n",
            "loss: 0.829507  [   74/ 3200]\n",
            "loss: 0.814370  [   75/ 3200]\n",
            "loss: 0.600382  [   76/ 3200]\n",
            "loss: 0.715975  [   77/ 3200]\n",
            "loss: 0.751842  [   78/ 3200]\n",
            "loss: 1.085817  [   79/ 3200]\n",
            "loss: 1.161165  [   80/ 3200]\n",
            "loss: 0.937403  [   81/ 3200]\n",
            "loss: 0.726118  [   82/ 3200]\n",
            "loss: 1.279613  [   83/ 3200]\n",
            "loss: 0.846812  [   84/ 3200]\n",
            "loss: 0.970800  [   85/ 3200]\n",
            "loss: 0.795271  [   86/ 3200]\n",
            "loss: 0.916309  [   87/ 3200]\n",
            "loss: 0.532821  [   88/ 3200]\n",
            "loss: 0.984371  [   89/ 3200]\n",
            "loss: 0.683262  [   90/ 3200]\n",
            "loss: 0.706704  [   91/ 3200]\n",
            "loss: 1.057755  [   92/ 3200]\n",
            "loss: 0.508160  [   93/ 3200]\n",
            "loss: 0.666804  [   94/ 3200]\n",
            "loss: 0.936179  [   95/ 3200]\n",
            "loss: 0.790860  [   96/ 3200]\n",
            "loss: 0.681099  [   97/ 3200]\n",
            "loss: 0.762884  [   98/ 3200]\n",
            "loss: 0.909715  [   99/ 3200]\n",
            "loss: 0.810427  [  100/ 3200]\n",
            "loss: 0.940601  [  101/ 3200]\n",
            "loss: 1.163316  [  102/ 3200]\n",
            "loss: 0.615402  [  103/ 3200]\n",
            "loss: 0.894966  [  104/ 3200]\n",
            "loss: 0.725631  [  105/ 3200]\n",
            "loss: 0.710669  [  106/ 3200]\n",
            "loss: 0.574585  [  107/ 3200]\n",
            "loss: 0.804917  [  108/ 3200]\n",
            "loss: 1.223101  [  109/ 3200]\n",
            "loss: 1.138327  [  110/ 3200]\n",
            "loss: 0.677017  [  111/ 3200]\n",
            "loss: 0.624941  [  112/ 3200]\n",
            "loss: 0.798043  [  113/ 3200]\n",
            "loss: 0.670793  [  114/ 3200]\n",
            "loss: 0.678346  [  115/ 3200]\n",
            "loss: 0.628681  [  116/ 3200]\n",
            "loss: 0.920027  [  117/ 3200]\n",
            "loss: 0.940312  [  118/ 3200]\n",
            "loss: 0.864342  [  119/ 3200]\n",
            "loss: 0.796993  [  120/ 3200]\n",
            "loss: 0.550895  [  121/ 3200]\n",
            "loss: 0.790980  [  122/ 3200]\n",
            "loss: 0.948860  [  123/ 3200]\n",
            "loss: 0.739791  [  124/ 3200]\n",
            "loss: 0.893283  [  125/ 3200]\n",
            "loss: 0.900743  [  126/ 3200]\n",
            "loss: 1.005510  [  127/ 3200]\n",
            "loss: 1.025468  [  128/ 3200]\n",
            "loss: 0.923640  [  129/ 3200]\n",
            "loss: 0.843179  [  130/ 3200]\n",
            "loss: 0.843752  [  131/ 3200]\n",
            "loss: 1.357431  [  132/ 3200]\n",
            "loss: 0.583435  [  133/ 3200]\n",
            "loss: 1.118350  [  134/ 3200]\n",
            "loss: 0.754750  [  135/ 3200]\n",
            "loss: 0.859781  [  136/ 3200]\n",
            "loss: 0.770599  [  137/ 3200]\n",
            "loss: 1.272090  [  138/ 3200]\n",
            "loss: 0.692847  [  139/ 3200]\n",
            "loss: 0.764316  [  140/ 3200]\n",
            "loss: 0.655134  [  141/ 3200]\n",
            "loss: 0.640334  [  142/ 3200]\n",
            "loss: 1.385854  [  143/ 3200]\n",
            "loss: 0.871003  [  144/ 3200]\n",
            "loss: 0.958859  [  145/ 3200]\n",
            "loss: 1.159207  [  146/ 3200]\n",
            "loss: 1.662273  [  147/ 3200]\n",
            "loss: 0.892196  [  148/ 3200]\n",
            "loss: 0.930685  [  149/ 3200]\n",
            "loss: 1.384685  [  150/ 3200]\n",
            "loss: 0.770984  [  151/ 3200]\n",
            "loss: 0.664127  [  152/ 3200]\n",
            "loss: 1.065946  [  153/ 3200]\n",
            "loss: 0.644562  [  154/ 3200]\n",
            "loss: 1.342740  [  155/ 3200]\n",
            "loss: 1.261166  [  156/ 3200]\n",
            "loss: 0.815969  [  157/ 3200]\n",
            "loss: 0.883552  [  158/ 3200]\n",
            "loss: 1.120947  [  159/ 3200]\n",
            "loss: 0.629103  [  160/ 3200]\n",
            "loss: 1.021539  [  161/ 3200]\n",
            "loss: 1.009616  [  162/ 3200]\n",
            "loss: 0.908301  [  163/ 3200]\n",
            "loss: 1.102727  [  164/ 3200]\n",
            "loss: 1.026319  [  165/ 3200]\n",
            "loss: 0.913562  [  166/ 3200]\n",
            "loss: 0.715602  [  167/ 3200]\n",
            "loss: 0.909443  [  168/ 3200]\n",
            "loss: 0.845423  [  169/ 3200]\n",
            "loss: 0.939490  [  170/ 3200]\n",
            "loss: 0.837694  [  171/ 3200]\n",
            "loss: 0.655719  [  172/ 3200]\n",
            "loss: 1.162738  [  173/ 3200]\n",
            "loss: 0.923823  [  174/ 3200]\n",
            "loss: 0.916266  [  175/ 3200]\n",
            "loss: 0.977774  [  176/ 3200]\n",
            "loss: 0.943381  [  177/ 3200]\n",
            "loss: 0.904138  [  178/ 3200]\n",
            "loss: 0.890912  [  179/ 3200]\n",
            "loss: 1.133615  [  180/ 3200]\n",
            "loss: 0.905483  [  181/ 3200]\n",
            "loss: 1.059507  [  182/ 3200]\n",
            "loss: 0.892830  [  183/ 3200]\n",
            "loss: 0.815558  [  184/ 3200]\n",
            "loss: 0.790455  [  185/ 3200]\n",
            "loss: 1.072534  [  186/ 3200]\n",
            "loss: 0.743040  [  187/ 3200]\n",
            "loss: 0.984751  [  188/ 3200]\n",
            "loss: 1.096481  [  189/ 3200]\n",
            "loss: 0.906163  [  190/ 3200]\n",
            "loss: 0.638173  [  191/ 3200]\n",
            "loss: 0.854639  [  192/ 3200]\n",
            "loss: 0.988648  [  193/ 3200]\n",
            "loss: 0.978858  [  194/ 3200]\n",
            "loss: 0.729100  [  195/ 3200]\n",
            "loss: 1.074703  [  196/ 3200]\n",
            "loss: 0.949298  [  197/ 3200]\n",
            "loss: 0.847135  [  198/ 3200]\n",
            "loss: 0.826352  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 14\n",
            "Validation Loss: 0.8418\n",
            "Validation F1: 0.6670\n",
            "Validation Accuracy: 0.6775\n",
            "Validation Confusion Matrix:\n",
            "[[ 78  63  36  23]\n",
            " [  5 188   3   4]\n",
            " [ 52   6 130  12]\n",
            " [ 24  23   7 146]]\n",
            "Epoch:  1\n",
            "loss: 0.954408  [    0/ 3200]\n",
            "loss: 0.846039  [    1/ 3200]\n",
            "loss: 1.533621  [    2/ 3200]\n",
            "loss: 1.010348  [    3/ 3200]\n",
            "loss: 0.642466  [    4/ 3200]\n",
            "loss: 1.044232  [    5/ 3200]\n",
            "loss: 1.214416  [    6/ 3200]\n",
            "loss: 0.898607  [    7/ 3200]\n",
            "loss: 0.849382  [    8/ 3200]\n",
            "loss: 0.626124  [    9/ 3200]\n",
            "loss: 0.931785  [   10/ 3200]\n",
            "loss: 1.353241  [   11/ 3200]\n",
            "loss: 1.173120  [   12/ 3200]\n",
            "loss: 0.983635  [   13/ 3200]\n",
            "loss: 0.898795  [   14/ 3200]\n",
            "loss: 0.697239  [   15/ 3200]\n",
            "loss: 0.878575  [   16/ 3200]\n",
            "loss: 1.176637  [   17/ 3200]\n",
            "loss: 0.835487  [   18/ 3200]\n",
            "loss: 1.094967  [   19/ 3200]\n",
            "loss: 0.924039  [   20/ 3200]\n",
            "loss: 0.880625  [   21/ 3200]\n",
            "loss: 1.186565  [   22/ 3200]\n",
            "loss: 0.588824  [   23/ 3200]\n",
            "loss: 0.963828  [   24/ 3200]\n",
            "loss: 0.433218  [   25/ 3200]\n",
            "loss: 0.757662  [   26/ 3200]\n",
            "loss: 0.849011  [   27/ 3200]\n",
            "loss: 0.821170  [   28/ 3200]\n",
            "loss: 0.930485  [   29/ 3200]\n",
            "loss: 0.791252  [   30/ 3200]\n",
            "loss: 0.919172  [   31/ 3200]\n",
            "loss: 1.023406  [   32/ 3200]\n",
            "loss: 0.720135  [   33/ 3200]\n",
            "loss: 1.111875  [   34/ 3200]\n",
            "loss: 1.014226  [   35/ 3200]\n",
            "loss: 0.771022  [   36/ 3200]\n",
            "loss: 0.653041  [   37/ 3200]\n",
            "loss: 0.651185  [   38/ 3200]\n",
            "loss: 1.217127  [   39/ 3200]\n",
            "loss: 1.084027  [   40/ 3200]\n",
            "loss: 0.930823  [   41/ 3200]\n",
            "loss: 0.967082  [   42/ 3200]\n",
            "loss: 0.825929  [   43/ 3200]\n",
            "loss: 0.822332  [   44/ 3200]\n",
            "loss: 0.800258  [   45/ 3200]\n",
            "loss: 0.574273  [   46/ 3200]\n",
            "loss: 0.988786  [   47/ 3200]\n",
            "loss: 0.793609  [   48/ 3200]\n",
            "loss: 0.697794  [   49/ 3200]\n",
            "loss: 1.217262  [   50/ 3200]\n",
            "loss: 1.033429  [   51/ 3200]\n",
            "loss: 1.063499  [   52/ 3200]\n",
            "loss: 0.736051  [   53/ 3200]\n",
            "loss: 0.753277  [   54/ 3200]\n",
            "loss: 0.780223  [   55/ 3200]\n",
            "loss: 1.054043  [   56/ 3200]\n",
            "loss: 0.754141  [   57/ 3200]\n",
            "loss: 1.137861  [   58/ 3200]\n",
            "loss: 0.785950  [   59/ 3200]\n",
            "loss: 0.807652  [   60/ 3200]\n",
            "loss: 0.777142  [   61/ 3200]\n",
            "loss: 0.779087  [   62/ 3200]\n",
            "loss: 0.803792  [   63/ 3200]\n",
            "loss: 1.243163  [   64/ 3200]\n",
            "loss: 1.012112  [   65/ 3200]\n",
            "loss: 0.572373  [   66/ 3200]\n",
            "loss: 0.750207  [   67/ 3200]\n",
            "loss: 0.739701  [   68/ 3200]\n",
            "loss: 1.039047  [   69/ 3200]\n",
            "loss: 0.722096  [   70/ 3200]\n",
            "loss: 0.987014  [   71/ 3200]\n",
            "loss: 1.089810  [   72/ 3200]\n",
            "loss: 0.815942  [   73/ 3200]\n",
            "loss: 0.557295  [   74/ 3200]\n",
            "loss: 0.940580  [   75/ 3200]\n",
            "loss: 0.965155  [   76/ 3200]\n",
            "loss: 0.825486  [   77/ 3200]\n",
            "loss: 1.052556  [   78/ 3200]\n",
            "loss: 0.903683  [   79/ 3200]\n",
            "loss: 0.984052  [   80/ 3200]\n",
            "loss: 0.995877  [   81/ 3200]\n",
            "loss: 0.729629  [   82/ 3200]\n",
            "loss: 0.793992  [   83/ 3200]\n",
            "loss: 0.766199  [   84/ 3200]\n",
            "loss: 0.688175  [   85/ 3200]\n",
            "loss: 1.235394  [   86/ 3200]\n",
            "loss: 0.994717  [   87/ 3200]\n",
            "loss: 0.785538  [   88/ 3200]\n",
            "loss: 0.822969  [   89/ 3200]\n",
            "loss: 0.935550  [   90/ 3200]\n",
            "loss: 0.743561  [   91/ 3200]\n",
            "loss: 0.986171  [   92/ 3200]\n",
            "loss: 0.854569  [   93/ 3200]\n",
            "loss: 0.901054  [   94/ 3200]\n",
            "loss: 0.923190  [   95/ 3200]\n",
            "loss: 0.865507  [   96/ 3200]\n",
            "loss: 0.741222  [   97/ 3200]\n",
            "loss: 1.060397  [   98/ 3200]\n",
            "loss: 0.821620  [   99/ 3200]\n",
            "loss: 0.707230  [  100/ 3200]\n",
            "loss: 1.026152  [  101/ 3200]\n",
            "loss: 0.847243  [  102/ 3200]\n",
            "loss: 0.625140  [  103/ 3200]\n",
            "loss: 0.965639  [  104/ 3200]\n",
            "loss: 0.951413  [  105/ 3200]\n",
            "loss: 0.597040  [  106/ 3200]\n",
            "loss: 1.066486  [  107/ 3200]\n",
            "loss: 0.894951  [  108/ 3200]\n",
            "loss: 1.084393  [  109/ 3200]\n",
            "loss: 0.927306  [  110/ 3200]\n",
            "loss: 0.673125  [  111/ 3200]\n",
            "loss: 1.051036  [  112/ 3200]\n",
            "loss: 0.786677  [  113/ 3200]\n",
            "loss: 0.710531  [  114/ 3200]\n",
            "loss: 0.862860  [  115/ 3200]\n",
            "loss: 0.925132  [  116/ 3200]\n",
            "loss: 1.088495  [  117/ 3200]\n",
            "loss: 0.917303  [  118/ 3200]\n",
            "loss: 1.199367  [  119/ 3200]\n",
            "loss: 1.213458  [  120/ 3200]\n",
            "loss: 1.023992  [  121/ 3200]\n",
            "loss: 0.835077  [  122/ 3200]\n",
            "loss: 1.312278  [  123/ 3200]\n",
            "loss: 0.993819  [  124/ 3200]\n",
            "loss: 0.842241  [  125/ 3200]\n",
            "loss: 0.815970  [  126/ 3200]\n",
            "loss: 0.890983  [  127/ 3200]\n",
            "loss: 0.816801  [  128/ 3200]\n",
            "loss: 1.317930  [  129/ 3200]\n",
            "loss: 0.742899  [  130/ 3200]\n",
            "loss: 0.904900  [  131/ 3200]\n",
            "loss: 0.575412  [  132/ 3200]\n",
            "loss: 0.680045  [  133/ 3200]\n",
            "loss: 0.697660  [  134/ 3200]\n",
            "loss: 0.672159  [  135/ 3200]\n",
            "loss: 0.699300  [  136/ 3200]\n",
            "loss: 1.247095  [  137/ 3200]\n",
            "loss: 1.007498  [  138/ 3200]\n",
            "loss: 1.017560  [  139/ 3200]\n",
            "loss: 0.975072  [  140/ 3200]\n",
            "loss: 0.807697  [  141/ 3200]\n",
            "loss: 0.939466  [  142/ 3200]\n",
            "loss: 1.046153  [  143/ 3200]\n",
            "loss: 0.790743  [  144/ 3200]\n",
            "loss: 0.875081  [  145/ 3200]\n",
            "loss: 0.587131  [  146/ 3200]\n",
            "loss: 1.361392  [  147/ 3200]\n",
            "loss: 0.857012  [  148/ 3200]\n",
            "loss: 0.524676  [  149/ 3200]\n",
            "loss: 0.595065  [  150/ 3200]\n",
            "loss: 0.987436  [  151/ 3200]\n",
            "loss: 0.799677  [  152/ 3200]\n",
            "loss: 0.882248  [  153/ 3200]\n",
            "loss: 0.560837  [  154/ 3200]\n",
            "loss: 1.209618  [  155/ 3200]\n",
            "loss: 1.332139  [  156/ 3200]\n",
            "loss: 0.406299  [  157/ 3200]\n",
            "loss: 1.134205  [  158/ 3200]\n",
            "loss: 0.937880  [  159/ 3200]\n",
            "loss: 0.929162  [  160/ 3200]\n",
            "loss: 0.723180  [  161/ 3200]\n",
            "loss: 0.575591  [  162/ 3200]\n",
            "loss: 1.261790  [  163/ 3200]\n",
            "loss: 1.119688  [  164/ 3200]\n",
            "loss: 0.690558  [  165/ 3200]\n",
            "loss: 0.934022  [  166/ 3200]\n",
            "loss: 0.818716  [  167/ 3200]\n",
            "loss: 0.869188  [  168/ 3200]\n",
            "loss: 0.857214  [  169/ 3200]\n",
            "loss: 0.798155  [  170/ 3200]\n",
            "loss: 0.829479  [  171/ 3200]\n",
            "loss: 0.968264  [  172/ 3200]\n",
            "loss: 0.816546  [  173/ 3200]\n",
            "loss: 1.143760  [  174/ 3200]\n",
            "loss: 0.735762  [  175/ 3200]\n",
            "loss: 0.885896  [  176/ 3200]\n",
            "loss: 0.775012  [  177/ 3200]\n",
            "loss: 0.862093  [  178/ 3200]\n",
            "loss: 0.765037  [  179/ 3200]\n",
            "loss: 0.999715  [  180/ 3200]\n",
            "loss: 1.275795  [  181/ 3200]\n",
            "loss: 1.158769  [  182/ 3200]\n",
            "loss: 0.641657  [  183/ 3200]\n",
            "loss: 0.908635  [  184/ 3200]\n",
            "loss: 0.999236  [  185/ 3200]\n",
            "loss: 1.012676  [  186/ 3200]\n",
            "loss: 0.967867  [  187/ 3200]\n",
            "loss: 0.655405  [  188/ 3200]\n",
            "loss: 0.920808  [  189/ 3200]\n",
            "loss: 0.892911  [  190/ 3200]\n",
            "loss: 0.889172  [  191/ 3200]\n",
            "loss: 0.874180  [  192/ 3200]\n",
            "loss: 0.627140  [  193/ 3200]\n",
            "loss: 0.939088  [  194/ 3200]\n",
            "loss: 1.079221  [  195/ 3200]\n",
            "loss: 0.706061  [  196/ 3200]\n",
            "loss: 1.254116  [  197/ 3200]\n",
            "loss: 0.777762  [  198/ 3200]\n",
            "loss: 0.986482  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 15\n",
            "Validation Loss: 0.8306\n",
            "Validation F1: 0.6635\n",
            "Validation Accuracy: 0.6800\n",
            "Validation Confusion Matrix:\n",
            "[[ 63  52  67  18]\n",
            " [  7 187   4   2]\n",
            " [ 35   4 154   7]\n",
            " [ 28  20  12 140]]\n",
            "Epoch:  1\n",
            "loss: 0.658436  [    0/ 3200]\n",
            "loss: 0.730462  [    1/ 3200]\n",
            "loss: 0.947832  [    2/ 3200]\n",
            "loss: 0.962639  [    3/ 3200]\n",
            "loss: 0.804664  [    4/ 3200]\n",
            "loss: 0.968091  [    5/ 3200]\n",
            "loss: 0.985972  [    6/ 3200]\n",
            "loss: 0.850673  [    7/ 3200]\n",
            "loss: 1.136817  [    8/ 3200]\n",
            "loss: 0.682015  [    9/ 3200]\n",
            "loss: 1.140054  [   10/ 3200]\n",
            "loss: 0.963442  [   11/ 3200]\n",
            "loss: 0.794965  [   12/ 3200]\n",
            "loss: 0.811816  [   13/ 3200]\n",
            "loss: 0.990513  [   14/ 3200]\n",
            "loss: 0.962321  [   15/ 3200]\n",
            "loss: 0.598293  [   16/ 3200]\n",
            "loss: 0.780946  [   17/ 3200]\n",
            "loss: 0.808343  [   18/ 3200]\n",
            "loss: 0.854879  [   19/ 3200]\n",
            "loss: 0.852742  [   20/ 3200]\n",
            "loss: 1.049735  [   21/ 3200]\n",
            "loss: 0.947437  [   22/ 3200]\n",
            "loss: 1.095962  [   23/ 3200]\n",
            "loss: 0.747049  [   24/ 3200]\n",
            "loss: 0.729372  [   25/ 3200]\n",
            "loss: 1.391559  [   26/ 3200]\n",
            "loss: 0.898103  [   27/ 3200]\n",
            "loss: 1.080112  [   28/ 3200]\n",
            "loss: 0.899342  [   29/ 3200]\n",
            "loss: 0.956286  [   30/ 3200]\n",
            "loss: 0.964217  [   31/ 3200]\n",
            "loss: 0.693932  [   32/ 3200]\n",
            "loss: 0.875351  [   33/ 3200]\n",
            "loss: 1.060086  [   34/ 3200]\n",
            "loss: 0.870604  [   35/ 3200]\n",
            "loss: 0.733034  [   36/ 3200]\n",
            "loss: 1.164366  [   37/ 3200]\n",
            "loss: 1.099896  [   38/ 3200]\n",
            "loss: 0.686200  [   39/ 3200]\n",
            "loss: 0.858813  [   40/ 3200]\n",
            "loss: 0.720058  [   41/ 3200]\n",
            "loss: 1.063441  [   42/ 3200]\n",
            "loss: 0.901598  [   43/ 3200]\n",
            "loss: 0.923077  [   44/ 3200]\n",
            "loss: 0.647241  [   45/ 3200]\n",
            "loss: 0.715710  [   46/ 3200]\n",
            "loss: 0.910673  [   47/ 3200]\n",
            "loss: 1.123768  [   48/ 3200]\n",
            "loss: 0.907306  [   49/ 3200]\n",
            "loss: 0.660435  [   50/ 3200]\n",
            "loss: 0.728709  [   51/ 3200]\n",
            "loss: 1.246980  [   52/ 3200]\n",
            "loss: 0.967918  [   53/ 3200]\n",
            "loss: 0.618864  [   54/ 3200]\n",
            "loss: 0.937843  [   55/ 3200]\n",
            "loss: 0.863469  [   56/ 3200]\n",
            "loss: 0.945149  [   57/ 3200]\n",
            "loss: 1.074885  [   58/ 3200]\n",
            "loss: 0.985422  [   59/ 3200]\n",
            "loss: 1.258548  [   60/ 3200]\n",
            "loss: 1.189772  [   61/ 3200]\n",
            "loss: 1.017452  [   62/ 3200]\n",
            "loss: 1.006151  [   63/ 3200]\n",
            "loss: 1.162804  [   64/ 3200]\n",
            "loss: 0.906116  [   65/ 3200]\n",
            "loss: 1.077383  [   66/ 3200]\n",
            "loss: 1.239432  [   67/ 3200]\n",
            "loss: 0.729881  [   68/ 3200]\n",
            "loss: 0.865785  [   69/ 3200]\n",
            "loss: 0.738432  [   70/ 3200]\n",
            "loss: 0.746885  [   71/ 3200]\n",
            "loss: 1.263183  [   72/ 3200]\n",
            "loss: 0.893260  [   73/ 3200]\n",
            "loss: 0.882349  [   74/ 3200]\n",
            "loss: 0.929809  [   75/ 3200]\n",
            "loss: 0.938690  [   76/ 3200]\n",
            "loss: 1.098118  [   77/ 3200]\n",
            "loss: 0.872026  [   78/ 3200]\n",
            "loss: 0.998684  [   79/ 3200]\n",
            "loss: 0.956049  [   80/ 3200]\n",
            "loss: 0.908738  [   81/ 3200]\n",
            "loss: 0.802574  [   82/ 3200]\n",
            "loss: 0.806994  [   83/ 3200]\n",
            "loss: 0.761590  [   84/ 3200]\n",
            "loss: 0.881894  [   85/ 3200]\n",
            "loss: 0.787137  [   86/ 3200]\n",
            "loss: 0.800753  [   87/ 3200]\n",
            "loss: 0.588219  [   88/ 3200]\n",
            "loss: 0.815871  [   89/ 3200]\n",
            "loss: 1.050271  [   90/ 3200]\n",
            "loss: 0.750948  [   91/ 3200]\n",
            "loss: 1.155764  [   92/ 3200]\n",
            "loss: 0.702231  [   93/ 3200]\n",
            "loss: 1.007026  [   94/ 3200]\n",
            "loss: 0.689064  [   95/ 3200]\n",
            "loss: 1.166640  [   96/ 3200]\n",
            "loss: 0.784312  [   97/ 3200]\n",
            "loss: 0.942085  [   98/ 3200]\n",
            "loss: 0.731546  [   99/ 3200]\n",
            "loss: 1.071635  [  100/ 3200]\n",
            "loss: 0.688316  [  101/ 3200]\n",
            "loss: 1.098793  [  102/ 3200]\n",
            "loss: 0.940189  [  103/ 3200]\n",
            "loss: 0.634367  [  104/ 3200]\n",
            "loss: 0.843743  [  105/ 3200]\n",
            "loss: 0.604638  [  106/ 3200]\n",
            "loss: 1.127488  [  107/ 3200]\n",
            "loss: 0.946416  [  108/ 3200]\n",
            "loss: 0.834826  [  109/ 3200]\n",
            "loss: 0.939877  [  110/ 3200]\n",
            "loss: 0.798784  [  111/ 3200]\n",
            "loss: 1.192602  [  112/ 3200]\n",
            "loss: 1.252338  [  113/ 3200]\n",
            "loss: 0.869810  [  114/ 3200]\n",
            "loss: 0.904939  [  115/ 3200]\n",
            "loss: 0.771367  [  116/ 3200]\n",
            "loss: 1.285089  [  117/ 3200]\n",
            "loss: 0.863347  [  118/ 3200]\n",
            "loss: 0.817796  [  119/ 3200]\n",
            "loss: 0.768495  [  120/ 3200]\n",
            "loss: 1.495916  [  121/ 3200]\n",
            "loss: 0.755538  [  122/ 3200]\n",
            "loss: 1.370267  [  123/ 3200]\n",
            "loss: 0.510562  [  124/ 3200]\n",
            "loss: 0.815473  [  125/ 3200]\n",
            "loss: 0.585177  [  126/ 3200]\n",
            "loss: 0.989838  [  127/ 3200]\n",
            "loss: 0.759111  [  128/ 3200]\n",
            "loss: 0.895688  [  129/ 3200]\n",
            "loss: 0.940639  [  130/ 3200]\n",
            "loss: 0.625086  [  131/ 3200]\n",
            "loss: 0.700703  [  132/ 3200]\n",
            "loss: 0.729218  [  133/ 3200]\n",
            "loss: 0.640034  [  134/ 3200]\n",
            "loss: 1.017871  [  135/ 3200]\n",
            "loss: 0.769851  [  136/ 3200]\n",
            "loss: 0.829765  [  137/ 3200]\n",
            "loss: 1.046473  [  138/ 3200]\n",
            "loss: 1.025605  [  139/ 3200]\n",
            "loss: 0.993341  [  140/ 3200]\n",
            "loss: 0.616566  [  141/ 3200]\n",
            "loss: 0.848373  [  142/ 3200]\n",
            "loss: 0.900898  [  143/ 3200]\n",
            "loss: 0.655758  [  144/ 3200]\n",
            "loss: 1.025761  [  145/ 3200]\n",
            "loss: 1.293478  [  146/ 3200]\n",
            "loss: 0.776645  [  147/ 3200]\n",
            "loss: 0.806640  [  148/ 3200]\n",
            "loss: 1.035987  [  149/ 3200]\n",
            "loss: 0.747242  [  150/ 3200]\n",
            "loss: 0.832294  [  151/ 3200]\n",
            "loss: 0.738962  [  152/ 3200]\n",
            "loss: 0.879854  [  153/ 3200]\n",
            "loss: 1.193121  [  154/ 3200]\n",
            "loss: 0.769018  [  155/ 3200]\n",
            "loss: 0.993695  [  156/ 3200]\n",
            "loss: 0.967388  [  157/ 3200]\n",
            "loss: 1.163003  [  158/ 3200]\n",
            "loss: 0.744314  [  159/ 3200]\n",
            "loss: 1.115258  [  160/ 3200]\n",
            "loss: 0.730612  [  161/ 3200]\n",
            "loss: 0.928905  [  162/ 3200]\n",
            "loss: 0.831654  [  163/ 3200]\n",
            "loss: 0.945579  [  164/ 3200]\n",
            "loss: 1.053187  [  165/ 3200]\n",
            "loss: 0.907807  [  166/ 3200]\n",
            "loss: 0.896931  [  167/ 3200]\n",
            "loss: 1.231653  [  168/ 3200]\n",
            "loss: 0.579103  [  169/ 3200]\n",
            "loss: 0.879634  [  170/ 3200]\n",
            "loss: 0.803693  [  171/ 3200]\n",
            "loss: 0.927824  [  172/ 3200]\n",
            "loss: 0.864337  [  173/ 3200]\n",
            "loss: 1.035911  [  174/ 3200]\n",
            "loss: 0.539439  [  175/ 3200]\n",
            "loss: 0.702631  [  176/ 3200]\n",
            "loss: 0.914320  [  177/ 3200]\n",
            "loss: 0.907193  [  178/ 3200]\n",
            "loss: 1.276224  [  179/ 3200]\n",
            "loss: 0.795824  [  180/ 3200]\n",
            "loss: 0.836177  [  181/ 3200]\n",
            "loss: 0.743690  [  182/ 3200]\n",
            "loss: 1.011215  [  183/ 3200]\n",
            "loss: 0.771787  [  184/ 3200]\n",
            "loss: 1.075695  [  185/ 3200]\n",
            "loss: 0.955856  [  186/ 3200]\n",
            "loss: 0.851488  [  187/ 3200]\n",
            "loss: 1.034683  [  188/ 3200]\n",
            "loss: 0.601509  [  189/ 3200]\n",
            "loss: 0.814501  [  190/ 3200]\n",
            "loss: 0.792363  [  191/ 3200]\n",
            "loss: 0.527124  [  192/ 3200]\n",
            "loss: 0.911207  [  193/ 3200]\n",
            "loss: 1.432642  [  194/ 3200]\n",
            "loss: 1.005982  [  195/ 3200]\n",
            "loss: 0.923190  [  196/ 3200]\n",
            "loss: 0.884121  [  197/ 3200]\n",
            "loss: 0.967842  [  198/ 3200]\n",
            "loss: 0.844005  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 16\n",
            "Validation Loss: 0.8494\n",
            "Validation F1: 0.5937\n",
            "Validation Accuracy: 0.6175\n",
            "Validation Confusion Matrix:\n",
            "[[ 37  24 121  18]\n",
            " [ 46 127   8  19]\n",
            " [  8   1 184   7]\n",
            " [ 17   6  31 146]]\n",
            "Epoch:  1\n",
            "loss: 0.577170  [    0/ 3200]\n",
            "loss: 1.253761  [    1/ 3200]\n",
            "loss: 0.936215  [    2/ 3200]\n",
            "loss: 1.079929  [    3/ 3200]\n",
            "loss: 0.977157  [    4/ 3200]\n",
            "loss: 0.825886  [    5/ 3200]\n",
            "loss: 0.858152  [    6/ 3200]\n",
            "loss: 0.764369  [    7/ 3200]\n",
            "loss: 0.569566  [    8/ 3200]\n",
            "loss: 0.881360  [    9/ 3200]\n",
            "loss: 0.787131  [   10/ 3200]\n",
            "loss: 0.685199  [   11/ 3200]\n",
            "loss: 0.864022  [   12/ 3200]\n",
            "loss: 1.027889  [   13/ 3200]\n",
            "loss: 1.050278  [   14/ 3200]\n",
            "loss: 1.027800  [   15/ 3200]\n",
            "loss: 0.924931  [   16/ 3200]\n",
            "loss: 1.167219  [   17/ 3200]\n",
            "loss: 0.612342  [   18/ 3200]\n",
            "loss: 0.835090  [   19/ 3200]\n",
            "loss: 0.974655  [   20/ 3200]\n",
            "loss: 0.586695  [   21/ 3200]\n",
            "loss: 0.990080  [   22/ 3200]\n",
            "loss: 0.699380  [   23/ 3200]\n",
            "loss: 0.772203  [   24/ 3200]\n",
            "loss: 1.299070  [   25/ 3200]\n",
            "loss: 0.807401  [   26/ 3200]\n",
            "loss: 0.949809  [   27/ 3200]\n",
            "loss: 0.768118  [   28/ 3200]\n",
            "loss: 0.688183  [   29/ 3200]\n",
            "loss: 0.985778  [   30/ 3200]\n",
            "loss: 0.814796  [   31/ 3200]\n",
            "loss: 0.943715  [   32/ 3200]\n",
            "loss: 1.277489  [   33/ 3200]\n",
            "loss: 0.830908  [   34/ 3200]\n",
            "loss: 0.811415  [   35/ 3200]\n",
            "loss: 0.789763  [   36/ 3200]\n",
            "loss: 1.421553  [   37/ 3200]\n",
            "loss: 1.042232  [   38/ 3200]\n",
            "loss: 1.177702  [   39/ 3200]\n",
            "loss: 0.654778  [   40/ 3200]\n",
            "loss: 0.917491  [   41/ 3200]\n",
            "loss: 0.831418  [   42/ 3200]\n",
            "loss: 0.799567  [   43/ 3200]\n",
            "loss: 0.977455  [   44/ 3200]\n",
            "loss: 0.939566  [   45/ 3200]\n",
            "loss: 0.948490  [   46/ 3200]\n",
            "loss: 0.731822  [   47/ 3200]\n",
            "loss: 0.965439  [   48/ 3200]\n",
            "loss: 0.798089  [   49/ 3200]\n",
            "loss: 0.692855  [   50/ 3200]\n",
            "loss: 0.750572  [   51/ 3200]\n",
            "loss: 0.828025  [   52/ 3200]\n",
            "loss: 0.920744  [   53/ 3200]\n",
            "loss: 0.923455  [   54/ 3200]\n",
            "loss: 0.902360  [   55/ 3200]\n",
            "loss: 0.883506  [   56/ 3200]\n",
            "loss: 1.075677  [   57/ 3200]\n",
            "loss: 1.277026  [   58/ 3200]\n",
            "loss: 0.963423  [   59/ 3200]\n",
            "loss: 0.750502  [   60/ 3200]\n",
            "loss: 0.620900  [   61/ 3200]\n",
            "loss: 0.583245  [   62/ 3200]\n",
            "loss: 0.895205  [   63/ 3200]\n",
            "loss: 0.589690  [   64/ 3200]\n",
            "loss: 0.943052  [   65/ 3200]\n",
            "loss: 0.746023  [   66/ 3200]\n",
            "loss: 0.726041  [   67/ 3200]\n",
            "loss: 1.044371  [   68/ 3200]\n",
            "loss: 0.770088  [   69/ 3200]\n",
            "loss: 0.976249  [   70/ 3200]\n",
            "loss: 0.846303  [   71/ 3200]\n",
            "loss: 0.851289  [   72/ 3200]\n",
            "loss: 0.678866  [   73/ 3200]\n",
            "loss: 0.988430  [   74/ 3200]\n",
            "loss: 0.746767  [   75/ 3200]\n",
            "loss: 0.901388  [   76/ 3200]\n",
            "loss: 0.922709  [   77/ 3200]\n",
            "loss: 1.000964  [   78/ 3200]\n",
            "loss: 1.091552  [   79/ 3200]\n",
            "loss: 0.828761  [   80/ 3200]\n",
            "loss: 0.882635  [   81/ 3200]\n",
            "loss: 0.839553  [   82/ 3200]\n",
            "loss: 0.997539  [   83/ 3200]\n",
            "loss: 0.876191  [   84/ 3200]\n",
            "loss: 0.839721  [   85/ 3200]\n",
            "loss: 0.776973  [   86/ 3200]\n",
            "loss: 1.075345  [   87/ 3200]\n",
            "loss: 0.957159  [   88/ 3200]\n",
            "loss: 0.836487  [   89/ 3200]\n",
            "loss: 0.678108  [   90/ 3200]\n",
            "loss: 0.727499  [   91/ 3200]\n",
            "loss: 1.204477  [   92/ 3200]\n",
            "loss: 0.881117  [   93/ 3200]\n",
            "loss: 1.045663  [   94/ 3200]\n",
            "loss: 0.786711  [   95/ 3200]\n",
            "loss: 0.869274  [   96/ 3200]\n",
            "loss: 1.169220  [   97/ 3200]\n",
            "loss: 0.856473  [   98/ 3200]\n",
            "loss: 0.821648  [   99/ 3200]\n",
            "loss: 0.914703  [  100/ 3200]\n",
            "loss: 1.268161  [  101/ 3200]\n",
            "loss: 0.950130  [  102/ 3200]\n",
            "loss: 0.726109  [  103/ 3200]\n",
            "loss: 0.833127  [  104/ 3200]\n",
            "loss: 0.812039  [  105/ 3200]\n",
            "loss: 0.835739  [  106/ 3200]\n",
            "loss: 1.210776  [  107/ 3200]\n",
            "loss: 1.119832  [  108/ 3200]\n",
            "loss: 0.860029  [  109/ 3200]\n",
            "loss: 0.878760  [  110/ 3200]\n",
            "loss: 0.757761  [  111/ 3200]\n",
            "loss: 1.068446  [  112/ 3200]\n",
            "loss: 0.671871  [  113/ 3200]\n",
            "loss: 0.840529  [  114/ 3200]\n",
            "loss: 0.793598  [  115/ 3200]\n",
            "loss: 0.781349  [  116/ 3200]\n",
            "loss: 0.764294  [  117/ 3200]\n",
            "loss: 0.994894  [  118/ 3200]\n",
            "loss: 0.976261  [  119/ 3200]\n",
            "loss: 0.748362  [  120/ 3200]\n",
            "loss: 0.857531  [  121/ 3200]\n",
            "loss: 1.184328  [  122/ 3200]\n",
            "loss: 1.042035  [  123/ 3200]\n",
            "loss: 0.825037  [  124/ 3200]\n",
            "loss: 1.407708  [  125/ 3200]\n",
            "loss: 1.134814  [  126/ 3200]\n",
            "loss: 0.531081  [  127/ 3200]\n",
            "loss: 0.764597  [  128/ 3200]\n",
            "loss: 0.894819  [  129/ 3200]\n",
            "loss: 1.084536  [  130/ 3200]\n",
            "loss: 0.903494  [  131/ 3200]\n",
            "loss: 0.590052  [  132/ 3200]\n",
            "loss: 1.232213  [  133/ 3200]\n",
            "loss: 1.223111  [  134/ 3200]\n",
            "loss: 1.013560  [  135/ 3200]\n",
            "loss: 0.820547  [  136/ 3200]\n",
            "loss: 0.990383  [  137/ 3200]\n",
            "loss: 1.075192  [  138/ 3200]\n",
            "loss: 0.776677  [  139/ 3200]\n",
            "loss: 0.853186  [  140/ 3200]\n",
            "loss: 1.256366  [  141/ 3200]\n",
            "loss: 0.721312  [  142/ 3200]\n",
            "loss: 0.761352  [  143/ 3200]\n",
            "loss: 1.026436  [  144/ 3200]\n",
            "loss: 0.869066  [  145/ 3200]\n",
            "loss: 0.977833  [  146/ 3200]\n",
            "loss: 0.843584  [  147/ 3200]\n",
            "loss: 0.835374  [  148/ 3200]\n",
            "loss: 1.002312  [  149/ 3200]\n",
            "loss: 1.088129  [  150/ 3200]\n",
            "loss: 0.926639  [  151/ 3200]\n",
            "loss: 0.734802  [  152/ 3200]\n",
            "loss: 1.015108  [  153/ 3200]\n",
            "loss: 1.059299  [  154/ 3200]\n",
            "loss: 1.068361  [  155/ 3200]\n",
            "loss: 0.958871  [  156/ 3200]\n",
            "loss: 0.886146  [  157/ 3200]\n",
            "loss: 0.700516  [  158/ 3200]\n",
            "loss: 0.914246  [  159/ 3200]\n",
            "loss: 0.635241  [  160/ 3200]\n",
            "loss: 0.683873  [  161/ 3200]\n",
            "loss: 0.738387  [  162/ 3200]\n",
            "loss: 0.815059  [  163/ 3200]\n",
            "loss: 1.120634  [  164/ 3200]\n",
            "loss: 0.933375  [  165/ 3200]\n",
            "loss: 1.302260  [  166/ 3200]\n",
            "loss: 0.915963  [  167/ 3200]\n",
            "loss: 0.791241  [  168/ 3200]\n",
            "loss: 0.732200  [  169/ 3200]\n",
            "loss: 0.924898  [  170/ 3200]\n",
            "loss: 0.827337  [  171/ 3200]\n",
            "loss: 0.865007  [  172/ 3200]\n",
            "loss: 0.868030  [  173/ 3200]\n",
            "loss: 1.146125  [  174/ 3200]\n",
            "loss: 0.760770  [  175/ 3200]\n",
            "loss: 0.926891  [  176/ 3200]\n",
            "loss: 1.085658  [  177/ 3200]\n",
            "loss: 0.694140  [  178/ 3200]\n",
            "loss: 0.740209  [  179/ 3200]\n",
            "loss: 0.922319  [  180/ 3200]\n",
            "loss: 0.728172  [  181/ 3200]\n",
            "loss: 1.161612  [  182/ 3200]\n",
            "loss: 0.819116  [  183/ 3200]\n",
            "loss: 0.868345  [  184/ 3200]\n",
            "loss: 1.147764  [  185/ 3200]\n",
            "loss: 1.383731  [  186/ 3200]\n",
            "loss: 0.842219  [  187/ 3200]\n",
            "loss: 0.827926  [  188/ 3200]\n",
            "loss: 1.002310  [  189/ 3200]\n",
            "loss: 0.701130  [  190/ 3200]\n",
            "loss: 0.969475  [  191/ 3200]\n",
            "loss: 1.106463  [  192/ 3200]\n",
            "loss: 0.990928  [  193/ 3200]\n",
            "loss: 0.965622  [  194/ 3200]\n",
            "loss: 0.551425  [  195/ 3200]\n",
            "loss: 0.651583  [  196/ 3200]\n",
            "loss: 0.845689  [  197/ 3200]\n",
            "loss: 0.766349  [  198/ 3200]\n",
            "loss: 0.754849  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 17\n",
            "Validation Loss: 0.8067\n",
            "Validation F1: 0.6644\n",
            "Validation Accuracy: 0.6813\n",
            "Validation Confusion Matrix:\n",
            "[[ 67  43  53  37]\n",
            " [  6 175   3  16]\n",
            " [ 36   2 139  23]\n",
            " [ 16  12   8 164]]\n",
            "Epoch:  1\n",
            "loss: 0.811102  [    0/ 3200]\n",
            "loss: 0.826007  [    1/ 3200]\n",
            "loss: 0.646387  [    2/ 3200]\n",
            "loss: 1.133926  [    3/ 3200]\n",
            "loss: 0.995022  [    4/ 3200]\n",
            "loss: 0.774396  [    5/ 3200]\n",
            "loss: 0.937579  [    6/ 3200]\n",
            "loss: 0.883781  [    7/ 3200]\n",
            "loss: 0.766845  [    8/ 3200]\n",
            "loss: 0.805194  [    9/ 3200]\n",
            "loss: 0.921200  [   10/ 3200]\n",
            "loss: 1.069666  [   11/ 3200]\n",
            "loss: 1.081850  [   12/ 3200]\n",
            "loss: 1.263039  [   13/ 3200]\n",
            "loss: 0.640839  [   14/ 3200]\n",
            "loss: 0.813021  [   15/ 3200]\n",
            "loss: 1.102290  [   16/ 3200]\n",
            "loss: 1.241828  [   17/ 3200]\n",
            "loss: 0.757595  [   18/ 3200]\n",
            "loss: 0.849235  [   19/ 3200]\n",
            "loss: 0.776817  [   20/ 3200]\n",
            "loss: 0.840144  [   21/ 3200]\n",
            "loss: 1.068212  [   22/ 3200]\n",
            "loss: 0.610655  [   23/ 3200]\n",
            "loss: 0.727856  [   24/ 3200]\n",
            "loss: 0.604004  [   25/ 3200]\n",
            "loss: 0.702290  [   26/ 3200]\n",
            "loss: 0.806302  [   27/ 3200]\n",
            "loss: 0.781201  [   28/ 3200]\n",
            "loss: 0.726416  [   29/ 3200]\n",
            "loss: 0.812816  [   30/ 3200]\n",
            "loss: 1.118722  [   31/ 3200]\n",
            "loss: 0.908879  [   32/ 3200]\n",
            "loss: 0.784205  [   33/ 3200]\n",
            "loss: 1.066630  [   34/ 3200]\n",
            "loss: 0.561638  [   35/ 3200]\n",
            "loss: 0.826997  [   36/ 3200]\n",
            "loss: 0.499291  [   37/ 3200]\n",
            "loss: 0.819206  [   38/ 3200]\n",
            "loss: 0.839201  [   39/ 3200]\n",
            "loss: 0.914458  [   40/ 3200]\n",
            "loss: 0.916854  [   41/ 3200]\n",
            "loss: 0.894758  [   42/ 3200]\n",
            "loss: 1.161090  [   43/ 3200]\n",
            "loss: 1.078793  [   44/ 3200]\n",
            "loss: 1.240710  [   45/ 3200]\n",
            "loss: 1.221386  [   46/ 3200]\n",
            "loss: 1.133464  [   47/ 3200]\n",
            "loss: 0.803519  [   48/ 3200]\n",
            "loss: 0.753406  [   49/ 3200]\n",
            "loss: 0.958069  [   50/ 3200]\n",
            "loss: 1.111182  [   51/ 3200]\n",
            "loss: 1.009471  [   52/ 3200]\n",
            "loss: 1.138446  [   53/ 3200]\n",
            "loss: 0.955261  [   54/ 3200]\n",
            "loss: 0.906395  [   55/ 3200]\n",
            "loss: 0.863398  [   56/ 3200]\n",
            "loss: 0.862215  [   57/ 3200]\n",
            "loss: 0.808849  [   58/ 3200]\n",
            "loss: 0.665281  [   59/ 3200]\n",
            "loss: 1.071398  [   60/ 3200]\n",
            "loss: 0.765763  [   61/ 3200]\n",
            "loss: 0.826800  [   62/ 3200]\n",
            "loss: 0.729952  [   63/ 3200]\n",
            "loss: 0.774413  [   64/ 3200]\n",
            "loss: 1.294511  [   65/ 3200]\n",
            "loss: 0.641636  [   66/ 3200]\n",
            "loss: 0.655687  [   67/ 3200]\n",
            "loss: 1.050574  [   68/ 3200]\n",
            "loss: 1.357902  [   69/ 3200]\n",
            "loss: 0.926400  [   70/ 3200]\n",
            "loss: 1.248111  [   71/ 3200]\n",
            "loss: 0.786340  [   72/ 3200]\n",
            "loss: 0.678720  [   73/ 3200]\n",
            "loss: 0.646755  [   74/ 3200]\n",
            "loss: 1.082959  [   75/ 3200]\n",
            "loss: 0.793010  [   76/ 3200]\n",
            "loss: 0.948169  [   77/ 3200]\n",
            "loss: 0.835755  [   78/ 3200]\n",
            "loss: 0.853305  [   79/ 3200]\n",
            "loss: 0.798367  [   80/ 3200]\n",
            "loss: 0.986985  [   81/ 3200]\n",
            "loss: 0.891638  [   82/ 3200]\n",
            "loss: 1.016254  [   83/ 3200]\n",
            "loss: 1.032010  [   84/ 3200]\n",
            "loss: 0.667122  [   85/ 3200]\n",
            "loss: 0.872001  [   86/ 3200]\n",
            "loss: 0.491868  [   87/ 3200]\n",
            "loss: 0.935005  [   88/ 3200]\n",
            "loss: 1.244726  [   89/ 3200]\n",
            "loss: 0.947217  [   90/ 3200]\n",
            "loss: 0.847987  [   91/ 3200]\n",
            "loss: 1.030539  [   92/ 3200]\n",
            "loss: 0.810630  [   93/ 3200]\n",
            "loss: 1.224151  [   94/ 3200]\n",
            "loss: 0.686188  [   95/ 3200]\n",
            "loss: 0.919229  [   96/ 3200]\n",
            "loss: 0.624026  [   97/ 3200]\n",
            "loss: 1.123291  [   98/ 3200]\n",
            "loss: 0.801980  [   99/ 3200]\n",
            "loss: 0.667093  [  100/ 3200]\n",
            "loss: 1.379115  [  101/ 3200]\n",
            "loss: 0.850734  [  102/ 3200]\n",
            "loss: 0.904140  [  103/ 3200]\n",
            "loss: 1.166556  [  104/ 3200]\n",
            "loss: 0.544818  [  105/ 3200]\n",
            "loss: 0.713795  [  106/ 3200]\n",
            "loss: 0.894960  [  107/ 3200]\n",
            "loss: 0.642991  [  108/ 3200]\n",
            "loss: 1.022958  [  109/ 3200]\n",
            "loss: 0.641788  [  110/ 3200]\n",
            "loss: 0.809540  [  111/ 3200]\n",
            "loss: 1.138160  [  112/ 3200]\n",
            "loss: 0.768118  [  113/ 3200]\n",
            "loss: 1.083895  [  114/ 3200]\n",
            "loss: 1.022986  [  115/ 3200]\n",
            "loss: 1.111310  [  116/ 3200]\n",
            "loss: 0.674991  [  117/ 3200]\n",
            "loss: 0.960439  [  118/ 3200]\n",
            "loss: 0.806487  [  119/ 3200]\n",
            "loss: 1.043062  [  120/ 3200]\n",
            "loss: 0.871422  [  121/ 3200]\n",
            "loss: 1.085044  [  122/ 3200]\n",
            "loss: 0.954273  [  123/ 3200]\n",
            "loss: 0.937778  [  124/ 3200]\n",
            "loss: 1.506868  [  125/ 3200]\n",
            "loss: 1.032153  [  126/ 3200]\n",
            "loss: 0.732507  [  127/ 3200]\n",
            "loss: 0.843950  [  128/ 3200]\n",
            "loss: 0.878458  [  129/ 3200]\n",
            "loss: 0.651182  [  130/ 3200]\n",
            "loss: 0.620824  [  131/ 3200]\n",
            "loss: 0.948771  [  132/ 3200]\n",
            "loss: 0.933843  [  133/ 3200]\n",
            "loss: 1.272234  [  134/ 3200]\n",
            "loss: 0.719519  [  135/ 3200]\n",
            "loss: 0.687800  [  136/ 3200]\n",
            "loss: 0.913426  [  137/ 3200]\n",
            "loss: 0.908627  [  138/ 3200]\n",
            "loss: 0.953854  [  139/ 3200]\n",
            "loss: 0.757566  [  140/ 3200]\n",
            "loss: 0.919053  [  141/ 3200]\n",
            "loss: 0.913402  [  142/ 3200]\n",
            "loss: 0.784612  [  143/ 3200]\n",
            "loss: 0.983821  [  144/ 3200]\n",
            "loss: 1.092970  [  145/ 3200]\n",
            "loss: 0.800819  [  146/ 3200]\n",
            "loss: 0.861761  [  147/ 3200]\n",
            "loss: 1.027561  [  148/ 3200]\n",
            "loss: 0.539272  [  149/ 3200]\n",
            "loss: 0.874360  [  150/ 3200]\n",
            "loss: 1.015465  [  151/ 3200]\n",
            "loss: 0.879299  [  152/ 3200]\n",
            "loss: 0.853538  [  153/ 3200]\n",
            "loss: 0.746365  [  154/ 3200]\n",
            "loss: 1.241094  [  155/ 3200]\n",
            "loss: 1.422709  [  156/ 3200]\n",
            "loss: 0.886022  [  157/ 3200]\n",
            "loss: 0.835195  [  158/ 3200]\n",
            "loss: 0.839954  [  159/ 3200]\n",
            "loss: 0.985223  [  160/ 3200]\n",
            "loss: 0.904181  [  161/ 3200]\n",
            "loss: 0.941409  [  162/ 3200]\n",
            "loss: 0.874685  [  163/ 3200]\n",
            "loss: 0.911400  [  164/ 3200]\n",
            "loss: 0.835719  [  165/ 3200]\n",
            "loss: 0.860420  [  166/ 3200]\n",
            "loss: 0.674214  [  167/ 3200]\n",
            "loss: 0.589868  [  168/ 3200]\n",
            "loss: 0.653112  [  169/ 3200]\n",
            "loss: 0.751021  [  170/ 3200]\n",
            "loss: 1.071218  [  171/ 3200]\n",
            "loss: 0.924102  [  172/ 3200]\n",
            "loss: 0.759372  [  173/ 3200]\n",
            "loss: 0.869433  [  174/ 3200]\n",
            "loss: 0.842458  [  175/ 3200]\n",
            "loss: 0.661850  [  176/ 3200]\n",
            "loss: 0.728257  [  177/ 3200]\n",
            "loss: 0.942289  [  178/ 3200]\n",
            "loss: 0.953696  [  179/ 3200]\n",
            "loss: 1.197929  [  180/ 3200]\n",
            "loss: 1.073152  [  181/ 3200]\n",
            "loss: 0.714919  [  182/ 3200]\n",
            "loss: 0.483327  [  183/ 3200]\n",
            "loss: 0.468873  [  184/ 3200]\n",
            "loss: 0.877029  [  185/ 3200]\n",
            "loss: 0.737374  [  186/ 3200]\n",
            "loss: 0.875275  [  187/ 3200]\n",
            "loss: 0.969588  [  188/ 3200]\n",
            "loss: 1.064720  [  189/ 3200]\n",
            "loss: 0.566014  [  190/ 3200]\n",
            "loss: 1.217704  [  191/ 3200]\n",
            "loss: 0.703831  [  192/ 3200]\n",
            "loss: 0.913600  [  193/ 3200]\n",
            "loss: 1.086840  [  194/ 3200]\n",
            "loss: 0.993529  [  195/ 3200]\n",
            "loss: 0.878420  [  196/ 3200]\n",
            "loss: 0.679077  [  197/ 3200]\n",
            "loss: 0.914605  [  198/ 3200]\n",
            "loss: 0.994682  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 18\n",
            "Validation Loss: 0.8642\n",
            "Validation F1: 0.6258\n",
            "Validation Accuracy: 0.6562\n",
            "Validation Confusion Matrix:\n",
            "[[ 50  73  17  60]\n",
            " [  1 186   3  10]\n",
            " [ 36  11 113  40]\n",
            " [  3  16   5 176]]\n",
            "Epoch:  1\n",
            "loss: 0.802861  [    0/ 3200]\n",
            "loss: 0.860639  [    1/ 3200]\n",
            "loss: 1.222139  [    2/ 3200]\n",
            "loss: 0.798457  [    3/ 3200]\n",
            "loss: 1.051191  [    4/ 3200]\n",
            "loss: 1.224073  [    5/ 3200]\n",
            "loss: 0.964201  [    6/ 3200]\n",
            "loss: 0.830890  [    7/ 3200]\n",
            "loss: 0.820093  [    8/ 3200]\n",
            "loss: 0.860009  [    9/ 3200]\n",
            "loss: 0.943261  [   10/ 3200]\n",
            "loss: 0.961052  [   11/ 3200]\n",
            "loss: 1.150170  [   12/ 3200]\n",
            "loss: 0.805672  [   13/ 3200]\n",
            "loss: 1.431481  [   14/ 3200]\n",
            "loss: 1.013575  [   15/ 3200]\n",
            "loss: 1.145224  [   16/ 3200]\n",
            "loss: 0.900361  [   17/ 3200]\n",
            "loss: 1.044401  [   18/ 3200]\n",
            "loss: 0.760347  [   19/ 3200]\n",
            "loss: 0.810443  [   20/ 3200]\n",
            "loss: 0.925851  [   21/ 3200]\n",
            "loss: 0.782396  [   22/ 3200]\n",
            "loss: 0.761342  [   23/ 3200]\n",
            "loss: 0.808713  [   24/ 3200]\n",
            "loss: 0.789136  [   25/ 3200]\n",
            "loss: 0.590073  [   26/ 3200]\n",
            "loss: 1.058679  [   27/ 3200]\n",
            "loss: 0.868870  [   28/ 3200]\n",
            "loss: 0.867699  [   29/ 3200]\n",
            "loss: 0.809328  [   30/ 3200]\n",
            "loss: 0.778545  [   31/ 3200]\n",
            "loss: 0.928208  [   32/ 3200]\n",
            "loss: 0.980777  [   33/ 3200]\n",
            "loss: 1.070066  [   34/ 3200]\n",
            "loss: 0.600340  [   35/ 3200]\n",
            "loss: 0.653985  [   36/ 3200]\n",
            "loss: 0.975414  [   37/ 3200]\n",
            "loss: 0.972573  [   38/ 3200]\n",
            "loss: 0.912209  [   39/ 3200]\n",
            "loss: 1.196878  [   40/ 3200]\n",
            "loss: 0.924101  [   41/ 3200]\n",
            "loss: 1.141986  [   42/ 3200]\n",
            "loss: 0.767181  [   43/ 3200]\n",
            "loss: 1.315264  [   44/ 3200]\n",
            "loss: 0.958842  [   45/ 3200]\n",
            "loss: 0.896760  [   46/ 3200]\n",
            "loss: 0.995012  [   47/ 3200]\n",
            "loss: 1.143681  [   48/ 3200]\n",
            "loss: 0.947244  [   49/ 3200]\n",
            "loss: 1.157947  [   50/ 3200]\n",
            "loss: 0.716079  [   51/ 3200]\n",
            "loss: 0.921404  [   52/ 3200]\n",
            "loss: 0.913048  [   53/ 3200]\n",
            "loss: 0.994001  [   54/ 3200]\n",
            "loss: 0.817078  [   55/ 3200]\n",
            "loss: 0.677422  [   56/ 3200]\n",
            "loss: 0.931550  [   57/ 3200]\n",
            "loss: 0.799293  [   58/ 3200]\n",
            "loss: 0.833068  [   59/ 3200]\n",
            "loss: 1.058556  [   60/ 3200]\n",
            "loss: 0.926678  [   61/ 3200]\n",
            "loss: 0.680593  [   62/ 3200]\n",
            "loss: 0.943888  [   63/ 3200]\n",
            "loss: 1.033185  [   64/ 3200]\n",
            "loss: 0.750758  [   65/ 3200]\n",
            "loss: 0.793953  [   66/ 3200]\n",
            "loss: 0.754261  [   67/ 3200]\n",
            "loss: 0.814080  [   68/ 3200]\n",
            "loss: 0.485398  [   69/ 3200]\n",
            "loss: 0.801792  [   70/ 3200]\n",
            "loss: 0.936655  [   71/ 3200]\n",
            "loss: 0.794472  [   72/ 3200]\n",
            "loss: 0.893454  [   73/ 3200]\n",
            "loss: 0.995621  [   74/ 3200]\n",
            "loss: 1.128342  [   75/ 3200]\n",
            "loss: 0.806677  [   76/ 3200]\n",
            "loss: 0.674624  [   77/ 3200]\n",
            "loss: 0.845882  [   78/ 3200]\n",
            "loss: 0.751124  [   79/ 3200]\n",
            "loss: 0.986467  [   80/ 3200]\n",
            "loss: 0.611441  [   81/ 3200]\n",
            "loss: 0.926458  [   82/ 3200]\n",
            "loss: 0.855884  [   83/ 3200]\n",
            "loss: 1.034628  [   84/ 3200]\n",
            "loss: 1.245140  [   85/ 3200]\n",
            "loss: 0.817605  [   86/ 3200]\n",
            "loss: 0.837309  [   87/ 3200]\n",
            "loss: 0.849910  [   88/ 3200]\n",
            "loss: 1.072201  [   89/ 3200]\n",
            "loss: 1.051192  [   90/ 3200]\n",
            "loss: 0.971156  [   91/ 3200]\n",
            "loss: 0.799297  [   92/ 3200]\n",
            "loss: 0.866697  [   93/ 3200]\n",
            "loss: 0.468881  [   94/ 3200]\n",
            "loss: 0.558420  [   95/ 3200]\n",
            "loss: 1.029023  [   96/ 3200]\n",
            "loss: 1.011593  [   97/ 3200]\n",
            "loss: 0.558636  [   98/ 3200]\n",
            "loss: 0.512003  [   99/ 3200]\n",
            "loss: 0.918707  [  100/ 3200]\n",
            "loss: 1.067178  [  101/ 3200]\n",
            "loss: 0.597090  [  102/ 3200]\n",
            "loss: 1.163373  [  103/ 3200]\n",
            "loss: 0.615481  [  104/ 3200]\n",
            "loss: 1.036445  [  105/ 3200]\n",
            "loss: 0.865210  [  106/ 3200]\n",
            "loss: 0.677464  [  107/ 3200]\n",
            "loss: 1.091422  [  108/ 3200]\n",
            "loss: 0.871799  [  109/ 3200]\n",
            "loss: 1.059919  [  110/ 3200]\n",
            "loss: 0.712494  [  111/ 3200]\n",
            "loss: 0.832926  [  112/ 3200]\n",
            "loss: 1.124089  [  113/ 3200]\n",
            "loss: 1.142571  [  114/ 3200]\n",
            "loss: 1.204973  [  115/ 3200]\n",
            "loss: 0.603379  [  116/ 3200]\n",
            "loss: 0.895447  [  117/ 3200]\n",
            "loss: 0.804010  [  118/ 3200]\n",
            "loss: 1.087346  [  119/ 3200]\n",
            "loss: 0.979173  [  120/ 3200]\n",
            "loss: 1.005925  [  121/ 3200]\n",
            "loss: 0.760273  [  122/ 3200]\n",
            "loss: 0.726022  [  123/ 3200]\n",
            "loss: 0.835336  [  124/ 3200]\n",
            "loss: 0.811913  [  125/ 3200]\n",
            "loss: 0.946987  [  126/ 3200]\n",
            "loss: 0.939695  [  127/ 3200]\n",
            "loss: 1.033608  [  128/ 3200]\n",
            "loss: 0.716311  [  129/ 3200]\n",
            "loss: 1.124137  [  130/ 3200]\n",
            "loss: 0.550648  [  131/ 3200]\n",
            "loss: 0.649694  [  132/ 3200]\n",
            "loss: 0.646656  [  133/ 3200]\n",
            "loss: 0.843732  [  134/ 3200]\n",
            "loss: 1.283074  [  135/ 3200]\n",
            "loss: 0.997715  [  136/ 3200]\n",
            "loss: 0.957169  [  137/ 3200]\n",
            "loss: 0.846830  [  138/ 3200]\n",
            "loss: 0.830283  [  139/ 3200]\n",
            "loss: 1.240887  [  140/ 3200]\n",
            "loss: 0.510702  [  141/ 3200]\n",
            "loss: 0.772612  [  142/ 3200]\n",
            "loss: 1.091956  [  143/ 3200]\n",
            "loss: 0.932062  [  144/ 3200]\n",
            "loss: 0.734730  [  145/ 3200]\n",
            "loss: 0.987678  [  146/ 3200]\n",
            "loss: 0.793617  [  147/ 3200]\n",
            "loss: 1.059561  [  148/ 3200]\n",
            "loss: 0.770588  [  149/ 3200]\n",
            "loss: 0.532400  [  150/ 3200]\n",
            "loss: 1.089568  [  151/ 3200]\n",
            "loss: 0.899162  [  152/ 3200]\n",
            "loss: 0.921164  [  153/ 3200]\n",
            "loss: 0.855912  [  154/ 3200]\n",
            "loss: 1.025928  [  155/ 3200]\n",
            "loss: 0.801158  [  156/ 3200]\n",
            "loss: 0.718087  [  157/ 3200]\n",
            "loss: 0.741879  [  158/ 3200]\n",
            "loss: 0.924583  [  159/ 3200]\n",
            "loss: 0.752899  [  160/ 3200]\n",
            "loss: 0.784571  [  161/ 3200]\n",
            "loss: 0.812907  [  162/ 3200]\n",
            "loss: 0.925172  [  163/ 3200]\n",
            "loss: 0.605751  [  164/ 3200]\n",
            "loss: 0.576141  [  165/ 3200]\n",
            "loss: 1.191522  [  166/ 3200]\n",
            "loss: 0.691812  [  167/ 3200]\n",
            "loss: 1.189650  [  168/ 3200]\n",
            "loss: 0.767191  [  169/ 3200]\n",
            "loss: 0.769350  [  170/ 3200]\n",
            "loss: 0.861187  [  171/ 3200]\n",
            "loss: 0.991905  [  172/ 3200]\n",
            "loss: 1.090330  [  173/ 3200]\n",
            "loss: 1.141725  [  174/ 3200]\n",
            "loss: 0.619545  [  175/ 3200]\n",
            "loss: 0.928340  [  176/ 3200]\n",
            "loss: 1.061190  [  177/ 3200]\n",
            "loss: 1.051267  [  178/ 3200]\n",
            "loss: 1.072512  [  179/ 3200]\n",
            "loss: 0.828302  [  180/ 3200]\n",
            "loss: 0.777232  [  181/ 3200]\n",
            "loss: 0.693003  [  182/ 3200]\n",
            "loss: 0.852894  [  183/ 3200]\n",
            "loss: 1.024384  [  184/ 3200]\n",
            "loss: 1.055553  [  185/ 3200]\n",
            "loss: 1.064502  [  186/ 3200]\n",
            "loss: 0.819658  [  187/ 3200]\n",
            "loss: 0.810617  [  188/ 3200]\n",
            "loss: 0.606691  [  189/ 3200]\n",
            "loss: 0.838983  [  190/ 3200]\n",
            "loss: 1.029644  [  191/ 3200]\n",
            "loss: 0.975357  [  192/ 3200]\n",
            "loss: 1.112135  [  193/ 3200]\n",
            "loss: 0.813163  [  194/ 3200]\n",
            "loss: 0.945387  [  195/ 3200]\n",
            "loss: 0.885220  [  196/ 3200]\n",
            "loss: 1.036622  [  197/ 3200]\n",
            "loss: 0.639550  [  198/ 3200]\n",
            "loss: 0.721094  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 19\n",
            "Validation Loss: 0.8241\n",
            "Validation F1: 0.6577\n",
            "Validation Accuracy: 0.6713\n",
            "Validation Confusion Matrix:\n",
            "[[ 66  44  75  15]\n",
            " [ 10 183   4   3]\n",
            " [ 32   2 160   6]\n",
            " [ 37  17  18 128]]\n",
            "Epoch:  1\n",
            "loss: 0.944245  [    0/ 3200]\n",
            "loss: 0.820747  [    1/ 3200]\n",
            "loss: 0.979367  [    2/ 3200]\n",
            "loss: 1.144784  [    3/ 3200]\n",
            "loss: 0.842546  [    4/ 3200]\n",
            "loss: 0.788237  [    5/ 3200]\n",
            "loss: 0.578976  [    6/ 3200]\n",
            "loss: 1.179747  [    7/ 3200]\n",
            "loss: 1.084507  [    8/ 3200]\n",
            "loss: 0.788009  [    9/ 3200]\n",
            "loss: 0.802841  [   10/ 3200]\n",
            "loss: 1.019061  [   11/ 3200]\n",
            "loss: 1.052670  [   12/ 3200]\n",
            "loss: 0.830938  [   13/ 3200]\n",
            "loss: 1.137848  [   14/ 3200]\n",
            "loss: 0.714851  [   15/ 3200]\n",
            "loss: 0.828254  [   16/ 3200]\n",
            "loss: 1.083872  [   17/ 3200]\n",
            "loss: 1.116129  [   18/ 3200]\n",
            "loss: 0.714419  [   19/ 3200]\n",
            "loss: 1.172331  [   20/ 3200]\n",
            "loss: 1.229501  [   21/ 3200]\n",
            "loss: 1.062145  [   22/ 3200]\n",
            "loss: 0.914151  [   23/ 3200]\n",
            "loss: 0.817282  [   24/ 3200]\n",
            "loss: 0.776676  [   25/ 3200]\n",
            "loss: 0.933238  [   26/ 3200]\n",
            "loss: 0.759195  [   27/ 3200]\n",
            "loss: 0.831864  [   28/ 3200]\n",
            "loss: 0.934537  [   29/ 3200]\n",
            "loss: 0.811316  [   30/ 3200]\n",
            "loss: 0.915016  [   31/ 3200]\n",
            "loss: 0.797299  [   32/ 3200]\n",
            "loss: 1.061105  [   33/ 3200]\n",
            "loss: 0.749030  [   34/ 3200]\n",
            "loss: 0.884738  [   35/ 3200]\n",
            "loss: 0.713628  [   36/ 3200]\n",
            "loss: 0.708120  [   37/ 3200]\n",
            "loss: 0.868239  [   38/ 3200]\n",
            "loss: 0.672183  [   39/ 3200]\n",
            "loss: 1.700527  [   40/ 3200]\n",
            "loss: 1.042055  [   41/ 3200]\n",
            "loss: 0.791830  [   42/ 3200]\n",
            "loss: 0.762451  [   43/ 3200]\n",
            "loss: 0.696234  [   44/ 3200]\n",
            "loss: 0.991624  [   45/ 3200]\n",
            "loss: 0.588037  [   46/ 3200]\n",
            "loss: 1.177463  [   47/ 3200]\n",
            "loss: 0.955127  [   48/ 3200]\n",
            "loss: 1.108920  [   49/ 3200]\n",
            "loss: 1.114521  [   50/ 3200]\n",
            "loss: 0.845327  [   51/ 3200]\n",
            "loss: 1.045750  [   52/ 3200]\n",
            "loss: 0.794864  [   53/ 3200]\n",
            "loss: 1.213749  [   54/ 3200]\n",
            "loss: 0.692515  [   55/ 3200]\n",
            "loss: 0.940300  [   56/ 3200]\n",
            "loss: 0.686840  [   57/ 3200]\n",
            "loss: 0.791033  [   58/ 3200]\n",
            "loss: 1.144961  [   59/ 3200]\n",
            "loss: 0.934968  [   60/ 3200]\n",
            "loss: 1.021097  [   61/ 3200]\n",
            "loss: 1.008133  [   62/ 3200]\n",
            "loss: 1.058394  [   63/ 3200]\n",
            "loss: 0.824114  [   64/ 3200]\n",
            "loss: 0.868536  [   65/ 3200]\n",
            "loss: 0.760933  [   66/ 3200]\n",
            "loss: 0.672319  [   67/ 3200]\n",
            "loss: 0.607197  [   68/ 3200]\n",
            "loss: 1.075049  [   69/ 3200]\n",
            "loss: 0.741059  [   70/ 3200]\n",
            "loss: 1.137935  [   71/ 3200]\n",
            "loss: 1.007269  [   72/ 3200]\n",
            "loss: 0.895285  [   73/ 3200]\n",
            "loss: 0.934935  [   74/ 3200]\n",
            "loss: 1.281191  [   75/ 3200]\n",
            "loss: 0.831305  [   76/ 3200]\n",
            "loss: 0.721377  [   77/ 3200]\n",
            "loss: 0.813236  [   78/ 3200]\n",
            "loss: 0.951475  [   79/ 3200]\n",
            "loss: 0.867082  [   80/ 3200]\n",
            "loss: 0.822901  [   81/ 3200]\n",
            "loss: 0.930266  [   82/ 3200]\n",
            "loss: 0.781860  [   83/ 3200]\n",
            "loss: 0.702682  [   84/ 3200]\n",
            "loss: 0.684453  [   85/ 3200]\n",
            "loss: 0.633551  [   86/ 3200]\n",
            "loss: 1.331583  [   87/ 3200]\n",
            "loss: 0.789889  [   88/ 3200]\n",
            "loss: 0.793049  [   89/ 3200]\n",
            "loss: 0.746094  [   90/ 3200]\n",
            "loss: 0.678451  [   91/ 3200]\n",
            "loss: 1.037440  [   92/ 3200]\n",
            "loss: 0.704907  [   93/ 3200]\n",
            "loss: 0.700120  [   94/ 3200]\n",
            "loss: 0.795551  [   95/ 3200]\n",
            "loss: 0.860523  [   96/ 3200]\n",
            "loss: 0.612890  [   97/ 3200]\n",
            "loss: 0.963511  [   98/ 3200]\n",
            "loss: 0.839007  [   99/ 3200]\n",
            "loss: 0.652827  [  100/ 3200]\n",
            "loss: 1.214551  [  101/ 3200]\n",
            "loss: 0.891204  [  102/ 3200]\n",
            "loss: 0.595732  [  103/ 3200]\n",
            "loss: 1.011152  [  104/ 3200]\n",
            "loss: 1.070674  [  105/ 3200]\n",
            "loss: 0.839249  [  106/ 3200]\n",
            "loss: 1.115568  [  107/ 3200]\n",
            "loss: 1.039996  [  108/ 3200]\n",
            "loss: 1.077785  [  109/ 3200]\n",
            "loss: 0.864688  [  110/ 3200]\n",
            "loss: 0.928312  [  111/ 3200]\n",
            "loss: 0.764355  [  112/ 3200]\n",
            "loss: 0.832579  [  113/ 3200]\n",
            "loss: 1.017090  [  114/ 3200]\n",
            "loss: 0.885602  [  115/ 3200]\n",
            "loss: 0.827219  [  116/ 3200]\n",
            "loss: 0.746536  [  117/ 3200]\n",
            "loss: 0.598148  [  118/ 3200]\n",
            "loss: 1.061763  [  119/ 3200]\n",
            "loss: 1.070887  [  120/ 3200]\n",
            "loss: 0.921972  [  121/ 3200]\n",
            "loss: 0.934333  [  122/ 3200]\n",
            "loss: 0.789139  [  123/ 3200]\n",
            "loss: 0.539637  [  124/ 3200]\n",
            "loss: 0.905834  [  125/ 3200]\n",
            "loss: 1.021619  [  126/ 3200]\n",
            "loss: 0.683615  [  127/ 3200]\n",
            "loss: 0.619938  [  128/ 3200]\n",
            "loss: 1.189083  [  129/ 3200]\n",
            "loss: 0.976197  [  130/ 3200]\n",
            "loss: 0.956728  [  131/ 3200]\n",
            "loss: 1.048034  [  132/ 3200]\n",
            "loss: 0.974962  [  133/ 3200]\n",
            "loss: 0.718153  [  134/ 3200]\n",
            "loss: 1.076308  [  135/ 3200]\n",
            "loss: 0.708221  [  136/ 3200]\n",
            "loss: 0.605853  [  137/ 3200]\n",
            "loss: 1.204312  [  138/ 3200]\n",
            "loss: 0.900488  [  139/ 3200]\n",
            "loss: 0.921994  [  140/ 3200]\n",
            "loss: 0.880015  [  141/ 3200]\n",
            "loss: 1.049415  [  142/ 3200]\n",
            "loss: 1.247804  [  143/ 3200]\n",
            "loss: 0.783408  [  144/ 3200]\n",
            "loss: 0.929231  [  145/ 3200]\n",
            "loss: 0.781878  [  146/ 3200]\n",
            "loss: 1.171432  [  147/ 3200]\n",
            "loss: 0.920489  [  148/ 3200]\n",
            "loss: 0.890115  [  149/ 3200]\n",
            "loss: 0.730788  [  150/ 3200]\n",
            "loss: 0.781389  [  151/ 3200]\n",
            "loss: 0.965778  [  152/ 3200]\n",
            "loss: 0.938742  [  153/ 3200]\n",
            "loss: 0.876988  [  154/ 3200]\n",
            "loss: 0.809869  [  155/ 3200]\n",
            "loss: 1.041846  [  156/ 3200]\n",
            "loss: 1.003134  [  157/ 3200]\n",
            "loss: 0.933393  [  158/ 3200]\n",
            "loss: 0.623217  [  159/ 3200]\n",
            "loss: 0.942517  [  160/ 3200]\n",
            "loss: 0.899659  [  161/ 3200]\n",
            "loss: 1.040832  [  162/ 3200]\n",
            "loss: 0.648999  [  163/ 3200]\n",
            "loss: 0.794915  [  164/ 3200]\n",
            "loss: 0.974129  [  165/ 3200]\n",
            "loss: 0.605617  [  166/ 3200]\n",
            "loss: 0.517265  [  167/ 3200]\n",
            "loss: 1.156053  [  168/ 3200]\n",
            "loss: 1.518267  [  169/ 3200]\n",
            "loss: 0.704307  [  170/ 3200]\n",
            "loss: 1.169533  [  171/ 3200]\n",
            "loss: 0.712905  [  172/ 3200]\n",
            "loss: 1.114481  [  173/ 3200]\n",
            "loss: 0.929001  [  174/ 3200]\n",
            "loss: 1.183843  [  175/ 3200]\n",
            "loss: 1.067305  [  176/ 3200]\n",
            "loss: 1.040118  [  177/ 3200]\n",
            "loss: 0.519076  [  178/ 3200]\n",
            "loss: 0.841127  [  179/ 3200]\n",
            "loss: 1.073674  [  180/ 3200]\n",
            "loss: 0.800091  [  181/ 3200]\n",
            "loss: 0.707320  [  182/ 3200]\n",
            "loss: 0.739036  [  183/ 3200]\n",
            "loss: 0.548680  [  184/ 3200]\n",
            "loss: 0.878004  [  185/ 3200]\n",
            "loss: 0.768737  [  186/ 3200]\n",
            "loss: 0.690310  [  187/ 3200]\n",
            "loss: 0.755571  [  188/ 3200]\n",
            "loss: 0.694726  [  189/ 3200]\n",
            "loss: 0.996001  [  190/ 3200]\n",
            "loss: 0.805416  [  191/ 3200]\n",
            "loss: 0.913075  [  192/ 3200]\n",
            "loss: 0.507482  [  193/ 3200]\n",
            "loss: 0.789816  [  194/ 3200]\n",
            "loss: 0.617241  [  195/ 3200]\n",
            "loss: 0.960986  [  196/ 3200]\n",
            "loss: 0.967424  [  197/ 3200]\n",
            "loss: 0.977589  [  198/ 3200]\n",
            "loss: 0.856469  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 20\n",
            "Validation Loss: 0.8087\n",
            "Validation F1: 0.6814\n",
            "Validation Accuracy: 0.6887\n",
            "Validation Confusion Matrix:\n",
            "[[ 82  44  55  19]\n",
            " [ 10 182   3   5]\n",
            " [ 45   2 144   9]\n",
            " [ 32  16   9 143]]\n",
            "Epoch:  1\n",
            "loss: 0.788339  [    0/ 3200]\n",
            "loss: 0.872292  [    1/ 3200]\n",
            "loss: 0.997023  [    2/ 3200]\n",
            "loss: 0.982527  [    3/ 3200]\n",
            "loss: 0.832263  [    4/ 3200]\n",
            "loss: 1.007995  [    5/ 3200]\n",
            "loss: 0.748480  [    6/ 3200]\n",
            "loss: 0.898783  [    7/ 3200]\n",
            "loss: 1.300732  [    8/ 3200]\n",
            "loss: 1.147444  [    9/ 3200]\n",
            "loss: 0.553585  [   10/ 3200]\n",
            "loss: 1.465613  [   11/ 3200]\n",
            "loss: 0.925096  [   12/ 3200]\n",
            "loss: 0.978965  [   13/ 3200]\n",
            "loss: 1.227518  [   14/ 3200]\n",
            "loss: 0.940090  [   15/ 3200]\n",
            "loss: 1.026426  [   16/ 3200]\n",
            "loss: 0.780651  [   17/ 3200]\n",
            "loss: 1.065754  [   18/ 3200]\n",
            "loss: 0.628919  [   19/ 3200]\n",
            "loss: 0.948740  [   20/ 3200]\n",
            "loss: 0.770988  [   21/ 3200]\n",
            "loss: 0.819377  [   22/ 3200]\n",
            "loss: 0.953592  [   23/ 3200]\n",
            "loss: 0.774170  [   24/ 3200]\n",
            "loss: 0.639185  [   25/ 3200]\n",
            "loss: 0.940875  [   26/ 3200]\n",
            "loss: 1.082678  [   27/ 3200]\n",
            "loss: 1.063333  [   28/ 3200]\n",
            "loss: 0.879235  [   29/ 3200]\n",
            "loss: 0.770217  [   30/ 3200]\n",
            "loss: 1.154910  [   31/ 3200]\n",
            "loss: 1.134802  [   32/ 3200]\n",
            "loss: 0.763016  [   33/ 3200]\n",
            "loss: 1.027022  [   34/ 3200]\n",
            "loss: 0.838446  [   35/ 3200]\n",
            "loss: 0.574975  [   36/ 3200]\n",
            "loss: 0.763682  [   37/ 3200]\n",
            "loss: 0.922463  [   38/ 3200]\n",
            "loss: 0.759443  [   39/ 3200]\n",
            "loss: 1.398804  [   40/ 3200]\n",
            "loss: 1.181076  [   41/ 3200]\n",
            "loss: 0.865820  [   42/ 3200]\n",
            "loss: 0.492885  [   43/ 3200]\n",
            "loss: 0.995934  [   44/ 3200]\n",
            "loss: 1.018845  [   45/ 3200]\n",
            "loss: 0.574450  [   46/ 3200]\n",
            "loss: 1.012923  [   47/ 3200]\n",
            "loss: 0.813794  [   48/ 3200]\n",
            "loss: 0.684803  [   49/ 3200]\n",
            "loss: 0.702019  [   50/ 3200]\n",
            "loss: 0.856304  [   51/ 3200]\n",
            "loss: 0.963168  [   52/ 3200]\n",
            "loss: 0.746997  [   53/ 3200]\n",
            "loss: 0.993776  [   54/ 3200]\n",
            "loss: 0.595462  [   55/ 3200]\n",
            "loss: 0.988416  [   56/ 3200]\n",
            "loss: 1.138596  [   57/ 3200]\n",
            "loss: 0.726637  [   58/ 3200]\n",
            "loss: 0.947258  [   59/ 3200]\n",
            "loss: 0.708547  [   60/ 3200]\n",
            "loss: 0.838088  [   61/ 3200]\n",
            "loss: 0.566949  [   62/ 3200]\n",
            "loss: 1.037847  [   63/ 3200]\n",
            "loss: 0.709081  [   64/ 3200]\n",
            "loss: 1.302368  [   65/ 3200]\n",
            "loss: 1.530745  [   66/ 3200]\n",
            "loss: 0.858339  [   67/ 3200]\n",
            "loss: 0.861106  [   68/ 3200]\n",
            "loss: 0.796337  [   69/ 3200]\n",
            "loss: 1.063492  [   70/ 3200]\n",
            "loss: 0.702034  [   71/ 3200]\n",
            "loss: 0.912053  [   72/ 3200]\n",
            "loss: 0.869503  [   73/ 3200]\n",
            "loss: 0.797671  [   74/ 3200]\n",
            "loss: 0.754095  [   75/ 3200]\n",
            "loss: 0.678893  [   76/ 3200]\n",
            "loss: 1.316335  [   77/ 3200]\n",
            "loss: 0.661029  [   78/ 3200]\n",
            "loss: 0.805082  [   79/ 3200]\n",
            "loss: 0.558655  [   80/ 3200]\n",
            "loss: 0.955513  [   81/ 3200]\n",
            "loss: 0.637848  [   82/ 3200]\n",
            "loss: 0.620511  [   83/ 3200]\n",
            "loss: 1.055192  [   84/ 3200]\n",
            "loss: 0.774689  [   85/ 3200]\n",
            "loss: 1.133326  [   86/ 3200]\n",
            "loss: 0.786709  [   87/ 3200]\n",
            "loss: 0.751666  [   88/ 3200]\n",
            "loss: 0.773885  [   89/ 3200]\n",
            "loss: 0.718210  [   90/ 3200]\n",
            "loss: 1.089680  [   91/ 3200]\n",
            "loss: 0.976954  [   92/ 3200]\n",
            "loss: 0.888197  [   93/ 3200]\n",
            "loss: 0.757993  [   94/ 3200]\n",
            "loss: 1.038285  [   95/ 3200]\n",
            "loss: 0.951545  [   96/ 3200]\n",
            "loss: 0.996813  [   97/ 3200]\n",
            "loss: 1.083343  [   98/ 3200]\n",
            "loss: 0.909507  [   99/ 3200]\n",
            "loss: 0.654334  [  100/ 3200]\n",
            "loss: 0.788841  [  101/ 3200]\n",
            "loss: 1.101326  [  102/ 3200]\n",
            "loss: 0.723165  [  103/ 3200]\n",
            "loss: 0.808128  [  104/ 3200]\n",
            "loss: 1.446506  [  105/ 3200]\n",
            "loss: 1.296757  [  106/ 3200]\n",
            "loss: 0.794801  [  107/ 3200]\n",
            "loss: 1.198896  [  108/ 3200]\n",
            "loss: 0.702802  [  109/ 3200]\n",
            "loss: 0.918397  [  110/ 3200]\n",
            "loss: 0.884625  [  111/ 3200]\n",
            "loss: 0.702899  [  112/ 3200]\n",
            "loss: 1.047003  [  113/ 3200]\n",
            "loss: 0.932986  [  114/ 3200]\n",
            "loss: 0.567766  [  115/ 3200]\n",
            "loss: 0.862496  [  116/ 3200]\n",
            "loss: 0.810102  [  117/ 3200]\n",
            "loss: 1.147391  [  118/ 3200]\n",
            "loss: 0.862572  [  119/ 3200]\n",
            "loss: 0.813035  [  120/ 3200]\n",
            "loss: 0.992590  [  121/ 3200]\n",
            "loss: 0.809357  [  122/ 3200]\n",
            "loss: 0.741059  [  123/ 3200]\n",
            "loss: 1.399193  [  124/ 3200]\n",
            "loss: 0.939282  [  125/ 3200]\n",
            "loss: 1.051412  [  126/ 3200]\n",
            "loss: 1.012225  [  127/ 3200]\n",
            "loss: 0.917926  [  128/ 3200]\n",
            "loss: 1.056954  [  129/ 3200]\n",
            "loss: 0.849132  [  130/ 3200]\n",
            "loss: 0.698370  [  131/ 3200]\n",
            "loss: 1.144490  [  132/ 3200]\n",
            "loss: 0.757628  [  133/ 3200]\n",
            "loss: 0.888461  [  134/ 3200]\n",
            "loss: 0.655491  [  135/ 3200]\n",
            "loss: 0.739239  [  136/ 3200]\n",
            "loss: 0.740454  [  137/ 3200]\n",
            "loss: 0.780363  [  138/ 3200]\n",
            "loss: 1.050119  [  139/ 3200]\n",
            "loss: 0.756810  [  140/ 3200]\n",
            "loss: 1.003384  [  141/ 3200]\n",
            "loss: 0.898659  [  142/ 3200]\n",
            "loss: 0.949764  [  143/ 3200]\n",
            "loss: 0.719555  [  144/ 3200]\n",
            "loss: 0.672092  [  145/ 3200]\n",
            "loss: 0.805234  [  146/ 3200]\n",
            "loss: 1.384012  [  147/ 3200]\n",
            "loss: 0.717499  [  148/ 3200]\n",
            "loss: 0.851141  [  149/ 3200]\n",
            "loss: 0.946202  [  150/ 3200]\n",
            "loss: 0.894124  [  151/ 3200]\n",
            "loss: 0.820387  [  152/ 3200]\n",
            "loss: 1.462565  [  153/ 3200]\n",
            "loss: 0.635476  [  154/ 3200]\n",
            "loss: 0.870438  [  155/ 3200]\n",
            "loss: 0.985715  [  156/ 3200]\n",
            "loss: 0.929880  [  157/ 3200]\n",
            "loss: 0.708668  [  158/ 3200]\n",
            "loss: 0.804572  [  159/ 3200]\n",
            "loss: 0.799959  [  160/ 3200]\n",
            "loss: 1.091107  [  161/ 3200]\n",
            "loss: 1.075083  [  162/ 3200]\n",
            "loss: 0.697612  [  163/ 3200]\n",
            "loss: 0.763890  [  164/ 3200]\n",
            "loss: 0.696671  [  165/ 3200]\n",
            "loss: 0.873390  [  166/ 3200]\n",
            "loss: 0.606806  [  167/ 3200]\n",
            "loss: 1.618668  [  168/ 3200]\n",
            "loss: 1.055050  [  169/ 3200]\n",
            "loss: 0.868141  [  170/ 3200]\n",
            "loss: 0.987259  [  171/ 3200]\n",
            "loss: 0.899517  [  172/ 3200]\n",
            "loss: 0.897120  [  173/ 3200]\n",
            "loss: 0.753844  [  174/ 3200]\n",
            "loss: 1.198284  [  175/ 3200]\n",
            "loss: 1.121217  [  176/ 3200]\n",
            "loss: 0.664914  [  177/ 3200]\n",
            "loss: 0.800554  [  178/ 3200]\n",
            "loss: 0.963889  [  179/ 3200]\n",
            "loss: 0.637161  [  180/ 3200]\n",
            "loss: 0.932955  [  181/ 3200]\n",
            "loss: 0.910788  [  182/ 3200]\n",
            "loss: 1.152895  [  183/ 3200]\n",
            "loss: 1.109662  [  184/ 3200]\n",
            "loss: 0.994115  [  185/ 3200]\n",
            "loss: 1.054769  [  186/ 3200]\n",
            "loss: 0.584724  [  187/ 3200]\n",
            "loss: 0.907308  [  188/ 3200]\n",
            "loss: 1.173295  [  189/ 3200]\n",
            "loss: 0.939529  [  190/ 3200]\n",
            "loss: 0.888976  [  191/ 3200]\n",
            "loss: 0.567271  [  192/ 3200]\n",
            "loss: 0.708160  [  193/ 3200]\n",
            "loss: 0.865615  [  194/ 3200]\n",
            "loss: 0.928088  [  195/ 3200]\n",
            "loss: 0.744714  [  196/ 3200]\n",
            "loss: 0.798718  [  197/ 3200]\n",
            "loss: 0.778578  [  198/ 3200]\n",
            "loss: 0.461531  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 21\n",
            "Validation Loss: 0.8546\n",
            "Validation F1: 0.6366\n",
            "Validation Accuracy: 0.6575\n",
            "Validation Confusion Matrix:\n",
            "[[ 66  59  15  60]\n",
            " [  3 178   3  16]\n",
            " [ 46   7 105  42]\n",
            " [  4  14   5 177]]\n",
            "Epoch:  1\n",
            "loss: 0.933466  [    0/ 3200]\n",
            "loss: 0.743357  [    1/ 3200]\n",
            "loss: 1.010143  [    2/ 3200]\n",
            "loss: 0.681498  [    3/ 3200]\n",
            "loss: 0.946317  [    4/ 3200]\n",
            "loss: 0.962076  [    5/ 3200]\n",
            "loss: 0.979861  [    6/ 3200]\n",
            "loss: 0.633491  [    7/ 3200]\n",
            "loss: 0.799839  [    8/ 3200]\n",
            "loss: 1.014686  [    9/ 3200]\n",
            "loss: 1.150026  [   10/ 3200]\n",
            "loss: 0.839435  [   11/ 3200]\n",
            "loss: 1.050229  [   12/ 3200]\n",
            "loss: 0.833790  [   13/ 3200]\n",
            "loss: 0.686506  [   14/ 3200]\n",
            "loss: 0.815291  [   15/ 3200]\n",
            "loss: 0.814444  [   16/ 3200]\n",
            "loss: 0.622707  [   17/ 3200]\n",
            "loss: 1.031779  [   18/ 3200]\n",
            "loss: 0.725841  [   19/ 3200]\n",
            "loss: 0.636601  [   20/ 3200]\n",
            "loss: 1.211374  [   21/ 3200]\n",
            "loss: 1.270161  [   22/ 3200]\n",
            "loss: 0.783903  [   23/ 3200]\n",
            "loss: 1.256746  [   24/ 3200]\n",
            "loss: 0.735709  [   25/ 3200]\n",
            "loss: 0.784249  [   26/ 3200]\n",
            "loss: 0.717744  [   27/ 3200]\n",
            "loss: 0.858978  [   28/ 3200]\n",
            "loss: 0.827918  [   29/ 3200]\n",
            "loss: 1.002476  [   30/ 3200]\n",
            "loss: 0.880734  [   31/ 3200]\n",
            "loss: 0.751735  [   32/ 3200]\n",
            "loss: 0.808185  [   33/ 3200]\n",
            "loss: 0.940658  [   34/ 3200]\n",
            "loss: 0.648878  [   35/ 3200]\n",
            "loss: 1.131137  [   36/ 3200]\n",
            "loss: 0.711263  [   37/ 3200]\n",
            "loss: 0.855647  [   38/ 3200]\n",
            "loss: 0.735669  [   39/ 3200]\n",
            "loss: 0.887661  [   40/ 3200]\n",
            "loss: 0.767262  [   41/ 3200]\n",
            "loss: 0.606280  [   42/ 3200]\n",
            "loss: 0.984172  [   43/ 3200]\n",
            "loss: 0.659150  [   44/ 3200]\n",
            "loss: 1.109750  [   45/ 3200]\n",
            "loss: 0.676487  [   46/ 3200]\n",
            "loss: 0.596728  [   47/ 3200]\n",
            "loss: 1.033618  [   48/ 3200]\n",
            "loss: 1.243689  [   49/ 3200]\n",
            "loss: 0.932993  [   50/ 3200]\n",
            "loss: 0.847106  [   51/ 3200]\n",
            "loss: 1.158941  [   52/ 3200]\n",
            "loss: 0.552837  [   53/ 3200]\n",
            "loss: 1.019236  [   54/ 3200]\n",
            "loss: 0.577794  [   55/ 3200]\n",
            "loss: 0.748595  [   56/ 3200]\n",
            "loss: 0.911566  [   57/ 3200]\n",
            "loss: 1.009789  [   58/ 3200]\n",
            "loss: 0.924068  [   59/ 3200]\n",
            "loss: 1.183859  [   60/ 3200]\n",
            "loss: 1.004769  [   61/ 3200]\n",
            "loss: 0.923907  [   62/ 3200]\n",
            "loss: 0.702418  [   63/ 3200]\n",
            "loss: 1.079370  [   64/ 3200]\n",
            "loss: 0.818886  [   65/ 3200]\n",
            "loss: 0.880982  [   66/ 3200]\n",
            "loss: 0.935401  [   67/ 3200]\n",
            "loss: 1.179551  [   68/ 3200]\n",
            "loss: 0.912733  [   69/ 3200]\n",
            "loss: 0.734335  [   70/ 3200]\n",
            "loss: 0.719825  [   71/ 3200]\n",
            "loss: 0.740385  [   72/ 3200]\n",
            "loss: 0.710637  [   73/ 3200]\n",
            "loss: 0.976994  [   74/ 3200]\n",
            "loss: 1.061877  [   75/ 3200]\n",
            "loss: 1.259695  [   76/ 3200]\n",
            "loss: 0.742246  [   77/ 3200]\n",
            "loss: 0.842956  [   78/ 3200]\n",
            "loss: 0.678751  [   79/ 3200]\n",
            "loss: 0.789895  [   80/ 3200]\n",
            "loss: 1.052088  [   81/ 3200]\n",
            "loss: 1.352503  [   82/ 3200]\n",
            "loss: 1.034304  [   83/ 3200]\n",
            "loss: 0.799550  [   84/ 3200]\n",
            "loss: 0.658667  [   85/ 3200]\n",
            "loss: 0.544170  [   86/ 3200]\n",
            "loss: 1.197554  [   87/ 3200]\n",
            "loss: 0.624813  [   88/ 3200]\n",
            "loss: 0.918590  [   89/ 3200]\n",
            "loss: 0.924873  [   90/ 3200]\n",
            "loss: 0.875449  [   91/ 3200]\n",
            "loss: 0.896096  [   92/ 3200]\n",
            "loss: 0.876015  [   93/ 3200]\n",
            "loss: 0.657371  [   94/ 3200]\n",
            "loss: 0.912868  [   95/ 3200]\n",
            "loss: 0.801124  [   96/ 3200]\n",
            "loss: 0.822008  [   97/ 3200]\n",
            "loss: 0.928185  [   98/ 3200]\n",
            "loss: 0.829953  [   99/ 3200]\n",
            "loss: 0.924127  [  100/ 3200]\n",
            "loss: 0.721433  [  101/ 3200]\n",
            "loss: 0.406576  [  102/ 3200]\n",
            "loss: 0.893021  [  103/ 3200]\n",
            "loss: 0.942268  [  104/ 3200]\n",
            "loss: 0.835060  [  105/ 3200]\n",
            "loss: 0.952096  [  106/ 3200]\n",
            "loss: 0.749573  [  107/ 3200]\n",
            "loss: 0.713967  [  108/ 3200]\n",
            "loss: 0.918314  [  109/ 3200]\n",
            "loss: 0.924559  [  110/ 3200]\n",
            "loss: 0.734915  [  111/ 3200]\n",
            "loss: 1.323986  [  112/ 3200]\n",
            "loss: 1.204313  [  113/ 3200]\n",
            "loss: 0.833426  [  114/ 3200]\n",
            "loss: 0.722155  [  115/ 3200]\n",
            "loss: 0.855580  [  116/ 3200]\n",
            "loss: 0.698425  [  117/ 3200]\n",
            "loss: 0.987920  [  118/ 3200]\n",
            "loss: 0.875058  [  119/ 3200]\n",
            "loss: 1.227121  [  120/ 3200]\n",
            "loss: 1.006027  [  121/ 3200]\n",
            "loss: 0.733795  [  122/ 3200]\n",
            "loss: 0.876502  [  123/ 3200]\n",
            "loss: 1.461856  [  124/ 3200]\n",
            "loss: 1.048755  [  125/ 3200]\n",
            "loss: 0.867694  [  126/ 3200]\n",
            "loss: 0.805691  [  127/ 3200]\n",
            "loss: 0.909058  [  128/ 3200]\n",
            "loss: 0.766620  [  129/ 3200]\n",
            "loss: 1.098076  [  130/ 3200]\n",
            "loss: 0.907137  [  131/ 3200]\n",
            "loss: 1.071911  [  132/ 3200]\n",
            "loss: 0.766992  [  133/ 3200]\n",
            "loss: 1.171293  [  134/ 3200]\n",
            "loss: 0.969208  [  135/ 3200]\n",
            "loss: 0.979563  [  136/ 3200]\n",
            "loss: 0.863446  [  137/ 3200]\n",
            "loss: 0.823202  [  138/ 3200]\n",
            "loss: 0.779681  [  139/ 3200]\n",
            "loss: 0.817090  [  140/ 3200]\n",
            "loss: 0.728983  [  141/ 3200]\n",
            "loss: 1.112797  [  142/ 3200]\n",
            "loss: 1.047537  [  143/ 3200]\n",
            "loss: 0.999777  [  144/ 3200]\n",
            "loss: 0.912175  [  145/ 3200]\n",
            "loss: 0.921515  [  146/ 3200]\n",
            "loss: 0.892142  [  147/ 3200]\n",
            "loss: 0.883067  [  148/ 3200]\n",
            "loss: 0.842599  [  149/ 3200]\n",
            "loss: 0.788919  [  150/ 3200]\n",
            "loss: 0.796867  [  151/ 3200]\n",
            "loss: 0.938242  [  152/ 3200]\n",
            "loss: 0.746563  [  153/ 3200]\n",
            "loss: 0.548307  [  154/ 3200]\n",
            "loss: 0.881950  [  155/ 3200]\n",
            "loss: 0.975071  [  156/ 3200]\n",
            "loss: 1.066258  [  157/ 3200]\n",
            "loss: 0.904560  [  158/ 3200]\n",
            "loss: 0.670621  [  159/ 3200]\n",
            "loss: 1.274351  [  160/ 3200]\n",
            "loss: 0.791471  [  161/ 3200]\n",
            "loss: 0.622203  [  162/ 3200]\n",
            "loss: 0.845662  [  163/ 3200]\n",
            "loss: 1.196742  [  164/ 3200]\n",
            "loss: 1.091732  [  165/ 3200]\n",
            "loss: 0.757762  [  166/ 3200]\n",
            "loss: 0.769382  [  167/ 3200]\n",
            "loss: 0.914626  [  168/ 3200]\n",
            "loss: 0.628190  [  169/ 3200]\n",
            "loss: 1.029269  [  170/ 3200]\n",
            "loss: 1.072643  [  171/ 3200]\n",
            "loss: 0.719329  [  172/ 3200]\n",
            "loss: 0.759037  [  173/ 3200]\n",
            "loss: 0.919026  [  174/ 3200]\n",
            "loss: 0.820906  [  175/ 3200]\n",
            "loss: 0.784045  [  176/ 3200]\n",
            "loss: 0.928067  [  177/ 3200]\n",
            "loss: 0.922627  [  178/ 3200]\n",
            "loss: 0.822408  [  179/ 3200]\n",
            "loss: 0.973502  [  180/ 3200]\n",
            "loss: 0.810989  [  181/ 3200]\n",
            "loss: 0.991838  [  182/ 3200]\n",
            "loss: 0.825633  [  183/ 3200]\n",
            "loss: 0.790742  [  184/ 3200]\n",
            "loss: 1.080112  [  185/ 3200]\n",
            "loss: 0.934565  [  186/ 3200]\n",
            "loss: 1.231978  [  187/ 3200]\n",
            "loss: 0.877713  [  188/ 3200]\n",
            "loss: 0.881703  [  189/ 3200]\n",
            "loss: 0.888920  [  190/ 3200]\n",
            "loss: 0.805405  [  191/ 3200]\n",
            "loss: 0.863735  [  192/ 3200]\n",
            "loss: 0.641541  [  193/ 3200]\n",
            "loss: 0.670124  [  194/ 3200]\n",
            "loss: 1.103790  [  195/ 3200]\n",
            "loss: 1.330010  [  196/ 3200]\n",
            "loss: 1.215211  [  197/ 3200]\n",
            "loss: 0.828522  [  198/ 3200]\n",
            "loss: 0.787918  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 22\n",
            "Validation Loss: 0.8149\n",
            "Validation F1: 0.6876\n",
            "Validation Accuracy: 0.6963\n",
            "Validation Confusion Matrix:\n",
            "[[ 94  47  25  34]\n",
            " [  4 181   3  12]\n",
            " [ 54   4 119  23]\n",
            " [ 18  14   5 163]]\n",
            "Epoch:  1\n",
            "loss: 0.729092  [    0/ 3200]\n",
            "loss: 0.625519  [    1/ 3200]\n",
            "loss: 0.880107  [    2/ 3200]\n",
            "loss: 0.766343  [    3/ 3200]\n",
            "loss: 1.053107  [    4/ 3200]\n",
            "loss: 0.796124  [    5/ 3200]\n",
            "loss: 1.073170  [    6/ 3200]\n",
            "loss: 1.577281  [    7/ 3200]\n",
            "loss: 0.795090  [    8/ 3200]\n",
            "loss: 1.016676  [    9/ 3200]\n",
            "loss: 0.699865  [   10/ 3200]\n",
            "loss: 0.955385  [   11/ 3200]\n",
            "loss: 0.688482  [   12/ 3200]\n",
            "loss: 0.683657  [   13/ 3200]\n",
            "loss: 1.136496  [   14/ 3200]\n",
            "loss: 0.765354  [   15/ 3200]\n",
            "loss: 0.995178  [   16/ 3200]\n",
            "loss: 0.919522  [   17/ 3200]\n",
            "loss: 0.948446  [   18/ 3200]\n",
            "loss: 0.749159  [   19/ 3200]\n",
            "loss: 1.058466  [   20/ 3200]\n",
            "loss: 0.892780  [   21/ 3200]\n",
            "loss: 0.678130  [   22/ 3200]\n",
            "loss: 0.746271  [   23/ 3200]\n",
            "loss: 0.716152  [   24/ 3200]\n",
            "loss: 0.889231  [   25/ 3200]\n",
            "loss: 0.763159  [   26/ 3200]\n",
            "loss: 0.703642  [   27/ 3200]\n",
            "loss: 0.856211  [   28/ 3200]\n",
            "loss: 1.017741  [   29/ 3200]\n",
            "loss: 0.910988  [   30/ 3200]\n",
            "loss: 0.998966  [   31/ 3200]\n",
            "loss: 1.040800  [   32/ 3200]\n",
            "loss: 0.920942  [   33/ 3200]\n",
            "loss: 0.917194  [   34/ 3200]\n",
            "loss: 0.965521  [   35/ 3200]\n",
            "loss: 1.087864  [   36/ 3200]\n",
            "loss: 1.073047  [   37/ 3200]\n",
            "loss: 0.896045  [   38/ 3200]\n",
            "loss: 0.873284  [   39/ 3200]\n",
            "loss: 1.049847  [   40/ 3200]\n",
            "loss: 1.078601  [   41/ 3200]\n",
            "loss: 1.007127  [   42/ 3200]\n",
            "loss: 1.002619  [   43/ 3200]\n",
            "loss: 0.896799  [   44/ 3200]\n",
            "loss: 0.759015  [   45/ 3200]\n",
            "loss: 0.757958  [   46/ 3200]\n",
            "loss: 0.824062  [   47/ 3200]\n",
            "loss: 1.011403  [   48/ 3200]\n",
            "loss: 0.843131  [   49/ 3200]\n",
            "loss: 1.145613  [   50/ 3200]\n",
            "loss: 0.618675  [   51/ 3200]\n",
            "loss: 1.014146  [   52/ 3200]\n",
            "loss: 0.919117  [   53/ 3200]\n",
            "loss: 1.343878  [   54/ 3200]\n",
            "loss: 0.932979  [   55/ 3200]\n",
            "loss: 0.821780  [   56/ 3200]\n",
            "loss: 0.450496  [   57/ 3200]\n",
            "loss: 0.705549  [   58/ 3200]\n",
            "loss: 0.655933  [   59/ 3200]\n",
            "loss: 0.725628  [   60/ 3200]\n",
            "loss: 0.796469  [   61/ 3200]\n",
            "loss: 0.865290  [   62/ 3200]\n",
            "loss: 0.928196  [   63/ 3200]\n",
            "loss: 0.903655  [   64/ 3200]\n",
            "loss: 0.715185  [   65/ 3200]\n",
            "loss: 0.601875  [   66/ 3200]\n",
            "loss: 1.005719  [   67/ 3200]\n",
            "loss: 1.095369  [   68/ 3200]\n",
            "loss: 1.197084  [   69/ 3200]\n",
            "loss: 0.896649  [   70/ 3200]\n",
            "loss: 0.828295  [   71/ 3200]\n",
            "loss: 1.017689  [   72/ 3200]\n",
            "loss: 0.648116  [   73/ 3200]\n",
            "loss: 0.751891  [   74/ 3200]\n",
            "loss: 1.189664  [   75/ 3200]\n",
            "loss: 0.912843  [   76/ 3200]\n",
            "loss: 0.791410  [   77/ 3200]\n",
            "loss: 0.832081  [   78/ 3200]\n",
            "loss: 0.731510  [   79/ 3200]\n",
            "loss: 0.853919  [   80/ 3200]\n",
            "loss: 0.844425  [   81/ 3200]\n",
            "loss: 1.161039  [   82/ 3200]\n",
            "loss: 0.973549  [   83/ 3200]\n",
            "loss: 0.737109  [   84/ 3200]\n",
            "loss: 0.878281  [   85/ 3200]\n",
            "loss: 1.048387  [   86/ 3200]\n",
            "loss: 1.253250  [   87/ 3200]\n",
            "loss: 0.866796  [   88/ 3200]\n",
            "loss: 0.889809  [   89/ 3200]\n",
            "loss: 0.893353  [   90/ 3200]\n",
            "loss: 0.753552  [   91/ 3200]\n",
            "loss: 0.985629  [   92/ 3200]\n",
            "loss: 0.935194  [   93/ 3200]\n",
            "loss: 0.718136  [   94/ 3200]\n",
            "loss: 0.957412  [   95/ 3200]\n",
            "loss: 1.026509  [   96/ 3200]\n",
            "loss: 1.098712  [   97/ 3200]\n",
            "loss: 0.970126  [   98/ 3200]\n",
            "loss: 0.938793  [   99/ 3200]\n",
            "loss: 0.728326  [  100/ 3200]\n",
            "loss: 0.754065  [  101/ 3200]\n",
            "loss: 0.795614  [  102/ 3200]\n",
            "loss: 0.867611  [  103/ 3200]\n",
            "loss: 0.672914  [  104/ 3200]\n",
            "loss: 1.105239  [  105/ 3200]\n",
            "loss: 1.066498  [  106/ 3200]\n",
            "loss: 0.634292  [  107/ 3200]\n",
            "loss: 1.085668  [  108/ 3200]\n",
            "loss: 0.671819  [  109/ 3200]\n",
            "loss: 0.667687  [  110/ 3200]\n",
            "loss: 0.834229  [  111/ 3200]\n",
            "loss: 1.262307  [  112/ 3200]\n",
            "loss: 0.760243  [  113/ 3200]\n",
            "loss: 0.877532  [  114/ 3200]\n",
            "loss: 0.728405  [  115/ 3200]\n",
            "loss: 0.852605  [  116/ 3200]\n",
            "loss: 0.782300  [  117/ 3200]\n",
            "loss: 0.733743  [  118/ 3200]\n",
            "loss: 0.922329  [  119/ 3200]\n",
            "loss: 0.680166  [  120/ 3200]\n",
            "loss: 0.921444  [  121/ 3200]\n",
            "loss: 0.820301  [  122/ 3200]\n",
            "loss: 0.853019  [  123/ 3200]\n",
            "loss: 1.173166  [  124/ 3200]\n",
            "loss: 0.583420  [  125/ 3200]\n",
            "loss: 1.100901  [  126/ 3200]\n",
            "loss: 1.115040  [  127/ 3200]\n",
            "loss: 1.099212  [  128/ 3200]\n",
            "loss: 0.934475  [  129/ 3200]\n",
            "loss: 0.988594  [  130/ 3200]\n",
            "loss: 0.747410  [  131/ 3200]\n",
            "loss: 0.847627  [  132/ 3200]\n",
            "loss: 1.269890  [  133/ 3200]\n",
            "loss: 0.502622  [  134/ 3200]\n",
            "loss: 1.239815  [  135/ 3200]\n",
            "loss: 0.939120  [  136/ 3200]\n",
            "loss: 0.912593  [  137/ 3200]\n",
            "loss: 0.918916  [  138/ 3200]\n",
            "loss: 0.778787  [  139/ 3200]\n",
            "loss: 1.111944  [  140/ 3200]\n",
            "loss: 0.834101  [  141/ 3200]\n",
            "loss: 0.804190  [  142/ 3200]\n",
            "loss: 0.762548  [  143/ 3200]\n",
            "loss: 1.137210  [  144/ 3200]\n",
            "loss: 0.918347  [  145/ 3200]\n",
            "loss: 0.848726  [  146/ 3200]\n",
            "loss: 0.901033  [  147/ 3200]\n",
            "loss: 0.630802  [  148/ 3200]\n",
            "loss: 0.825256  [  149/ 3200]\n",
            "loss: 0.887204  [  150/ 3200]\n",
            "loss: 0.853728  [  151/ 3200]\n",
            "loss: 0.840590  [  152/ 3200]\n",
            "loss: 0.790556  [  153/ 3200]\n",
            "loss: 0.824616  [  154/ 3200]\n",
            "loss: 0.598279  [  155/ 3200]\n",
            "loss: 0.832934  [  156/ 3200]\n",
            "loss: 1.093714  [  157/ 3200]\n",
            "loss: 1.025188  [  158/ 3200]\n",
            "loss: 0.736660  [  159/ 3200]\n",
            "loss: 0.994563  [  160/ 3200]\n",
            "loss: 0.951306  [  161/ 3200]\n",
            "loss: 0.731527  [  162/ 3200]\n",
            "loss: 0.876474  [  163/ 3200]\n",
            "loss: 0.683048  [  164/ 3200]\n",
            "loss: 0.994379  [  165/ 3200]\n",
            "loss: 0.616033  [  166/ 3200]\n",
            "loss: 0.555262  [  167/ 3200]\n",
            "loss: 1.004456  [  168/ 3200]\n",
            "loss: 0.817080  [  169/ 3200]\n",
            "loss: 0.591003  [  170/ 3200]\n",
            "loss: 0.624470  [  171/ 3200]\n",
            "loss: 0.783905  [  172/ 3200]\n",
            "loss: 0.872164  [  173/ 3200]\n",
            "loss: 0.966076  [  174/ 3200]\n",
            "loss: 1.179679  [  175/ 3200]\n",
            "loss: 1.009662  [  176/ 3200]\n",
            "loss: 0.441079  [  177/ 3200]\n",
            "loss: 1.146268  [  178/ 3200]\n",
            "loss: 0.871899  [  179/ 3200]\n",
            "loss: 0.849002  [  180/ 3200]\n",
            "loss: 0.635538  [  181/ 3200]\n",
            "loss: 0.807305  [  182/ 3200]\n",
            "loss: 1.043725  [  183/ 3200]\n",
            "loss: 0.848883  [  184/ 3200]\n",
            "loss: 0.838119  [  185/ 3200]\n",
            "loss: 0.860664  [  186/ 3200]\n",
            "loss: 1.263238  [  187/ 3200]\n",
            "loss: 1.024225  [  188/ 3200]\n",
            "loss: 0.822507  [  189/ 3200]\n",
            "loss: 1.175967  [  190/ 3200]\n",
            "loss: 0.991773  [  191/ 3200]\n",
            "loss: 0.661086  [  192/ 3200]\n",
            "loss: 0.730165  [  193/ 3200]\n",
            "loss: 1.115956  [  194/ 3200]\n",
            "loss: 0.974337  [  195/ 3200]\n",
            "loss: 0.599725  [  196/ 3200]\n",
            "loss: 0.708913  [  197/ 3200]\n",
            "loss: 0.770021  [  198/ 3200]\n",
            "loss: 0.864857  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 23\n",
            "Validation Loss: 0.8534\n",
            "Validation F1: 0.6366\n",
            "Validation Accuracy: 0.6550\n",
            "Validation Confusion Matrix:\n",
            "[[ 64  80  27  29]\n",
            " [  1 192   3   4]\n",
            " [ 41  14 122  23]\n",
            " [ 13  35   6 146]]\n",
            "Epoch:  1\n",
            "loss: 0.785804  [    0/ 3200]\n",
            "loss: 1.086627  [    1/ 3200]\n",
            "loss: 0.628437  [    2/ 3200]\n",
            "loss: 0.978107  [    3/ 3200]\n",
            "loss: 1.277661  [    4/ 3200]\n",
            "loss: 1.033883  [    5/ 3200]\n",
            "loss: 0.697792  [    6/ 3200]\n",
            "loss: 0.911112  [    7/ 3200]\n",
            "loss: 1.165391  [    8/ 3200]\n",
            "loss: 1.075479  [    9/ 3200]\n",
            "loss: 0.722376  [   10/ 3200]\n",
            "loss: 1.149378  [   11/ 3200]\n",
            "loss: 0.925745  [   12/ 3200]\n",
            "loss: 1.267295  [   13/ 3200]\n",
            "loss: 0.894722  [   14/ 3200]\n",
            "loss: 0.969938  [   15/ 3200]\n",
            "loss: 0.979300  [   16/ 3200]\n",
            "loss: 0.841519  [   17/ 3200]\n",
            "loss: 1.250809  [   18/ 3200]\n",
            "loss: 0.775061  [   19/ 3200]\n",
            "loss: 0.725186  [   20/ 3200]\n",
            "loss: 0.780510  [   21/ 3200]\n",
            "loss: 1.090837  [   22/ 3200]\n",
            "loss: 0.635436  [   23/ 3200]\n",
            "loss: 0.845264  [   24/ 3200]\n",
            "loss: 0.811218  [   25/ 3200]\n",
            "loss: 0.579413  [   26/ 3200]\n",
            "loss: 0.920797  [   27/ 3200]\n",
            "loss: 1.000163  [   28/ 3200]\n",
            "loss: 0.821665  [   29/ 3200]\n",
            "loss: 0.794607  [   30/ 3200]\n",
            "loss: 0.802927  [   31/ 3200]\n",
            "loss: 0.903090  [   32/ 3200]\n",
            "loss: 0.950310  [   33/ 3200]\n",
            "loss: 0.732291  [   34/ 3200]\n",
            "loss: 1.063738  [   35/ 3200]\n",
            "loss: 1.117488  [   36/ 3200]\n",
            "loss: 0.811497  [   37/ 3200]\n",
            "loss: 0.881980  [   38/ 3200]\n",
            "loss: 0.657579  [   39/ 3200]\n",
            "loss: 1.119962  [   40/ 3200]\n",
            "loss: 0.800773  [   41/ 3200]\n",
            "loss: 0.674193  [   42/ 3200]\n",
            "loss: 1.220383  [   43/ 3200]\n",
            "loss: 0.889819  [   44/ 3200]\n",
            "loss: 0.725302  [   45/ 3200]\n",
            "loss: 0.794548  [   46/ 3200]\n",
            "loss: 1.317249  [   47/ 3200]\n",
            "loss: 0.894147  [   48/ 3200]\n",
            "loss: 0.783144  [   49/ 3200]\n",
            "loss: 0.658536  [   50/ 3200]\n",
            "loss: 1.030753  [   51/ 3200]\n",
            "loss: 0.952215  [   52/ 3200]\n",
            "loss: 0.788463  [   53/ 3200]\n",
            "loss: 0.582086  [   54/ 3200]\n",
            "loss: 0.803438  [   55/ 3200]\n",
            "loss: 1.033349  [   56/ 3200]\n",
            "loss: 0.782199  [   57/ 3200]\n",
            "loss: 0.978794  [   58/ 3200]\n",
            "loss: 0.915223  [   59/ 3200]\n",
            "loss: 0.637607  [   60/ 3200]\n",
            "loss: 0.626161  [   61/ 3200]\n",
            "loss: 1.015416  [   62/ 3200]\n",
            "loss: 0.913685  [   63/ 3200]\n",
            "loss: 0.879425  [   64/ 3200]\n",
            "loss: 0.929480  [   65/ 3200]\n",
            "loss: 0.902147  [   66/ 3200]\n",
            "loss: 0.812730  [   67/ 3200]\n",
            "loss: 0.790059  [   68/ 3200]\n",
            "loss: 0.943196  [   69/ 3200]\n",
            "loss: 0.712262  [   70/ 3200]\n",
            "loss: 0.993245  [   71/ 3200]\n",
            "loss: 0.489407  [   72/ 3200]\n",
            "loss: 0.678166  [   73/ 3200]\n",
            "loss: 0.883305  [   74/ 3200]\n",
            "loss: 0.767754  [   75/ 3200]\n",
            "loss: 1.157923  [   76/ 3200]\n",
            "loss: 0.672047  [   77/ 3200]\n",
            "loss: 0.660714  [   78/ 3200]\n",
            "loss: 0.880307  [   79/ 3200]\n",
            "loss: 1.009629  [   80/ 3200]\n",
            "loss: 0.660254  [   81/ 3200]\n",
            "loss: 0.910437  [   82/ 3200]\n",
            "loss: 1.276991  [   83/ 3200]\n",
            "loss: 0.905001  [   84/ 3200]\n",
            "loss: 1.213151  [   85/ 3200]\n",
            "loss: 0.978447  [   86/ 3200]\n",
            "loss: 1.030384  [   87/ 3200]\n",
            "loss: 0.760790  [   88/ 3200]\n",
            "loss: 0.860121  [   89/ 3200]\n",
            "loss: 0.944170  [   90/ 3200]\n",
            "loss: 0.644527  [   91/ 3200]\n",
            "loss: 0.707603  [   92/ 3200]\n",
            "loss: 1.160898  [   93/ 3200]\n",
            "loss: 0.900571  [   94/ 3200]\n",
            "loss: 0.921759  [   95/ 3200]\n",
            "loss: 0.972595  [   96/ 3200]\n",
            "loss: 0.978146  [   97/ 3200]\n",
            "loss: 0.892367  [   98/ 3200]\n",
            "loss: 1.243691  [   99/ 3200]\n",
            "loss: 0.811145  [  100/ 3200]\n",
            "loss: 0.785626  [  101/ 3200]\n",
            "loss: 0.807251  [  102/ 3200]\n",
            "loss: 1.219446  [  103/ 3200]\n",
            "loss: 0.871261  [  104/ 3200]\n",
            "loss: 1.116280  [  105/ 3200]\n",
            "loss: 1.125627  [  106/ 3200]\n",
            "loss: 0.862034  [  107/ 3200]\n",
            "loss: 0.582909  [  108/ 3200]\n",
            "loss: 0.831096  [  109/ 3200]\n",
            "loss: 0.779799  [  110/ 3200]\n",
            "loss: 1.065374  [  111/ 3200]\n",
            "loss: 0.767491  [  112/ 3200]\n",
            "loss: 0.849364  [  113/ 3200]\n",
            "loss: 1.075127  [  114/ 3200]\n",
            "loss: 0.766070  [  115/ 3200]\n",
            "loss: 0.843494  [  116/ 3200]\n",
            "loss: 0.952386  [  117/ 3200]\n",
            "loss: 0.688975  [  118/ 3200]\n",
            "loss: 0.811649  [  119/ 3200]\n",
            "loss: 0.600274  [  120/ 3200]\n",
            "loss: 0.983308  [  121/ 3200]\n",
            "loss: 1.463231  [  122/ 3200]\n",
            "loss: 0.636728  [  123/ 3200]\n",
            "loss: 1.062609  [  124/ 3200]\n",
            "loss: 0.701887  [  125/ 3200]\n",
            "loss: 0.970901  [  126/ 3200]\n",
            "loss: 0.887405  [  127/ 3200]\n",
            "loss: 0.981676  [  128/ 3200]\n",
            "loss: 0.644705  [  129/ 3200]\n",
            "loss: 0.908067  [  130/ 3200]\n",
            "loss: 0.624398  [  131/ 3200]\n",
            "loss: 1.086589  [  132/ 3200]\n",
            "loss: 0.685392  [  133/ 3200]\n",
            "loss: 1.417979  [  134/ 3200]\n",
            "loss: 0.646425  [  135/ 3200]\n",
            "loss: 0.748785  [  136/ 3200]\n",
            "loss: 0.823788  [  137/ 3200]\n",
            "loss: 0.783297  [  138/ 3200]\n",
            "loss: 0.851292  [  139/ 3200]\n",
            "loss: 1.169947  [  140/ 3200]\n",
            "loss: 0.726766  [  141/ 3200]\n",
            "loss: 0.773365  [  142/ 3200]\n",
            "loss: 0.902371  [  143/ 3200]\n",
            "loss: 1.239959  [  144/ 3200]\n",
            "loss: 0.555844  [  145/ 3200]\n",
            "loss: 0.652804  [  146/ 3200]\n",
            "loss: 1.043459  [  147/ 3200]\n",
            "loss: 1.155957  [  148/ 3200]\n",
            "loss: 1.122754  [  149/ 3200]\n",
            "loss: 0.690752  [  150/ 3200]\n",
            "loss: 0.654849  [  151/ 3200]\n",
            "loss: 0.872135  [  152/ 3200]\n",
            "loss: 0.723169  [  153/ 3200]\n",
            "loss: 0.704950  [  154/ 3200]\n",
            "loss: 0.787916  [  155/ 3200]\n",
            "loss: 1.015578  [  156/ 3200]\n",
            "loss: 0.803791  [  157/ 3200]\n",
            "loss: 0.978756  [  158/ 3200]\n",
            "loss: 0.805054  [  159/ 3200]\n",
            "loss: 0.506961  [  160/ 3200]\n",
            "loss: 0.615168  [  161/ 3200]\n",
            "loss: 0.928915  [  162/ 3200]\n",
            "loss: 0.646713  [  163/ 3200]\n",
            "loss: 0.698422  [  164/ 3200]\n",
            "loss: 0.827395  [  165/ 3200]\n",
            "loss: 0.715739  [  166/ 3200]\n",
            "loss: 0.625887  [  167/ 3200]\n",
            "loss: 0.774326  [  168/ 3200]\n",
            "loss: 1.041668  [  169/ 3200]\n",
            "loss: 1.018724  [  170/ 3200]\n",
            "loss: 1.079419  [  171/ 3200]\n",
            "loss: 1.037930  [  172/ 3200]\n",
            "loss: 1.177985  [  173/ 3200]\n",
            "loss: 0.614840  [  174/ 3200]\n",
            "loss: 0.942494  [  175/ 3200]\n",
            "loss: 0.693838  [  176/ 3200]\n",
            "loss: 1.027994  [  177/ 3200]\n",
            "loss: 0.896338  [  178/ 3200]\n",
            "loss: 0.936929  [  179/ 3200]\n",
            "loss: 0.992815  [  180/ 3200]\n",
            "loss: 0.878137  [  181/ 3200]\n",
            "loss: 1.031896  [  182/ 3200]\n",
            "loss: 0.793915  [  183/ 3200]\n",
            "loss: 1.032810  [  184/ 3200]\n",
            "loss: 0.802811  [  185/ 3200]\n",
            "loss: 0.937434  [  186/ 3200]\n",
            "loss: 0.963326  [  187/ 3200]\n",
            "loss: 0.925158  [  188/ 3200]\n",
            "loss: 0.836830  [  189/ 3200]\n",
            "loss: 0.671949  [  190/ 3200]\n",
            "loss: 0.901513  [  191/ 3200]\n",
            "loss: 0.921797  [  192/ 3200]\n",
            "loss: 1.044686  [  193/ 3200]\n",
            "loss: 0.894383  [  194/ 3200]\n",
            "loss: 1.096439  [  195/ 3200]\n",
            "loss: 0.923588  [  196/ 3200]\n",
            "loss: 0.517731  [  197/ 3200]\n",
            "loss: 0.853249  [  198/ 3200]\n",
            "loss: 0.996646  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 24\n",
            "Validation Loss: 0.7900\n",
            "Validation F1: 0.6759\n",
            "Validation Accuracy: 0.6913\n",
            "Validation Confusion Matrix:\n",
            "[[ 69  37  65  29]\n",
            " [  9 174   3  14]\n",
            " [ 32   1 149  18]\n",
            " [ 16  11  12 161]]\n",
            "Epoch:  1\n",
            "loss: 0.683116  [    0/ 3200]\n",
            "loss: 1.108012  [    1/ 3200]\n",
            "loss: 0.874449  [    2/ 3200]\n",
            "loss: 1.032488  [    3/ 3200]\n",
            "loss: 0.986851  [    4/ 3200]\n",
            "loss: 0.868231  [    5/ 3200]\n",
            "loss: 0.761238  [    6/ 3200]\n",
            "loss: 0.725354  [    7/ 3200]\n",
            "loss: 1.184590  [    8/ 3200]\n",
            "loss: 0.604828  [    9/ 3200]\n",
            "loss: 0.836674  [   10/ 3200]\n",
            "loss: 0.673007  [   11/ 3200]\n",
            "loss: 1.077510  [   12/ 3200]\n",
            "loss: 0.725332  [   13/ 3200]\n",
            "loss: 0.755510  [   14/ 3200]\n",
            "loss: 1.029874  [   15/ 3200]\n",
            "loss: 0.779544  [   16/ 3200]\n",
            "loss: 1.351885  [   17/ 3200]\n",
            "loss: 0.701590  [   18/ 3200]\n",
            "loss: 1.247933  [   19/ 3200]\n",
            "loss: 1.010240  [   20/ 3200]\n",
            "loss: 0.791063  [   21/ 3200]\n",
            "loss: 0.898929  [   22/ 3200]\n",
            "loss: 1.095768  [   23/ 3200]\n",
            "loss: 1.371277  [   24/ 3200]\n",
            "loss: 0.703432  [   25/ 3200]\n",
            "loss: 0.793335  [   26/ 3200]\n",
            "loss: 0.877460  [   27/ 3200]\n",
            "loss: 0.699687  [   28/ 3200]\n",
            "loss: 0.771532  [   29/ 3200]\n",
            "loss: 0.658750  [   30/ 3200]\n",
            "loss: 1.194715  [   31/ 3200]\n",
            "loss: 1.300905  [   32/ 3200]\n",
            "loss: 0.915069  [   33/ 3200]\n",
            "loss: 0.516375  [   34/ 3200]\n",
            "loss: 0.825058  [   35/ 3200]\n",
            "loss: 1.008209  [   36/ 3200]\n",
            "loss: 0.698945  [   37/ 3200]\n",
            "loss: 0.918713  [   38/ 3200]\n",
            "loss: 0.692458  [   39/ 3200]\n",
            "loss: 1.118234  [   40/ 3200]\n",
            "loss: 1.137263  [   41/ 3200]\n",
            "loss: 0.652561  [   42/ 3200]\n",
            "loss: 0.711395  [   43/ 3200]\n",
            "loss: 0.838251  [   44/ 3200]\n",
            "loss: 1.005858  [   45/ 3200]\n",
            "loss: 0.724470  [   46/ 3200]\n",
            "loss: 0.884554  [   47/ 3200]\n",
            "loss: 0.874377  [   48/ 3200]\n",
            "loss: 0.802722  [   49/ 3200]\n",
            "loss: 0.791969  [   50/ 3200]\n",
            "loss: 1.290249  [   51/ 3200]\n",
            "loss: 0.426642  [   52/ 3200]\n",
            "loss: 0.813598  [   53/ 3200]\n",
            "loss: 0.780619  [   54/ 3200]\n",
            "loss: 1.081996  [   55/ 3200]\n",
            "loss: 0.940501  [   56/ 3200]\n",
            "loss: 0.785005  [   57/ 3200]\n",
            "loss: 0.859162  [   58/ 3200]\n",
            "loss: 0.952634  [   59/ 3200]\n",
            "loss: 0.853705  [   60/ 3200]\n",
            "loss: 0.942170  [   61/ 3200]\n",
            "loss: 0.460465  [   62/ 3200]\n",
            "loss: 1.046653  [   63/ 3200]\n",
            "loss: 0.843247  [   64/ 3200]\n",
            "loss: 0.752098  [   65/ 3200]\n",
            "loss: 1.235815  [   66/ 3200]\n",
            "loss: 1.093019  [   67/ 3200]\n",
            "loss: 0.694633  [   68/ 3200]\n",
            "loss: 0.721795  [   69/ 3200]\n",
            "loss: 0.799260  [   70/ 3200]\n",
            "loss: 0.808790  [   71/ 3200]\n",
            "loss: 1.000590  [   72/ 3200]\n",
            "loss: 0.812655  [   73/ 3200]\n",
            "loss: 0.934026  [   74/ 3200]\n",
            "loss: 0.805505  [   75/ 3200]\n",
            "loss: 0.964596  [   76/ 3200]\n",
            "loss: 0.872205  [   77/ 3200]\n",
            "loss: 1.063001  [   78/ 3200]\n",
            "loss: 0.702515  [   79/ 3200]\n",
            "loss: 0.736615  [   80/ 3200]\n",
            "loss: 0.876421  [   81/ 3200]\n",
            "loss: 0.811362  [   82/ 3200]\n",
            "loss: 1.060330  [   83/ 3200]\n",
            "loss: 0.561848  [   84/ 3200]\n",
            "loss: 0.727265  [   85/ 3200]\n",
            "loss: 0.724194  [   86/ 3200]\n",
            "loss: 0.776784  [   87/ 3200]\n",
            "loss: 0.883785  [   88/ 3200]\n",
            "loss: 0.724043  [   89/ 3200]\n",
            "loss: 0.633877  [   90/ 3200]\n",
            "loss: 0.702062  [   91/ 3200]\n",
            "loss: 1.124207  [   92/ 3200]\n",
            "loss: 1.128592  [   93/ 3200]\n",
            "loss: 1.204302  [   94/ 3200]\n",
            "loss: 1.009004  [   95/ 3200]\n",
            "loss: 1.100723  [   96/ 3200]\n",
            "loss: 1.052742  [   97/ 3200]\n",
            "loss: 0.835205  [   98/ 3200]\n",
            "loss: 0.704231  [   99/ 3200]\n",
            "loss: 0.713618  [  100/ 3200]\n",
            "loss: 0.844537  [  101/ 3200]\n",
            "loss: 0.828684  [  102/ 3200]\n",
            "loss: 0.783328  [  103/ 3200]\n",
            "loss: 0.930683  [  104/ 3200]\n",
            "loss: 0.956506  [  105/ 3200]\n",
            "loss: 0.614626  [  106/ 3200]\n",
            "loss: 0.672444  [  107/ 3200]\n",
            "loss: 1.119846  [  108/ 3200]\n",
            "loss: 0.793406  [  109/ 3200]\n",
            "loss: 0.971626  [  110/ 3200]\n",
            "loss: 0.936566  [  111/ 3200]\n",
            "loss: 1.064460  [  112/ 3200]\n",
            "loss: 0.739675  [  113/ 3200]\n",
            "loss: 0.783843  [  114/ 3200]\n",
            "loss: 1.020723  [  115/ 3200]\n",
            "loss: 1.032183  [  116/ 3200]\n",
            "loss: 0.925826  [  117/ 3200]\n",
            "loss: 0.867787  [  118/ 3200]\n",
            "loss: 0.881948  [  119/ 3200]\n",
            "loss: 0.678696  [  120/ 3200]\n",
            "loss: 0.778632  [  121/ 3200]\n",
            "loss: 0.843285  [  122/ 3200]\n",
            "loss: 0.766350  [  123/ 3200]\n",
            "loss: 1.271498  [  124/ 3200]\n",
            "loss: 0.924578  [  125/ 3200]\n",
            "loss: 0.463841  [  126/ 3200]\n",
            "loss: 0.685741  [  127/ 3200]\n",
            "loss: 1.301116  [  128/ 3200]\n",
            "loss: 0.723299  [  129/ 3200]\n",
            "loss: 0.759519  [  130/ 3200]\n",
            "loss: 1.130532  [  131/ 3200]\n",
            "loss: 0.942889  [  132/ 3200]\n",
            "loss: 1.002598  [  133/ 3200]\n",
            "loss: 1.097863  [  134/ 3200]\n",
            "loss: 1.361802  [  135/ 3200]\n",
            "loss: 1.069206  [  136/ 3200]\n",
            "loss: 0.670629  [  137/ 3200]\n",
            "loss: 0.596880  [  138/ 3200]\n",
            "loss: 0.823660  [  139/ 3200]\n",
            "loss: 1.021736  [  140/ 3200]\n",
            "loss: 0.749849  [  141/ 3200]\n",
            "loss: 0.946553  [  142/ 3200]\n",
            "loss: 1.296466  [  143/ 3200]\n",
            "loss: 0.968746  [  144/ 3200]\n",
            "loss: 0.775739  [  145/ 3200]\n",
            "loss: 0.977205  [  146/ 3200]\n",
            "loss: 1.018778  [  147/ 3200]\n",
            "loss: 0.908382  [  148/ 3200]\n",
            "loss: 1.088427  [  149/ 3200]\n",
            "loss: 0.705292  [  150/ 3200]\n",
            "loss: 1.074749  [  151/ 3200]\n",
            "loss: 0.945056  [  152/ 3200]\n",
            "loss: 0.904255  [  153/ 3200]\n",
            "loss: 1.322667  [  154/ 3200]\n",
            "loss: 1.051304  [  155/ 3200]\n",
            "loss: 1.050270  [  156/ 3200]\n",
            "loss: 1.053443  [  157/ 3200]\n",
            "loss: 0.635451  [  158/ 3200]\n",
            "loss: 0.626790  [  159/ 3200]\n",
            "loss: 1.036564  [  160/ 3200]\n",
            "loss: 0.778885  [  161/ 3200]\n",
            "loss: 0.891191  [  162/ 3200]\n",
            "loss: 0.710508  [  163/ 3200]\n",
            "loss: 1.061642  [  164/ 3200]\n",
            "loss: 0.694732  [  165/ 3200]\n",
            "loss: 1.175656  [  166/ 3200]\n",
            "loss: 0.869358  [  167/ 3200]\n",
            "loss: 0.671441  [  168/ 3200]\n",
            "loss: 1.130671  [  169/ 3200]\n",
            "loss: 0.717173  [  170/ 3200]\n",
            "loss: 0.819725  [  171/ 3200]\n",
            "loss: 0.937571  [  172/ 3200]\n",
            "loss: 0.922372  [  173/ 3200]\n",
            "loss: 1.106921  [  174/ 3200]\n",
            "loss: 0.865292  [  175/ 3200]\n",
            "loss: 0.730426  [  176/ 3200]\n",
            "loss: 0.686607  [  177/ 3200]\n",
            "loss: 0.999841  [  178/ 3200]\n",
            "loss: 0.781768  [  179/ 3200]\n",
            "loss: 0.917360  [  180/ 3200]\n",
            "loss: 0.841068  [  181/ 3200]\n",
            "loss: 1.063664  [  182/ 3200]\n",
            "loss: 0.877563  [  183/ 3200]\n",
            "loss: 0.566467  [  184/ 3200]\n",
            "loss: 0.557926  [  185/ 3200]\n",
            "loss: 0.951089  [  186/ 3200]\n",
            "loss: 0.817747  [  187/ 3200]\n",
            "loss: 1.160034  [  188/ 3200]\n",
            "loss: 0.935098  [  189/ 3200]\n",
            "loss: 0.733652  [  190/ 3200]\n",
            "loss: 0.745817  [  191/ 3200]\n",
            "loss: 0.543700  [  192/ 3200]\n",
            "loss: 0.957162  [  193/ 3200]\n",
            "loss: 0.966681  [  194/ 3200]\n",
            "loss: 0.957214  [  195/ 3200]\n",
            "loss: 1.118186  [  196/ 3200]\n",
            "loss: 1.155505  [  197/ 3200]\n",
            "loss: 0.769213  [  198/ 3200]\n",
            "loss: 1.030753  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 25\n",
            "Validation Loss: 0.8275\n",
            "Validation F1: 0.6628\n",
            "Validation Accuracy: 0.6663\n",
            "Validation Confusion Matrix:\n",
            "[[ 82  32  76  10]\n",
            " [ 22 170   4   4]\n",
            " [ 30   1 163   6]\n",
            " [ 54  10  18 118]]\n",
            "Epoch:  1\n",
            "loss: 0.886436  [    0/ 3200]\n",
            "loss: 0.803105  [    1/ 3200]\n",
            "loss: 0.998181  [    2/ 3200]\n",
            "loss: 0.738803  [    3/ 3200]\n",
            "loss: 0.903087  [    4/ 3200]\n",
            "loss: 0.924801  [    5/ 3200]\n",
            "loss: 1.050002  [    6/ 3200]\n",
            "loss: 0.549462  [    7/ 3200]\n",
            "loss: 0.973690  [    8/ 3200]\n",
            "loss: 0.655390  [    9/ 3200]\n",
            "loss: 0.573541  [   10/ 3200]\n",
            "loss: 0.703478  [   11/ 3200]\n",
            "loss: 1.053001  [   12/ 3200]\n",
            "loss: 0.747456  [   13/ 3200]\n",
            "loss: 1.050262  [   14/ 3200]\n",
            "loss: 1.234478  [   15/ 3200]\n",
            "loss: 1.133438  [   16/ 3200]\n",
            "loss: 1.120390  [   17/ 3200]\n",
            "loss: 0.954744  [   18/ 3200]\n",
            "loss: 0.921504  [   19/ 3200]\n",
            "loss: 0.747429  [   20/ 3200]\n",
            "loss: 0.712510  [   21/ 3200]\n",
            "loss: 0.755906  [   22/ 3200]\n",
            "loss: 0.854389  [   23/ 3200]\n",
            "loss: 0.764719  [   24/ 3200]\n",
            "loss: 1.201860  [   25/ 3200]\n",
            "loss: 0.925393  [   26/ 3200]\n",
            "loss: 1.172107  [   27/ 3200]\n",
            "loss: 0.572960  [   28/ 3200]\n",
            "loss: 0.849100  [   29/ 3200]\n",
            "loss: 0.874643  [   30/ 3200]\n",
            "loss: 0.744219  [   31/ 3200]\n",
            "loss: 1.322188  [   32/ 3200]\n",
            "loss: 1.073221  [   33/ 3200]\n",
            "loss: 1.068795  [   34/ 3200]\n",
            "loss: 1.026540  [   35/ 3200]\n",
            "loss: 0.886687  [   36/ 3200]\n",
            "loss: 0.880245  [   37/ 3200]\n",
            "loss: 0.988251  [   38/ 3200]\n",
            "loss: 0.979798  [   39/ 3200]\n",
            "loss: 0.720743  [   40/ 3200]\n",
            "loss: 1.028370  [   41/ 3200]\n",
            "loss: 0.888788  [   42/ 3200]\n",
            "loss: 0.934883  [   43/ 3200]\n",
            "loss: 0.761646  [   44/ 3200]\n",
            "loss: 0.846561  [   45/ 3200]\n",
            "loss: 0.894889  [   46/ 3200]\n",
            "loss: 1.083256  [   47/ 3200]\n",
            "loss: 0.787091  [   48/ 3200]\n",
            "loss: 0.798102  [   49/ 3200]\n",
            "loss: 1.033423  [   50/ 3200]\n",
            "loss: 0.898770  [   51/ 3200]\n",
            "loss: 0.893415  [   52/ 3200]\n",
            "loss: 0.901413  [   53/ 3200]\n",
            "loss: 0.947594  [   54/ 3200]\n",
            "loss: 0.964051  [   55/ 3200]\n",
            "loss: 1.167175  [   56/ 3200]\n",
            "loss: 0.769845  [   57/ 3200]\n",
            "loss: 0.662287  [   58/ 3200]\n",
            "loss: 0.719416  [   59/ 3200]\n",
            "loss: 1.093352  [   60/ 3200]\n",
            "loss: 1.127344  [   61/ 3200]\n",
            "loss: 1.164732  [   62/ 3200]\n",
            "loss: 0.949189  [   63/ 3200]\n",
            "loss: 0.892133  [   64/ 3200]\n",
            "loss: 0.766342  [   65/ 3200]\n",
            "loss: 0.950224  [   66/ 3200]\n",
            "loss: 1.026295  [   67/ 3200]\n",
            "loss: 0.674152  [   68/ 3200]\n",
            "loss: 0.908237  [   69/ 3200]\n",
            "loss: 0.774494  [   70/ 3200]\n",
            "loss: 0.712377  [   71/ 3200]\n",
            "loss: 0.735145  [   72/ 3200]\n",
            "loss: 0.909953  [   73/ 3200]\n",
            "loss: 0.747995  [   74/ 3200]\n",
            "loss: 0.737580  [   75/ 3200]\n",
            "loss: 0.827054  [   76/ 3200]\n",
            "loss: 1.066767  [   77/ 3200]\n",
            "loss: 0.625400  [   78/ 3200]\n",
            "loss: 1.117466  [   79/ 3200]\n",
            "loss: 0.642375  [   80/ 3200]\n",
            "loss: 0.694145  [   81/ 3200]\n",
            "loss: 1.145998  [   82/ 3200]\n",
            "loss: 0.894804  [   83/ 3200]\n",
            "loss: 0.709451  [   84/ 3200]\n",
            "loss: 0.554293  [   85/ 3200]\n",
            "loss: 0.743532  [   86/ 3200]\n",
            "loss: 0.913366  [   87/ 3200]\n",
            "loss: 0.987093  [   88/ 3200]\n",
            "loss: 0.989083  [   89/ 3200]\n",
            "loss: 1.384990  [   90/ 3200]\n",
            "loss: 0.703394  [   91/ 3200]\n",
            "loss: 1.094725  [   92/ 3200]\n",
            "loss: 1.191514  [   93/ 3200]\n",
            "loss: 0.760260  [   94/ 3200]\n",
            "loss: 0.685746  [   95/ 3200]\n",
            "loss: 0.745222  [   96/ 3200]\n",
            "loss: 1.058241  [   97/ 3200]\n",
            "loss: 1.039738  [   98/ 3200]\n",
            "loss: 0.860251  [   99/ 3200]\n",
            "loss: 0.819008  [  100/ 3200]\n",
            "loss: 0.816065  [  101/ 3200]\n",
            "loss: 0.411017  [  102/ 3200]\n",
            "loss: 0.771392  [  103/ 3200]\n",
            "loss: 1.028831  [  104/ 3200]\n",
            "loss: 0.832865  [  105/ 3200]\n",
            "loss: 0.799952  [  106/ 3200]\n",
            "loss: 1.109292  [  107/ 3200]\n",
            "loss: 1.101411  [  108/ 3200]\n",
            "loss: 0.803594  [  109/ 3200]\n",
            "loss: 0.952195  [  110/ 3200]\n",
            "loss: 0.946155  [  111/ 3200]\n",
            "loss: 0.881813  [  112/ 3200]\n",
            "loss: 0.932644  [  113/ 3200]\n",
            "loss: 1.071970  [  114/ 3200]\n",
            "loss: 1.121084  [  115/ 3200]\n",
            "loss: 1.112531  [  116/ 3200]\n",
            "loss: 0.667548  [  117/ 3200]\n",
            "loss: 0.750641  [  118/ 3200]\n",
            "loss: 1.025852  [  119/ 3200]\n",
            "loss: 0.851325  [  120/ 3200]\n",
            "loss: 0.951665  [  121/ 3200]\n",
            "loss: 0.830515  [  122/ 3200]\n",
            "loss: 0.972766  [  123/ 3200]\n",
            "loss: 0.738029  [  124/ 3200]\n",
            "loss: 1.072849  [  125/ 3200]\n",
            "loss: 0.820444  [  126/ 3200]\n",
            "loss: 0.988509  [  127/ 3200]\n",
            "loss: 0.656678  [  128/ 3200]\n",
            "loss: 0.768799  [  129/ 3200]\n",
            "loss: 0.871713  [  130/ 3200]\n",
            "loss: 1.106278  [  131/ 3200]\n",
            "loss: 0.834818  [  132/ 3200]\n",
            "loss: 0.740387  [  133/ 3200]\n",
            "loss: 1.007752  [  134/ 3200]\n",
            "loss: 1.007248  [  135/ 3200]\n",
            "loss: 1.266367  [  136/ 3200]\n",
            "loss: 0.770039  [  137/ 3200]\n",
            "loss: 0.517357  [  138/ 3200]\n",
            "loss: 0.913101  [  139/ 3200]\n",
            "loss: 0.629147  [  140/ 3200]\n",
            "loss: 0.961641  [  141/ 3200]\n",
            "loss: 0.889262  [  142/ 3200]\n",
            "loss: 0.749336  [  143/ 3200]\n",
            "loss: 0.629540  [  144/ 3200]\n",
            "loss: 0.881025  [  145/ 3200]\n",
            "loss: 1.145263  [  146/ 3200]\n",
            "loss: 0.906597  [  147/ 3200]\n",
            "loss: 0.729368  [  148/ 3200]\n",
            "loss: 1.072218  [  149/ 3200]\n",
            "loss: 1.220438  [  150/ 3200]\n",
            "loss: 0.739714  [  151/ 3200]\n",
            "loss: 0.943071  [  152/ 3200]\n",
            "loss: 0.662300  [  153/ 3200]\n",
            "loss: 0.864658  [  154/ 3200]\n",
            "loss: 0.726587  [  155/ 3200]\n",
            "loss: 0.692642  [  156/ 3200]\n",
            "loss: 0.738662  [  157/ 3200]\n",
            "loss: 0.687236  [  158/ 3200]\n",
            "loss: 0.935484  [  159/ 3200]\n",
            "loss: 0.854042  [  160/ 3200]\n",
            "loss: 0.673199  [  161/ 3200]\n",
            "loss: 0.723818  [  162/ 3200]\n",
            "loss: 0.897309  [  163/ 3200]\n",
            "loss: 1.109133  [  164/ 3200]\n",
            "loss: 0.643881  [  165/ 3200]\n",
            "loss: 0.803800  [  166/ 3200]\n",
            "loss: 0.825085  [  167/ 3200]\n",
            "loss: 0.966455  [  168/ 3200]\n",
            "loss: 1.003783  [  169/ 3200]\n",
            "loss: 1.024094  [  170/ 3200]\n",
            "loss: 1.002547  [  171/ 3200]\n",
            "loss: 1.025676  [  172/ 3200]\n",
            "loss: 0.963001  [  173/ 3200]\n",
            "loss: 0.944538  [  174/ 3200]\n",
            "loss: 0.337672  [  175/ 3200]\n",
            "loss: 0.783177  [  176/ 3200]\n",
            "loss: 0.699759  [  177/ 3200]\n",
            "loss: 0.795488  [  178/ 3200]\n",
            "loss: 0.747298  [  179/ 3200]\n",
            "loss: 0.891625  [  180/ 3200]\n",
            "loss: 1.031134  [  181/ 3200]\n",
            "loss: 0.743549  [  182/ 3200]\n",
            "loss: 0.758493  [  183/ 3200]\n",
            "loss: 0.725183  [  184/ 3200]\n",
            "loss: 0.996814  [  185/ 3200]\n",
            "loss: 0.864051  [  186/ 3200]\n",
            "loss: 1.035785  [  187/ 3200]\n",
            "loss: 0.705464  [  188/ 3200]\n",
            "loss: 1.158483  [  189/ 3200]\n",
            "loss: 0.861003  [  190/ 3200]\n",
            "loss: 0.866550  [  191/ 3200]\n",
            "loss: 0.799623  [  192/ 3200]\n",
            "loss: 0.832465  [  193/ 3200]\n",
            "loss: 0.696259  [  194/ 3200]\n",
            "loss: 1.057444  [  195/ 3200]\n",
            "loss: 0.902829  [  196/ 3200]\n",
            "loss: 0.913305  [  197/ 3200]\n",
            "loss: 1.242381  [  198/ 3200]\n",
            "loss: 1.098393  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 26\n",
            "Validation Loss: 0.8106\n",
            "Validation F1: 0.6203\n",
            "Validation Accuracy: 0.6400\n",
            "Validation Confusion Matrix:\n",
            "[[ 55  25  79  41]\n",
            " [ 32 126   3  39]\n",
            " [ 16   1 158  25]\n",
            " [ 10   3  14 173]]\n",
            "Epoch:  1\n",
            "loss: 0.727361  [    0/ 3200]\n",
            "loss: 1.056801  [    1/ 3200]\n",
            "loss: 0.686571  [    2/ 3200]\n",
            "loss: 0.789168  [    3/ 3200]\n",
            "loss: 0.916764  [    4/ 3200]\n",
            "loss: 0.713474  [    5/ 3200]\n",
            "loss: 0.908854  [    6/ 3200]\n",
            "loss: 1.183623  [    7/ 3200]\n",
            "loss: 1.359493  [    8/ 3200]\n",
            "loss: 1.039905  [    9/ 3200]\n",
            "loss: 0.613430  [   10/ 3200]\n",
            "loss: 1.554724  [   11/ 3200]\n",
            "loss: 0.807061  [   12/ 3200]\n",
            "loss: 0.868459  [   13/ 3200]\n",
            "loss: 1.200573  [   14/ 3200]\n",
            "loss: 0.594012  [   15/ 3200]\n",
            "loss: 0.908207  [   16/ 3200]\n",
            "loss: 1.122286  [   17/ 3200]\n",
            "loss: 0.577744  [   18/ 3200]\n",
            "loss: 1.082585  [   19/ 3200]\n",
            "loss: 0.726485  [   20/ 3200]\n",
            "loss: 1.036103  [   21/ 3200]\n",
            "loss: 0.635641  [   22/ 3200]\n",
            "loss: 1.098510  [   23/ 3200]\n",
            "loss: 0.803777  [   24/ 3200]\n",
            "loss: 0.803531  [   25/ 3200]\n",
            "loss: 0.716908  [   26/ 3200]\n",
            "loss: 0.985928  [   27/ 3200]\n",
            "loss: 0.732484  [   28/ 3200]\n",
            "loss: 1.062070  [   29/ 3200]\n",
            "loss: 0.847280  [   30/ 3200]\n",
            "loss: 0.761848  [   31/ 3200]\n",
            "loss: 0.961710  [   32/ 3200]\n",
            "loss: 0.853719  [   33/ 3200]\n",
            "loss: 0.744048  [   34/ 3200]\n",
            "loss: 0.938453  [   35/ 3200]\n",
            "loss: 0.681633  [   36/ 3200]\n",
            "loss: 0.733336  [   37/ 3200]\n",
            "loss: 1.054924  [   38/ 3200]\n",
            "loss: 0.787852  [   39/ 3200]\n",
            "loss: 0.961586  [   40/ 3200]\n",
            "loss: 0.869794  [   41/ 3200]\n",
            "loss: 0.842366  [   42/ 3200]\n",
            "loss: 1.142856  [   43/ 3200]\n",
            "loss: 1.186922  [   44/ 3200]\n",
            "loss: 0.783758  [   45/ 3200]\n",
            "loss: 0.989949  [   46/ 3200]\n",
            "loss: 0.634984  [   47/ 3200]\n",
            "loss: 0.875693  [   48/ 3200]\n",
            "loss: 0.607358  [   49/ 3200]\n",
            "loss: 0.695115  [   50/ 3200]\n",
            "loss: 0.626277  [   51/ 3200]\n",
            "loss: 1.276352  [   52/ 3200]\n",
            "loss: 1.069446  [   53/ 3200]\n",
            "loss: 0.730417  [   54/ 3200]\n",
            "loss: 0.826888  [   55/ 3200]\n",
            "loss: 0.795158  [   56/ 3200]\n",
            "loss: 0.858813  [   57/ 3200]\n",
            "loss: 0.936900  [   58/ 3200]\n",
            "loss: 1.105699  [   59/ 3200]\n",
            "loss: 0.855479  [   60/ 3200]\n",
            "loss: 0.703245  [   61/ 3200]\n",
            "loss: 1.164824  [   62/ 3200]\n",
            "loss: 0.904826  [   63/ 3200]\n",
            "loss: 1.002314  [   64/ 3200]\n",
            "loss: 0.629259  [   65/ 3200]\n",
            "loss: 0.702023  [   66/ 3200]\n",
            "loss: 0.896300  [   67/ 3200]\n",
            "loss: 0.771875  [   68/ 3200]\n",
            "loss: 0.946648  [   69/ 3200]\n",
            "loss: 0.753410  [   70/ 3200]\n",
            "loss: 1.018832  [   71/ 3200]\n",
            "loss: 0.763817  [   72/ 3200]\n",
            "loss: 1.075421  [   73/ 3200]\n",
            "loss: 1.187263  [   74/ 3200]\n",
            "loss: 0.820747  [   75/ 3200]\n",
            "loss: 1.089468  [   76/ 3200]\n",
            "loss: 0.754671  [   77/ 3200]\n",
            "loss: 0.965602  [   78/ 3200]\n",
            "loss: 1.307380  [   79/ 3200]\n",
            "loss: 1.087438  [   80/ 3200]\n",
            "loss: 0.606945  [   81/ 3200]\n",
            "loss: 0.790649  [   82/ 3200]\n",
            "loss: 0.682533  [   83/ 3200]\n",
            "loss: 0.743844  [   84/ 3200]\n",
            "loss: 0.896152  [   85/ 3200]\n",
            "loss: 0.786750  [   86/ 3200]\n",
            "loss: 1.004537  [   87/ 3200]\n",
            "loss: 0.838120  [   88/ 3200]\n",
            "loss: 0.831615  [   89/ 3200]\n",
            "loss: 0.885849  [   90/ 3200]\n",
            "loss: 0.665906  [   91/ 3200]\n",
            "loss: 0.796190  [   92/ 3200]\n",
            "loss: 0.766339  [   93/ 3200]\n",
            "loss: 0.552835  [   94/ 3200]\n",
            "loss: 0.962552  [   95/ 3200]\n",
            "loss: 0.823263  [   96/ 3200]\n",
            "loss: 1.085193  [   97/ 3200]\n",
            "loss: 1.016534  [   98/ 3200]\n",
            "loss: 0.847927  [   99/ 3200]\n",
            "loss: 0.890609  [  100/ 3200]\n",
            "loss: 0.757368  [  101/ 3200]\n",
            "loss: 0.853875  [  102/ 3200]\n",
            "loss: 0.760182  [  103/ 3200]\n",
            "loss: 0.624472  [  104/ 3200]\n",
            "loss: 0.769405  [  105/ 3200]\n",
            "loss: 1.216304  [  106/ 3200]\n",
            "loss: 0.719638  [  107/ 3200]\n",
            "loss: 0.822079  [  108/ 3200]\n",
            "loss: 0.830188  [  109/ 3200]\n",
            "loss: 1.014939  [  110/ 3200]\n",
            "loss: 0.769235  [  111/ 3200]\n",
            "loss: 0.951006  [  112/ 3200]\n",
            "loss: 1.183372  [  113/ 3200]\n",
            "loss: 0.949975  [  114/ 3200]\n",
            "loss: 0.755993  [  115/ 3200]\n",
            "loss: 0.644640  [  116/ 3200]\n",
            "loss: 0.689247  [  117/ 3200]\n",
            "loss: 0.913711  [  118/ 3200]\n",
            "loss: 0.959623  [  119/ 3200]\n",
            "loss: 0.687915  [  120/ 3200]\n",
            "loss: 1.295807  [  121/ 3200]\n",
            "loss: 1.037576  [  122/ 3200]\n",
            "loss: 1.292186  [  123/ 3200]\n",
            "loss: 0.790229  [  124/ 3200]\n",
            "loss: 1.176618  [  125/ 3200]\n",
            "loss: 0.950607  [  126/ 3200]\n",
            "loss: 1.053599  [  127/ 3200]\n",
            "loss: 0.754951  [  128/ 3200]\n",
            "loss: 1.467500  [  129/ 3200]\n",
            "loss: 0.683159  [  130/ 3200]\n",
            "loss: 0.944969  [  131/ 3200]\n",
            "loss: 0.974406  [  132/ 3200]\n",
            "loss: 1.099248  [  133/ 3200]\n",
            "loss: 0.572884  [  134/ 3200]\n",
            "loss: 0.854731  [  135/ 3200]\n",
            "loss: 0.724234  [  136/ 3200]\n",
            "loss: 0.892957  [  137/ 3200]\n",
            "loss: 0.611920  [  138/ 3200]\n",
            "loss: 0.762781  [  139/ 3200]\n",
            "loss: 0.858210  [  140/ 3200]\n",
            "loss: 1.285498  [  141/ 3200]\n",
            "loss: 0.490624  [  142/ 3200]\n",
            "loss: 0.910650  [  143/ 3200]\n",
            "loss: 0.967139  [  144/ 3200]\n",
            "loss: 0.790539  [  145/ 3200]\n",
            "loss: 0.743544  [  146/ 3200]\n",
            "loss: 0.723432  [  147/ 3200]\n",
            "loss: 0.721444  [  148/ 3200]\n",
            "loss: 0.526772  [  149/ 3200]\n",
            "loss: 0.874016  [  150/ 3200]\n",
            "loss: 0.585951  [  151/ 3200]\n",
            "loss: 0.615380  [  152/ 3200]\n",
            "loss: 1.078734  [  153/ 3200]\n",
            "loss: 0.865467  [  154/ 3200]\n",
            "loss: 1.036427  [  155/ 3200]\n",
            "loss: 0.676053  [  156/ 3200]\n",
            "loss: 0.969153  [  157/ 3200]\n",
            "loss: 1.225929  [  158/ 3200]\n",
            "loss: 1.194902  [  159/ 3200]\n",
            "loss: 0.938552  [  160/ 3200]\n",
            "loss: 0.854384  [  161/ 3200]\n",
            "loss: 0.584590  [  162/ 3200]\n",
            "loss: 0.833736  [  163/ 3200]\n",
            "loss: 0.803431  [  164/ 3200]\n",
            "loss: 0.842652  [  165/ 3200]\n",
            "loss: 0.936365  [  166/ 3200]\n",
            "loss: 0.610997  [  167/ 3200]\n",
            "loss: 0.705562  [  168/ 3200]\n",
            "loss: 0.731571  [  169/ 3200]\n",
            "loss: 0.748980  [  170/ 3200]\n",
            "loss: 1.177697  [  171/ 3200]\n",
            "loss: 0.637070  [  172/ 3200]\n",
            "loss: 1.062883  [  173/ 3200]\n",
            "loss: 0.859040  [  174/ 3200]\n",
            "loss: 0.795767  [  175/ 3200]\n",
            "loss: 0.975130  [  176/ 3200]\n",
            "loss: 0.819632  [  177/ 3200]\n",
            "loss: 0.648683  [  178/ 3200]\n",
            "loss: 1.112246  [  179/ 3200]\n",
            "loss: 0.901848  [  180/ 3200]\n",
            "loss: 0.945720  [  181/ 3200]\n",
            "loss: 0.830888  [  182/ 3200]\n",
            "loss: 0.924757  [  183/ 3200]\n",
            "loss: 1.010143  [  184/ 3200]\n",
            "loss: 1.041367  [  185/ 3200]\n",
            "loss: 1.228433  [  186/ 3200]\n",
            "loss: 1.276092  [  187/ 3200]\n",
            "loss: 1.062821  [  188/ 3200]\n",
            "loss: 0.704350  [  189/ 3200]\n",
            "loss: 0.806633  [  190/ 3200]\n",
            "loss: 0.739666  [  191/ 3200]\n",
            "loss: 0.856399  [  192/ 3200]\n",
            "loss: 0.701645  [  193/ 3200]\n",
            "loss: 0.969096  [  194/ 3200]\n",
            "loss: 0.985306  [  195/ 3200]\n",
            "loss: 0.897473  [  196/ 3200]\n",
            "loss: 0.794835  [  197/ 3200]\n",
            "loss: 0.912408  [  198/ 3200]\n",
            "loss: 0.982320  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 27\n",
            "Validation Loss: 0.8168\n",
            "Validation F1: 0.6615\n",
            "Validation Accuracy: 0.6650\n",
            "Validation Confusion Matrix:\n",
            "[[ 79  30  79  12]\n",
            " [ 26 166   4   4]\n",
            " [ 28   1 165   6]\n",
            " [ 48   8  22 122]]\n",
            "Epoch:  1\n",
            "loss: 0.756206  [    0/ 3200]\n",
            "loss: 1.024045  [    1/ 3200]\n",
            "loss: 0.879252  [    2/ 3200]\n",
            "loss: 0.719108  [    3/ 3200]\n",
            "loss: 0.829043  [    4/ 3200]\n",
            "loss: 1.189979  [    5/ 3200]\n",
            "loss: 0.970483  [    6/ 3200]\n",
            "loss: 0.891147  [    7/ 3200]\n",
            "loss: 0.859741  [    8/ 3200]\n",
            "loss: 0.765833  [    9/ 3200]\n",
            "loss: 0.507866  [   10/ 3200]\n",
            "loss: 0.886515  [   11/ 3200]\n",
            "loss: 0.886735  [   12/ 3200]\n",
            "loss: 0.904725  [   13/ 3200]\n",
            "loss: 0.797081  [   14/ 3200]\n",
            "loss: 0.918303  [   15/ 3200]\n",
            "loss: 1.296883  [   16/ 3200]\n",
            "loss: 1.245353  [   17/ 3200]\n",
            "loss: 1.410703  [   18/ 3200]\n",
            "loss: 1.013085  [   19/ 3200]\n",
            "loss: 0.833866  [   20/ 3200]\n",
            "loss: 0.930857  [   21/ 3200]\n",
            "loss: 0.601318  [   22/ 3200]\n",
            "loss: 1.085445  [   23/ 3200]\n",
            "loss: 0.782214  [   24/ 3200]\n",
            "loss: 0.890386  [   25/ 3200]\n",
            "loss: 0.957705  [   26/ 3200]\n",
            "loss: 0.932879  [   27/ 3200]\n",
            "loss: 0.807804  [   28/ 3200]\n",
            "loss: 1.020042  [   29/ 3200]\n",
            "loss: 0.826103  [   30/ 3200]\n",
            "loss: 0.961453  [   31/ 3200]\n",
            "loss: 0.529515  [   32/ 3200]\n",
            "loss: 0.702714  [   33/ 3200]\n",
            "loss: 0.814344  [   34/ 3200]\n",
            "loss: 0.900438  [   35/ 3200]\n",
            "loss: 0.884908  [   36/ 3200]\n",
            "loss: 0.669184  [   37/ 3200]\n",
            "loss: 1.054692  [   38/ 3200]\n",
            "loss: 0.838065  [   39/ 3200]\n",
            "loss: 0.971000  [   40/ 3200]\n",
            "loss: 0.908605  [   41/ 3200]\n",
            "loss: 0.881628  [   42/ 3200]\n",
            "loss: 1.414034  [   43/ 3200]\n",
            "loss: 0.655988  [   44/ 3200]\n",
            "loss: 0.893570  [   45/ 3200]\n",
            "loss: 0.874441  [   46/ 3200]\n",
            "loss: 0.740487  [   47/ 3200]\n",
            "loss: 0.837333  [   48/ 3200]\n",
            "loss: 0.835132  [   49/ 3200]\n",
            "loss: 0.737296  [   50/ 3200]\n",
            "loss: 0.787337  [   51/ 3200]\n",
            "loss: 1.205022  [   52/ 3200]\n",
            "loss: 0.698697  [   53/ 3200]\n",
            "loss: 0.580883  [   54/ 3200]\n",
            "loss: 0.931056  [   55/ 3200]\n",
            "loss: 0.814225  [   56/ 3200]\n",
            "loss: 0.805626  [   57/ 3200]\n",
            "loss: 0.700748  [   58/ 3200]\n",
            "loss: 1.058036  [   59/ 3200]\n",
            "loss: 1.250569  [   60/ 3200]\n",
            "loss: 1.099671  [   61/ 3200]\n",
            "loss: 0.896027  [   62/ 3200]\n",
            "loss: 1.144028  [   63/ 3200]\n",
            "loss: 0.946230  [   64/ 3200]\n",
            "loss: 0.965557  [   65/ 3200]\n",
            "loss: 0.523351  [   66/ 3200]\n",
            "loss: 0.720350  [   67/ 3200]\n",
            "loss: 0.721286  [   68/ 3200]\n",
            "loss: 0.938732  [   69/ 3200]\n",
            "loss: 1.011229  [   70/ 3200]\n",
            "loss: 0.724535  [   71/ 3200]\n",
            "loss: 1.059259  [   72/ 3200]\n",
            "loss: 0.809670  [   73/ 3200]\n",
            "loss: 0.761866  [   74/ 3200]\n",
            "loss: 0.919097  [   75/ 3200]\n",
            "loss: 0.493830  [   76/ 3200]\n",
            "loss: 1.520815  [   77/ 3200]\n",
            "loss: 1.040389  [   78/ 3200]\n",
            "loss: 1.025321  [   79/ 3200]\n",
            "loss: 0.664157  [   80/ 3200]\n",
            "loss: 1.068680  [   81/ 3200]\n",
            "loss: 0.862373  [   82/ 3200]\n",
            "loss: 1.025747  [   83/ 3200]\n",
            "loss: 1.032169  [   84/ 3200]\n",
            "loss: 0.815726  [   85/ 3200]\n",
            "loss: 0.628211  [   86/ 3200]\n",
            "loss: 0.795692  [   87/ 3200]\n",
            "loss: 1.034628  [   88/ 3200]\n",
            "loss: 0.678641  [   89/ 3200]\n",
            "loss: 0.969269  [   90/ 3200]\n",
            "loss: 1.172423  [   91/ 3200]\n",
            "loss: 1.222485  [   92/ 3200]\n",
            "loss: 0.805242  [   93/ 3200]\n",
            "loss: 0.767259  [   94/ 3200]\n",
            "loss: 1.036749  [   95/ 3200]\n",
            "loss: 0.879220  [   96/ 3200]\n",
            "loss: 0.782900  [   97/ 3200]\n",
            "loss: 0.672562  [   98/ 3200]\n",
            "loss: 0.663391  [   99/ 3200]\n",
            "loss: 1.052579  [  100/ 3200]\n",
            "loss: 0.677309  [  101/ 3200]\n",
            "loss: 0.888778  [  102/ 3200]\n",
            "loss: 0.830097  [  103/ 3200]\n",
            "loss: 1.358333  [  104/ 3200]\n",
            "loss: 0.696567  [  105/ 3200]\n",
            "loss: 1.087388  [  106/ 3200]\n",
            "loss: 0.928020  [  107/ 3200]\n",
            "loss: 0.882847  [  108/ 3200]\n",
            "loss: 0.753682  [  109/ 3200]\n",
            "loss: 0.764698  [  110/ 3200]\n",
            "loss: 0.839085  [  111/ 3200]\n",
            "loss: 0.717347  [  112/ 3200]\n",
            "loss: 0.793132  [  113/ 3200]\n",
            "loss: 0.848564  [  114/ 3200]\n",
            "loss: 0.612058  [  115/ 3200]\n",
            "loss: 0.895779  [  116/ 3200]\n",
            "loss: 0.785031  [  117/ 3200]\n",
            "loss: 0.797817  [  118/ 3200]\n",
            "loss: 0.750148  [  119/ 3200]\n",
            "loss: 0.882355  [  120/ 3200]\n",
            "loss: 0.817498  [  121/ 3200]\n",
            "loss: 0.887861  [  122/ 3200]\n",
            "loss: 1.093607  [  123/ 3200]\n",
            "loss: 1.072830  [  124/ 3200]\n",
            "loss: 0.721695  [  125/ 3200]\n",
            "loss: 0.840196  [  126/ 3200]\n",
            "loss: 0.752041  [  127/ 3200]\n",
            "loss: 1.114267  [  128/ 3200]\n",
            "loss: 0.612797  [  129/ 3200]\n",
            "loss: 1.139547  [  130/ 3200]\n",
            "loss: 1.143181  [  131/ 3200]\n",
            "loss: 0.790847  [  132/ 3200]\n",
            "loss: 0.711029  [  133/ 3200]\n",
            "loss: 0.807749  [  134/ 3200]\n",
            "loss: 0.856553  [  135/ 3200]\n",
            "loss: 0.889667  [  136/ 3200]\n",
            "loss: 0.873668  [  137/ 3200]\n",
            "loss: 0.554243  [  138/ 3200]\n",
            "loss: 0.806624  [  139/ 3200]\n",
            "loss: 0.705262  [  140/ 3200]\n",
            "loss: 0.463747  [  141/ 3200]\n",
            "loss: 0.893744  [  142/ 3200]\n",
            "loss: 0.790635  [  143/ 3200]\n",
            "loss: 0.770856  [  144/ 3200]\n",
            "loss: 0.884074  [  145/ 3200]\n",
            "loss: 1.063026  [  146/ 3200]\n",
            "loss: 0.957210  [  147/ 3200]\n",
            "loss: 1.016892  [  148/ 3200]\n",
            "loss: 0.841699  [  149/ 3200]\n",
            "loss: 0.912577  [  150/ 3200]\n",
            "loss: 0.660983  [  151/ 3200]\n",
            "loss: 1.030036  [  152/ 3200]\n",
            "loss: 0.825131  [  153/ 3200]\n",
            "loss: 0.828039  [  154/ 3200]\n",
            "loss: 0.718710  [  155/ 3200]\n",
            "loss: 0.873310  [  156/ 3200]\n",
            "loss: 0.916939  [  157/ 3200]\n",
            "loss: 1.404213  [  158/ 3200]\n",
            "loss: 0.832159  [  159/ 3200]\n",
            "loss: 0.584186  [  160/ 3200]\n",
            "loss: 1.000664  [  161/ 3200]\n",
            "loss: 0.800492  [  162/ 3200]\n",
            "loss: 0.811006  [  163/ 3200]\n",
            "loss: 0.807859  [  164/ 3200]\n",
            "loss: 0.856897  [  165/ 3200]\n",
            "loss: 1.164519  [  166/ 3200]\n",
            "loss: 0.886134  [  167/ 3200]\n",
            "loss: 1.002946  [  168/ 3200]\n",
            "loss: 0.614075  [  169/ 3200]\n",
            "loss: 0.616381  [  170/ 3200]\n",
            "loss: 0.799351  [  171/ 3200]\n",
            "loss: 0.687928  [  172/ 3200]\n",
            "loss: 0.861851  [  173/ 3200]\n",
            "loss: 1.348758  [  174/ 3200]\n",
            "loss: 0.846994  [  175/ 3200]\n",
            "loss: 0.819547  [  176/ 3200]\n",
            "loss: 0.705313  [  177/ 3200]\n",
            "loss: 0.863922  [  178/ 3200]\n",
            "loss: 0.933520  [  179/ 3200]\n",
            "loss: 1.163637  [  180/ 3200]\n",
            "loss: 0.984203  [  181/ 3200]\n",
            "loss: 0.866262  [  182/ 3200]\n",
            "loss: 0.990926  [  183/ 3200]\n",
            "loss: 0.834035  [  184/ 3200]\n",
            "loss: 1.054025  [  185/ 3200]\n",
            "loss: 1.266607  [  186/ 3200]\n",
            "loss: 0.800210  [  187/ 3200]\n",
            "loss: 0.789118  [  188/ 3200]\n",
            "loss: 0.857061  [  189/ 3200]\n",
            "loss: 0.768558  [  190/ 3200]\n",
            "loss: 1.014011  [  191/ 3200]\n",
            "loss: 0.856930  [  192/ 3200]\n",
            "loss: 1.043351  [  193/ 3200]\n",
            "loss: 1.375107  [  194/ 3200]\n",
            "loss: 1.015365  [  195/ 3200]\n",
            "loss: 0.715848  [  196/ 3200]\n",
            "loss: 1.132932  [  197/ 3200]\n",
            "loss: 1.021145  [  198/ 3200]\n",
            "loss: 0.735449  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 28\n",
            "Validation Loss: 0.8244\n",
            "Validation F1: 0.6731\n",
            "Validation Accuracy: 0.6775\n",
            "Validation Confusion Matrix:\n",
            "[[ 92  51  45  12]\n",
            " [  8 187   4   1]\n",
            " [ 50   3 141   6]\n",
            " [ 45  23  10 122]]\n",
            "Epoch:  1\n",
            "loss: 1.037228  [    0/ 3200]\n",
            "loss: 0.661457  [    1/ 3200]\n",
            "loss: 0.848703  [    2/ 3200]\n",
            "loss: 0.673883  [    3/ 3200]\n",
            "loss: 0.771650  [    4/ 3200]\n",
            "loss: 0.840855  [    5/ 3200]\n",
            "loss: 0.950893  [    6/ 3200]\n",
            "loss: 1.273719  [    7/ 3200]\n",
            "loss: 1.509736  [    8/ 3200]\n",
            "loss: 0.933052  [    9/ 3200]\n",
            "loss: 0.836347  [   10/ 3200]\n",
            "loss: 0.839483  [   11/ 3200]\n",
            "loss: 0.680655  [   12/ 3200]\n",
            "loss: 0.622221  [   13/ 3200]\n",
            "loss: 1.013089  [   14/ 3200]\n",
            "loss: 0.910128  [   15/ 3200]\n",
            "loss: 0.731952  [   16/ 3200]\n",
            "loss: 1.109103  [   17/ 3200]\n",
            "loss: 0.740167  [   18/ 3200]\n",
            "loss: 1.198850  [   19/ 3200]\n",
            "loss: 0.836179  [   20/ 3200]\n",
            "loss: 0.689397  [   21/ 3200]\n",
            "loss: 1.018410  [   22/ 3200]\n",
            "loss: 0.993709  [   23/ 3200]\n",
            "loss: 0.597448  [   24/ 3200]\n",
            "loss: 0.653314  [   25/ 3200]\n",
            "loss: 0.825674  [   26/ 3200]\n",
            "loss: 0.952733  [   27/ 3200]\n",
            "loss: 1.006168  [   28/ 3200]\n",
            "loss: 1.036101  [   29/ 3200]\n",
            "loss: 1.081108  [   30/ 3200]\n",
            "loss: 0.624381  [   31/ 3200]\n",
            "loss: 0.642374  [   32/ 3200]\n",
            "loss: 1.248351  [   33/ 3200]\n",
            "loss: 0.744624  [   34/ 3200]\n",
            "loss: 0.996540  [   35/ 3200]\n",
            "loss: 0.959958  [   36/ 3200]\n",
            "loss: 1.108337  [   37/ 3200]\n",
            "loss: 1.225361  [   38/ 3200]\n",
            "loss: 0.716858  [   39/ 3200]\n",
            "loss: 0.906145  [   40/ 3200]\n",
            "loss: 0.826044  [   41/ 3200]\n",
            "loss: 0.758845  [   42/ 3200]\n",
            "loss: 1.048874  [   43/ 3200]\n",
            "loss: 0.913859  [   44/ 3200]\n",
            "loss: 0.978254  [   45/ 3200]\n",
            "loss: 1.032951  [   46/ 3200]\n",
            "loss: 1.091848  [   47/ 3200]\n",
            "loss: 0.842269  [   48/ 3200]\n",
            "loss: 0.759592  [   49/ 3200]\n",
            "loss: 0.996482  [   50/ 3200]\n",
            "loss: 0.709026  [   51/ 3200]\n",
            "loss: 0.902048  [   52/ 3200]\n",
            "loss: 0.601285  [   53/ 3200]\n",
            "loss: 0.767091  [   54/ 3200]\n",
            "loss: 0.778752  [   55/ 3200]\n",
            "loss: 1.099750  [   56/ 3200]\n",
            "loss: 1.165115  [   57/ 3200]\n",
            "loss: 0.621954  [   58/ 3200]\n",
            "loss: 1.084751  [   59/ 3200]\n",
            "loss: 0.813560  [   60/ 3200]\n",
            "loss: 1.178738  [   61/ 3200]\n",
            "loss: 0.640862  [   62/ 3200]\n",
            "loss: 0.888750  [   63/ 3200]\n",
            "loss: 1.078626  [   64/ 3200]\n",
            "loss: 1.013919  [   65/ 3200]\n",
            "loss: 1.017735  [   66/ 3200]\n",
            "loss: 1.108004  [   67/ 3200]\n",
            "loss: 0.698759  [   68/ 3200]\n",
            "loss: 0.950733  [   69/ 3200]\n",
            "loss: 0.726843  [   70/ 3200]\n",
            "loss: 0.708160  [   71/ 3200]\n",
            "loss: 0.709099  [   72/ 3200]\n",
            "loss: 0.812204  [   73/ 3200]\n",
            "loss: 0.847652  [   74/ 3200]\n",
            "loss: 0.994960  [   75/ 3200]\n",
            "loss: 0.616142  [   76/ 3200]\n",
            "loss: 0.980681  [   77/ 3200]\n",
            "loss: 1.148946  [   78/ 3200]\n",
            "loss: 0.770515  [   79/ 3200]\n",
            "loss: 0.834033  [   80/ 3200]\n",
            "loss: 0.690174  [   81/ 3200]\n",
            "loss: 1.095031  [   82/ 3200]\n",
            "loss: 1.453320  [   83/ 3200]\n",
            "loss: 0.689334  [   84/ 3200]\n",
            "loss: 1.319313  [   85/ 3200]\n",
            "loss: 0.917728  [   86/ 3200]\n",
            "loss: 0.882820  [   87/ 3200]\n",
            "loss: 1.015436  [   88/ 3200]\n",
            "loss: 0.856225  [   89/ 3200]\n",
            "loss: 0.717356  [   90/ 3200]\n",
            "loss: 1.321814  [   91/ 3200]\n",
            "loss: 0.891714  [   92/ 3200]\n",
            "loss: 1.165426  [   93/ 3200]\n",
            "loss: 0.850888  [   94/ 3200]\n",
            "loss: 1.096791  [   95/ 3200]\n",
            "loss: 0.670818  [   96/ 3200]\n",
            "loss: 0.856910  [   97/ 3200]\n",
            "loss: 0.870056  [   98/ 3200]\n",
            "loss: 0.594764  [   99/ 3200]\n",
            "loss: 0.866644  [  100/ 3200]\n",
            "loss: 0.848814  [  101/ 3200]\n",
            "loss: 0.901947  [  102/ 3200]\n",
            "loss: 0.925879  [  103/ 3200]\n",
            "loss: 0.608932  [  104/ 3200]\n",
            "loss: 0.883582  [  105/ 3200]\n",
            "loss: 0.691180  [  106/ 3200]\n",
            "loss: 0.856292  [  107/ 3200]\n",
            "loss: 0.826367  [  108/ 3200]\n",
            "loss: 0.867663  [  109/ 3200]\n",
            "loss: 0.800045  [  110/ 3200]\n",
            "loss: 0.808993  [  111/ 3200]\n",
            "loss: 0.901652  [  112/ 3200]\n",
            "loss: 0.679284  [  113/ 3200]\n",
            "loss: 0.718677  [  114/ 3200]\n",
            "loss: 0.626415  [  115/ 3200]\n",
            "loss: 0.837650  [  116/ 3200]\n",
            "loss: 0.788229  [  117/ 3200]\n",
            "loss: 0.918289  [  118/ 3200]\n",
            "loss: 1.045492  [  119/ 3200]\n",
            "loss: 0.834993  [  120/ 3200]\n",
            "loss: 0.851475  [  121/ 3200]\n",
            "loss: 0.787316  [  122/ 3200]\n",
            "loss: 0.659020  [  123/ 3200]\n",
            "loss: 0.483494  [  124/ 3200]\n",
            "loss: 0.658414  [  125/ 3200]\n",
            "loss: 0.591662  [  126/ 3200]\n",
            "loss: 0.794892  [  127/ 3200]\n",
            "loss: 1.002585  [  128/ 3200]\n",
            "loss: 0.779105  [  129/ 3200]\n",
            "loss: 0.999935  [  130/ 3200]\n",
            "loss: 0.852285  [  131/ 3200]\n",
            "loss: 0.928450  [  132/ 3200]\n",
            "loss: 1.056473  [  133/ 3200]\n",
            "loss: 1.116683  [  134/ 3200]\n",
            "loss: 1.414592  [  135/ 3200]\n",
            "loss: 0.726690  [  136/ 3200]\n",
            "loss: 0.947394  [  137/ 3200]\n",
            "loss: 0.815054  [  138/ 3200]\n",
            "loss: 0.726722  [  139/ 3200]\n",
            "loss: 0.765682  [  140/ 3200]\n",
            "loss: 1.177117  [  141/ 3200]\n",
            "loss: 0.619870  [  142/ 3200]\n",
            "loss: 0.665563  [  143/ 3200]\n",
            "loss: 0.820798  [  144/ 3200]\n",
            "loss: 0.910124  [  145/ 3200]\n",
            "loss: 0.905547  [  146/ 3200]\n",
            "loss: 0.570226  [  147/ 3200]\n",
            "loss: 1.041647  [  148/ 3200]\n",
            "loss: 0.771286  [  149/ 3200]\n",
            "loss: 0.905046  [  150/ 3200]\n",
            "loss: 1.061721  [  151/ 3200]\n",
            "loss: 1.020724  [  152/ 3200]\n",
            "loss: 0.744621  [  153/ 3200]\n",
            "loss: 0.815501  [  154/ 3200]\n",
            "loss: 0.701336  [  155/ 3200]\n",
            "loss: 1.069436  [  156/ 3200]\n",
            "loss: 0.717348  [  157/ 3200]\n",
            "loss: 0.971492  [  158/ 3200]\n",
            "loss: 0.966664  [  159/ 3200]\n",
            "loss: 0.933522  [  160/ 3200]\n",
            "loss: 1.055155  [  161/ 3200]\n",
            "loss: 1.051324  [  162/ 3200]\n",
            "loss: 1.054895  [  163/ 3200]\n",
            "loss: 0.972398  [  164/ 3200]\n",
            "loss: 1.431748  [  165/ 3200]\n",
            "loss: 0.737692  [  166/ 3200]\n",
            "loss: 1.060453  [  167/ 3200]\n",
            "loss: 0.857614  [  168/ 3200]\n",
            "loss: 0.651938  [  169/ 3200]\n",
            "loss: 0.744638  [  170/ 3200]\n",
            "loss: 0.897806  [  171/ 3200]\n",
            "loss: 0.705322  [  172/ 3200]\n",
            "loss: 0.513651  [  173/ 3200]\n",
            "loss: 0.817356  [  174/ 3200]\n",
            "loss: 0.698689  [  175/ 3200]\n",
            "loss: 0.833233  [  176/ 3200]\n",
            "loss: 0.876847  [  177/ 3200]\n",
            "loss: 0.470928  [  178/ 3200]\n",
            "loss: 0.911812  [  179/ 3200]\n",
            "loss: 0.868096  [  180/ 3200]\n",
            "loss: 0.783890  [  181/ 3200]\n",
            "loss: 0.980228  [  182/ 3200]\n",
            "loss: 0.821610  [  183/ 3200]\n",
            "loss: 0.492932  [  184/ 3200]\n",
            "loss: 1.166774  [  185/ 3200]\n",
            "loss: 1.092351  [  186/ 3200]\n",
            "loss: 0.584202  [  187/ 3200]\n",
            "loss: 0.947978  [  188/ 3200]\n",
            "loss: 0.920307  [  189/ 3200]\n",
            "loss: 0.802765  [  190/ 3200]\n",
            "loss: 0.817734  [  191/ 3200]\n",
            "loss: 1.112723  [  192/ 3200]\n",
            "loss: 0.808388  [  193/ 3200]\n",
            "loss: 0.825212  [  194/ 3200]\n",
            "loss: 0.856570  [  195/ 3200]\n",
            "loss: 1.199632  [  196/ 3200]\n",
            "loss: 0.801512  [  197/ 3200]\n",
            "loss: 0.756815  [  198/ 3200]\n",
            "loss: 0.914927  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 29\n",
            "Validation Loss: 0.8127\n",
            "Validation F1: 0.6758\n",
            "Validation Accuracy: 0.6863\n",
            "Validation Confusion Matrix:\n",
            "[[ 79  59  43  19]\n",
            " [  6 189   3   2]\n",
            " [ 47   4 139  10]\n",
            " [ 24  26   8 142]]\n",
            "Epoch:  1\n",
            "loss: 1.192992  [    0/ 3200]\n",
            "loss: 0.978596  [    1/ 3200]\n",
            "loss: 0.907459  [    2/ 3200]\n",
            "loss: 0.981882  [    3/ 3200]\n",
            "loss: 0.685306  [    4/ 3200]\n",
            "loss: 0.832409  [    5/ 3200]\n",
            "loss: 1.156424  [    6/ 3200]\n",
            "loss: 0.662646  [    7/ 3200]\n",
            "loss: 0.841056  [    8/ 3200]\n",
            "loss: 0.995913  [    9/ 3200]\n",
            "loss: 1.112135  [   10/ 3200]\n",
            "loss: 0.942321  [   11/ 3200]\n",
            "loss: 1.120650  [   12/ 3200]\n",
            "loss: 0.824123  [   13/ 3200]\n",
            "loss: 0.785412  [   14/ 3200]\n",
            "loss: 0.734655  [   15/ 3200]\n",
            "loss: 1.149295  [   16/ 3200]\n",
            "loss: 0.995893  [   17/ 3200]\n",
            "loss: 0.995786  [   18/ 3200]\n",
            "loss: 0.953808  [   19/ 3200]\n",
            "loss: 0.767404  [   20/ 3200]\n",
            "loss: 0.446546  [   21/ 3200]\n",
            "loss: 0.593233  [   22/ 3200]\n",
            "loss: 1.156594  [   23/ 3200]\n",
            "loss: 0.923888  [   24/ 3200]\n",
            "loss: 1.055839  [   25/ 3200]\n",
            "loss: 1.115291  [   26/ 3200]\n",
            "loss: 0.861481  [   27/ 3200]\n",
            "loss: 0.922074  [   28/ 3200]\n",
            "loss: 1.159421  [   29/ 3200]\n",
            "loss: 1.188279  [   30/ 3200]\n",
            "loss: 0.828356  [   31/ 3200]\n",
            "loss: 0.572863  [   32/ 3200]\n",
            "loss: 0.749917  [   33/ 3200]\n",
            "loss: 1.041093  [   34/ 3200]\n",
            "loss: 0.891921  [   35/ 3200]\n",
            "loss: 0.840636  [   36/ 3200]\n",
            "loss: 0.907519  [   37/ 3200]\n",
            "loss: 0.812314  [   38/ 3200]\n",
            "loss: 1.115221  [   39/ 3200]\n",
            "loss: 1.000319  [   40/ 3200]\n",
            "loss: 1.170222  [   41/ 3200]\n",
            "loss: 1.454346  [   42/ 3200]\n",
            "loss: 0.786540  [   43/ 3200]\n",
            "loss: 0.796375  [   44/ 3200]\n",
            "loss: 1.047084  [   45/ 3200]\n",
            "loss: 1.212851  [   46/ 3200]\n",
            "loss: 1.019153  [   47/ 3200]\n",
            "loss: 0.696564  [   48/ 3200]\n",
            "loss: 0.756346  [   49/ 3200]\n",
            "loss: 0.715455  [   50/ 3200]\n",
            "loss: 0.883782  [   51/ 3200]\n",
            "loss: 1.081466  [   52/ 3200]\n",
            "loss: 1.202355  [   53/ 3200]\n",
            "loss: 0.732534  [   54/ 3200]\n",
            "loss: 0.920723  [   55/ 3200]\n",
            "loss: 0.721500  [   56/ 3200]\n",
            "loss: 0.666715  [   57/ 3200]\n",
            "loss: 0.699233  [   58/ 3200]\n",
            "loss: 0.768836  [   59/ 3200]\n",
            "loss: 0.756733  [   60/ 3200]\n",
            "loss: 0.619915  [   61/ 3200]\n",
            "loss: 1.258098  [   62/ 3200]\n",
            "loss: 1.115343  [   63/ 3200]\n",
            "loss: 0.946835  [   64/ 3200]\n",
            "loss: 1.318366  [   65/ 3200]\n",
            "loss: 1.164249  [   66/ 3200]\n",
            "loss: 1.027360  [   67/ 3200]\n",
            "loss: 0.736540  [   68/ 3200]\n",
            "loss: 0.681990  [   69/ 3200]\n",
            "loss: 0.794228  [   70/ 3200]\n",
            "loss: 1.025974  [   71/ 3200]\n",
            "loss: 0.619078  [   72/ 3200]\n",
            "loss: 0.824481  [   73/ 3200]\n",
            "loss: 0.757756  [   74/ 3200]\n",
            "loss: 1.030594  [   75/ 3200]\n",
            "loss: 0.768970  [   76/ 3200]\n",
            "loss: 0.819185  [   77/ 3200]\n",
            "loss: 0.659688  [   78/ 3200]\n",
            "loss: 0.835393  [   79/ 3200]\n",
            "loss: 0.651529  [   80/ 3200]\n",
            "loss: 0.995633  [   81/ 3200]\n",
            "loss: 0.897995  [   82/ 3200]\n",
            "loss: 0.918928  [   83/ 3200]\n",
            "loss: 0.976251  [   84/ 3200]\n",
            "loss: 1.199642  [   85/ 3200]\n",
            "loss: 0.964872  [   86/ 3200]\n",
            "loss: 0.782147  [   87/ 3200]\n",
            "loss: 0.645333  [   88/ 3200]\n",
            "loss: 0.837587  [   89/ 3200]\n",
            "loss: 0.999114  [   90/ 3200]\n",
            "loss: 0.784565  [   91/ 3200]\n",
            "loss: 0.845826  [   92/ 3200]\n",
            "loss: 0.654083  [   93/ 3200]\n",
            "loss: 1.423160  [   94/ 3200]\n",
            "loss: 0.872384  [   95/ 3200]\n",
            "loss: 1.216541  [   96/ 3200]\n",
            "loss: 0.940491  [   97/ 3200]\n",
            "loss: 0.786906  [   98/ 3200]\n",
            "loss: 0.961440  [   99/ 3200]\n",
            "loss: 1.208538  [  100/ 3200]\n",
            "loss: 0.824629  [  101/ 3200]\n",
            "loss: 0.912837  [  102/ 3200]\n",
            "loss: 0.520230  [  103/ 3200]\n",
            "loss: 0.698200  [  104/ 3200]\n",
            "loss: 0.552642  [  105/ 3200]\n",
            "loss: 0.964317  [  106/ 3200]\n",
            "loss: 1.260880  [  107/ 3200]\n",
            "loss: 0.916431  [  108/ 3200]\n",
            "loss: 0.866844  [  109/ 3200]\n",
            "loss: 0.892604  [  110/ 3200]\n",
            "loss: 0.761705  [  111/ 3200]\n",
            "loss: 0.808910  [  112/ 3200]\n",
            "loss: 1.004023  [  113/ 3200]\n",
            "loss: 0.742375  [  114/ 3200]\n",
            "loss: 0.811965  [  115/ 3200]\n",
            "loss: 0.559541  [  116/ 3200]\n",
            "loss: 1.050161  [  117/ 3200]\n",
            "loss: 0.671124  [  118/ 3200]\n",
            "loss: 0.710006  [  119/ 3200]\n",
            "loss: 0.718371  [  120/ 3200]\n",
            "loss: 0.927098  [  121/ 3200]\n",
            "loss: 0.774411  [  122/ 3200]\n",
            "loss: 0.680983  [  123/ 3200]\n",
            "loss: 1.062334  [  124/ 3200]\n",
            "loss: 0.739708  [  125/ 3200]\n",
            "loss: 0.708482  [  126/ 3200]\n",
            "loss: 0.907146  [  127/ 3200]\n",
            "loss: 0.859274  [  128/ 3200]\n",
            "loss: 0.966658  [  129/ 3200]\n",
            "loss: 0.871182  [  130/ 3200]\n",
            "loss: 1.028875  [  131/ 3200]\n",
            "loss: 0.568023  [  132/ 3200]\n",
            "loss: 0.840699  [  133/ 3200]\n",
            "loss: 0.985783  [  134/ 3200]\n",
            "loss: 0.608529  [  135/ 3200]\n",
            "loss: 1.021693  [  136/ 3200]\n",
            "loss: 0.538727  [  137/ 3200]\n",
            "loss: 0.733478  [  138/ 3200]\n",
            "loss: 0.787755  [  139/ 3200]\n",
            "loss: 0.743906  [  140/ 3200]\n",
            "loss: 1.104514  [  141/ 3200]\n",
            "loss: 0.601340  [  142/ 3200]\n",
            "loss: 0.911363  [  143/ 3200]\n",
            "loss: 0.605903  [  144/ 3200]\n",
            "loss: 0.956424  [  145/ 3200]\n",
            "loss: 0.607659  [  146/ 3200]\n",
            "loss: 0.896177  [  147/ 3200]\n",
            "loss: 0.809854  [  148/ 3200]\n",
            "loss: 0.861614  [  149/ 3200]\n",
            "loss: 0.673991  [  150/ 3200]\n",
            "loss: 0.843807  [  151/ 3200]\n",
            "loss: 0.767797  [  152/ 3200]\n",
            "loss: 0.977382  [  153/ 3200]\n",
            "loss: 0.673612  [  154/ 3200]\n",
            "loss: 1.420185  [  155/ 3200]\n",
            "loss: 0.698325  [  156/ 3200]\n",
            "loss: 0.780374  [  157/ 3200]\n",
            "loss: 0.587694  [  158/ 3200]\n",
            "loss: 1.127551  [  159/ 3200]\n",
            "loss: 1.067556  [  160/ 3200]\n",
            "loss: 1.084123  [  161/ 3200]\n",
            "loss: 0.536066  [  162/ 3200]\n",
            "loss: 0.847520  [  163/ 3200]\n",
            "loss: 0.671852  [  164/ 3200]\n",
            "loss: 1.207601  [  165/ 3200]\n",
            "loss: 0.873933  [  166/ 3200]\n",
            "loss: 0.983557  [  167/ 3200]\n",
            "loss: 0.706577  [  168/ 3200]\n",
            "loss: 0.592831  [  169/ 3200]\n",
            "loss: 0.946542  [  170/ 3200]\n",
            "loss: 0.601469  [  171/ 3200]\n",
            "loss: 0.576794  [  172/ 3200]\n",
            "loss: 0.621457  [  173/ 3200]\n",
            "loss: 0.748135  [  174/ 3200]\n",
            "loss: 0.910745  [  175/ 3200]\n",
            "loss: 0.964551  [  176/ 3200]\n",
            "loss: 0.995978  [  177/ 3200]\n",
            "loss: 1.195683  [  178/ 3200]\n",
            "loss: 0.933074  [  179/ 3200]\n",
            "loss: 0.677031  [  180/ 3200]\n",
            "loss: 0.915420  [  181/ 3200]\n",
            "loss: 0.779369  [  182/ 3200]\n",
            "loss: 0.822822  [  183/ 3200]\n",
            "loss: 0.576888  [  184/ 3200]\n",
            "loss: 1.088326  [  185/ 3200]\n",
            "loss: 1.096556  [  186/ 3200]\n",
            "loss: 0.653725  [  187/ 3200]\n",
            "loss: 1.325356  [  188/ 3200]\n",
            "loss: 1.342020  [  189/ 3200]\n",
            "loss: 0.582551  [  190/ 3200]\n",
            "loss: 0.834899  [  191/ 3200]\n",
            "loss: 0.670688  [  192/ 3200]\n",
            "loss: 0.822649  [  193/ 3200]\n",
            "loss: 0.545132  [  194/ 3200]\n",
            "loss: 0.746405  [  195/ 3200]\n",
            "loss: 1.276043  [  196/ 3200]\n",
            "loss: 0.870067  [  197/ 3200]\n",
            "loss: 0.937240  [  198/ 3200]\n",
            "loss: 1.041654  [  199/ 3200]\n",
            "Training finished.\n",
            "Epoch: 30\n",
            "Validation Loss: 0.8410\n",
            "Validation F1: 0.6429\n",
            "Validation Accuracy: 0.6575\n",
            "Validation Confusion Matrix:\n",
            "[[ 76  70  15  39]\n",
            " [  2 186   3   9]\n",
            " [ 59   7 106  28]\n",
            " [ 16  21   5 158]]\n",
            "Best Model Performance on Test Set:\n",
            "Test Loss: 0.9560\n",
            "Test F1: 0.5955\n",
            "Test Accuracy: 0.6112\n",
            "Test Confusion Matrix:\n",
            "[[ 96  53  47 128]\n",
            " [ 13 263   5  16]\n",
            " [100  22 195  39]\n",
            " [ 28  56  28 287]]\n"
          ]
        }
      ],
      "source": [
        "best_model, test_loss, test_f1, test_accuracy, test_confusion_mat = train_and_evaluate(\n",
        "    model, optimizer, train_dataloader, validation_dataloader, test_dataloader, loss_function, num_epochs\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/eva/Documents/machine_learning/Project3\n",
            "torch.Size([3200, 21, 128])\n",
            "torch.Size([3200])\n",
            "torch.Size([800, 21, 128])\n",
            "torch.Size([800])\n",
            "torch.Size([1376, 21, 128])\n",
            "torch.Size([1376])\n"
          ]
        }
      ],
      "source": [
        "path = os.getcwd()\n",
        "print(path)\n",
        "\n",
        "#path = '/content/gdrive/My Drive/Project3'\n",
        "\n",
        "#create the paths for the data\n",
        "training_data_path = path + '/data/music_genre_data_di/train/melgrams'\n",
        "test_data_path = path + '/data/music_genre_data_di/test/melgrams'\n",
        "validation_data_path = path + '/data/music_genre_data_di/val/melgrams'\n",
        "\n",
        "X_train = np.load(training_data_path + '/X.npy')\n",
        "y_train = np.load(training_data_path + '/labels.npy')\n",
        "X_val   = np.load(validation_data_path + '/X.npy')\n",
        "y_val   = np.load(validation_data_path + '/labels.npy')\n",
        "X_test  = np.load(test_data_path + '/X.npy')\n",
        "y_test  = np.load(test_data_path + '/labels.npy')\n",
        "\n",
        "# Μετατροπή των labels σε ακέραιους αριθμούς (0, 1, 2, 3)\n",
        "classes = np.unique(y_train)\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "y_train = np.array([class_to_idx[label] for label in y_train])\n",
        "y_val = np.array([class_to_idx[label] for label in y_val])\n",
        "y_test = np.array([class_to_idx[label] for label in y_test])\n",
        "\n",
        "# Δημιουργία των Tensor αντικειμένων από τα numpy arrays\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Εμφάνιση των διαστάσεων των Tensor\n",
        "print(train_dataset.tensors[0].shape)\n",
        "print(train_dataset.tensors[1].shape)\n",
        "print(val_dataset.tensors[0].shape)\n",
        "print(val_dataset.tensors[1].shape)\n",
        "print(test_dataset.tensors[0].shape)\n",
        "print(test_dataset.tensors[1].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def visualize_melgram(melgram, class_name):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(melgram, cmap='hot', interpolation='nearest', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(class_name + ' melgram')\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot a random melgram from each class\n",
        "def plot_random_melgram_from_each_class(dataset, class_labels, class_names):\n",
        "    classes = np.unique(class_labels)\n",
        "    for class_label in classes:\n",
        "        indices = np.where(class_labels == class_label)[0]\n",
        "        # Select a random index\n",
        "        random_index = random.choice(indices)\n",
        "        melgram, label = dataset[random_index]\n",
        "        class_name = class_names[class_label]  # Get the class name corresponding to the class label\n",
        "        visualize_melgram(melgram, class_name)\n",
        "\n",
        "# Retrieve the class names from the class_to_idx dictionary\n",
        "class_names = [class_label for class_label, class_index in class_to_idx.items()]\n",
        "\n",
        "# Plot a random melgram from each class in the training dataset\n",
        "plot_random_melgram_from_each_class(train_dataset, y_train, class_names)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
